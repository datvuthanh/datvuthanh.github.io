<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Paper notes | Learning-Deep-Learning</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Paper notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper reading notes on Deep Learning and Machine Learning" />
<meta property="og:description" content="Paper reading notes on Deep Learning and Machine Learning" />
<link rel="canonical" href="https://patrick-llgc.github.io/Learning-Deep-Learning/" />
<meta property="og:url" content="https://patrick-llgc.github.io/Learning-Deep-Learning/" />
<meta property="og:site_name" content="Learning-Deep-Learning" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Paper notes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Paper reading notes on Deep Learning and Machine Learning","headline":"Paper notes","name":"Learning-Deep-Learning","url":"https://patrick-llgc.github.io/Learning-Deep-Learning/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Learning-Deep-Learning/assets/css/style.css?v=15ffc9f3d4ef254d85c48a2a8b8d1b61f4e78b45">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Learning-Deep-Learning/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="https://patrick-llgc.github.io/Learning-Deep-Learning/">Learning-Deep-Learning</a></h1>
      

      <h1 id="paper-notes">Paper notes</h1>
<p>This repository contains my paper reading notes on deep learning and machine learning. It is inspired by <a href="https://github.com/dennybritz/deeplearning-papernotes">Denny Britz</a> and <a href="https://github.com/DanielTakeshi/Paper_Notes">Daniel Takeshi</a>. A minimalistic webpage generated with Github io can be found <a href="https://patrick-llgc.github.io/Learning-Deep-Learning/">here</a>.</p>

<h2 id="about-me">About me</h2>
<p>My name is <a href="https://www.linkedin.com/in/patrick-llgc/">Patrick Langechuan Liu</a>. After about a decade of education and research in physics, I found my passion in deep learning and autonomous driving. Currently I am leading the development of perception features at <a href="https://en.xiaopeng.com/">Xpeng Motors</a>, a fast growing autonomous driving company.</p>

<h2 id="what-to-read">What to read</h2>
<p>If you are new to deep learning in computer vision and don’t know where to start, I suggest you spend your first month or so dive deep into <a href="/Learning-Deep-Learning/start/first_cnn_papers.html">this list of papers</a>. I did so (<a href="/Learning-Deep-Learning/start/first_cnn_papers_notes.html">see my notes</a>) and it served me well.</p>

<p>Here is <a href="/Learning-Deep-Learning/trusty.html">a list of trustworthy sources of papers</a> in case I ran out of papers to read.</p>

<h2 id="my-review-posts-by-topics">My Review Posts by Topics</h2>
<p>I regularly update <a href="https://medium.com/@patrickllgc">my blog in Toward Data Science</a>.</p>

<ul>
  <li><a href="https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41">Drivable Space in Autonomous Driving — The Industry</a></li>
  <li><a href="https://towardsdatascience.com/drivable-space-in-autonomous-driving-a-review-of-academia-ef1a6aa4dc15">Drivable Space in Autonomous Driving — The Academia</a></li>
  <li><a href="https://towardsdatascience.com/drivable-space-in-autonomous-driving-the-concept-df699bb8682f">Drivable Space in Autonomous Driving - The Concept</a></li>
  <li><a href="https://towardsdatascience.com/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944">Monocular BEV Perception with Transformers in Autonomous Driving</a> (<a href="/Learning-Deep-Learning/topics/topic_transformers_bev.html">related paper notes</a>)</li>
  <li><a href="https://towardsdatascience.com/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89">Illustrated Differences between MLP and Transformers for Tensor Reshaping in Deep Learning</a></li>
  <li><a href="https://towardsdatascience.com/monocular-3d-lane-line-detection-in-autonomous-driving-4d7cdfabf3b6">Monocular 3D Lane Line Detection in Autonomous Driving</a> (<a href="/Learning-Deep-Learning/topics/topic_3d_lld.html">related paper notes</a>)</li>
  <li><a href="https://towardsdatascience.com/deep-learning-based-object-detection-in-crowded-scenes-1c9fddbd7bc4">Deep-Learning based Object detection in Crowded Scenes</a> (<a href="/Learning-Deep-Learning/topics/topic_crowd_detection.html">related paper notes</a>)</li>
  <li><a href="https://towardsdatascience.com/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59">Monocular Bird’s-Eye-View Semantic Segmentation for Autonomous Driving</a> (<a href="/Learning-Deep-Learning/topics/topic_bev_segmentation.html">related paper notes</a>)</li>
  <li><a href="https://towardsdatascience.com/deep-learning-in-mapping-for-autonomous-driving-9e33ee951a44">Deep Learning in Mapping for Autonomous Driving</a></li>
  <li><a href="https://towardsdatascience.com/monocular-dynamic-object-slam-in-autonomous-driving-f12249052bf1">Monocular Dynamic Object SLAM in Autonomous Driving</a></li>
  <li><a href="https://towardsdatascience.com/monocular-3d-object-detection-in-autonomous-driving-2476a3c7f57e">Monocular 3D Object Detection in Autonomous Driving — A Review</a></li>
  <li><a href="https://towardsdatascience.com/self-supervised-keypoint-learning-aade18081fc3">Self-supervised Keypoint Learning — A Review</a></li>
  <li><a href="https://towardsdatascience.com/single-stage-instance-segmentation-a-review-1eeb66e0cc49">Single Stage Instance Segmentation — A Review</a></li>
  <li><a href="https://towardsdatascience.com/self-paced-multitask-learning-76c26e9532d0">Self-paced Multitask Learning — A Review</a></li>
  <li><a href="https://towardsdatascience.com/convolutional-neural-networks-with-heterogeneous-metadata-2af9241218a9">Convolutional Neural Networks with Heterogeneous Metadata</a></li>
  <li><a href="https://towardsdatascience.com/geometric-reasoning-based-cuboid-generation-in-monocular-3d-object-detection-5ee2996270d1">Lifting 2D object detection to 3D in autonomous driving</a></li>
  <li><a href="https://towardsdatascience.com/anchors-and-multi-bin-loss-for-multi-modal-target-regression-647ea1974617">Multimodal Regression</a></li>
  <li><a href="https://towardsdatascience.com/the-200-deep-learning-papers-i-read-in-2019-7fb7034f05f7?source=friends_link&amp;sk=7628c5be39f876b2c05e43c13d0b48a3">Paper Reading in 2019</a></li>
</ul>

<h2 id="2023-05">2023-05</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2303.09551">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/surroundocc.html">Notes</a>] [Occupancy Network, Wei Yi, Jiwen Lu]</li>
  <li><a href="https://arxiv.org/abs/2304.14365">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</a> [Occupancy Network, Zhao Hang]</li>
  <li><a href="https://arxiv.org/abs/2304.05316">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a> [Occupancy Network, PhiGent]</li>
  <li><a href="https://arxiv.org/abs/1611.08974">SSCNet: Semantic Scene Completion from a Single Depth Image</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.01416">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2107.06278">MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation</a> <kbd>NeurIPS 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.07466">3D Semantic Scene Completion: a Survey</a> <kbd>IJCV 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2201.02605">DETIC: Detecting Twenty-thousand Classes using Image-level Supervision</a> <kbd>ECCV 2022</kbd></li>
  <li><a href="https://github.com/magicleap/Atlas">Atlas: End-to-End 3D Scene Reconstruction from Posed Images</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2107.02191">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</a> <kbd>NeurIPS 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2303.10076">SimpleOccupancy: A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving</a> [Occupancy Network]</li>
  <li><a href="https://arxiv.org/abs/2302.13540">OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion</a> [Occupancy Network, stereo]</li>
  <li><a href="https://arxiv.org/abs/2301.07870">Fast-BEV: Towards Real-time On-vehicle Bird’s-Eye View Perception</a> <kbd>NeurIPS 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2301.12511">Fast-BEV: A Fast and Strong Bird’s-Eye View Perception Baseline</a></li>
  <li><a href="https://arxiv.org/abs/2303.12071">ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals</a> <kbd>CVPR 2023</kbd> [Qcraft, prediction]</li>
  <li><a href="https://arxiv.org/abs/2209.13508">Motion Transformer with Global Intention Localization and Local Movement Refinement</a> <kbd>NeurIPS 2022 Oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2211.01634">P4P: Conflict-Aware Motion Prediction for Planning in Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/2111.14973">MultiPath++: Efficient Information Fusion and Trajectory Aggregation for Behavior Prediction</a></li>
  <li><a href="https://arxiv.org/abs/2208.01582">ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries</a></li>
  <li><a href="https://arxiv.org/abs/2304.02643">SAM: Segment Anything</a> [FAIR]</li>
  <li><a href="https://arxiv.org/abs/2303.11325">GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding</a></li>
  <li><a href="https://arxiv.org/abs/2006.04767">Motion Prediction using Trajectory Sets and Self-Driving Domain Knowledge</a> [Encode Road requirement to prediction]</li>
  <li>[Hivt: Hierarchical vector transformer for multi-agent motion prediction]</li>
  <li><a href="https://arxiv.org/abs/2012.14913">Transformer Feed-Forward Layers Are Key-Value Memories</a> <kbd>EMNLP 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2210.06006">BEV-LaneDet: a Simple and Effective 3D Lane Detection Baseline</a> <kbd>CVPR 2023</kbd> [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2303.05970">Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D Perception</a> [BEVNet, megvii]</li>
  <li><a href="https://arxiv.org/abs/2303.12077">VAD: Vectorized Scene Representation for Efficient Autonomous Driving</a> [Horizon]</li>
  <li><a href="https://arxiv.org/abs/2303.10076">A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/2211.17111">BEVPoolv2: A Cutting-edge Implementation of BEVDet Toward Deployment</a> [BEVDet, PhiGent]</li>
  <li><a href="http://aixpaper.com/view/nvradarnet_realtime_radar_obstacle_and_free_space_detection_for_autonomous_driving">NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving</a></li>
  <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.pdf">GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping</a> <kbd>CVPR 2020</kbd> [Cewu Lu]</li>
  <li><a href="https://arxiv.org/abs/2212.08333">AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains</a> [Cewu Lu]</li>
  <li><a href="https://arxiv.org/abs/2302.13130">Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting</a></li>
  <li><a href="https://arxiv.org/abs/2205.09753">HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding</a></li>
  <li><a href="https://arxiv.org/abs/2209.13508">MTR: Motion Transformer with Global Intention Localization and Local Movement Refinement</a> <kbd>NeurIPS 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/1812.03828">Occupancy Networks: Learning 3D Reconstruction in Function Space</a> <kbd>CVPR 2019</kbd> [Andreas Geiger]</li>
  <li><a href="https://arxiv.org/abs/2206.00630">UVTR: Unifying Voxel-based Representation with Transformer for 3D Object Detection</a> [BEVFusion, Megvii, BEVNet, camera + lidar]</li>
  <li><a href="https://arxiv.org/abs/1808.07217">Don’t Use Large Mini-Batches, Use Local SGD</a> <kbd>ICLR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2201.02177">Grokking: Generalization beyond Overfitting on small algorithmic datasets</a></li>
  <li><a href="">Progress measures for grokking via mechanistic interpretability</a></li>
  <li><a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a> <kbd>ICLR 2017</kbd></li>
  <li><a href="https://openreview.net/forum?id=JqtHMZtqWm">Unifying Grokking and Double DescentDownload</a></li>
  <li><a href="https://arxiv.org/abs/2204.02392">Deep Interactive Motion Prediction and Planning: Playing Games with Motion Prediction Models</a> <kbd>L4DC 2022</kbd></li>
  <li><a href="https://escholarship.org/uc/item/0vf4q2x1">Interactive Prediction and Planning for Autonomous Driving: from Algorithms to Fundamental Aspects</a> [PhD thesis of Wei Zhan, 2019]</li>
  <li><a href="https://arxiv.org/abs/2006.14480">Lyft1001: One Thousand and One Hours: Self-driving Motion Prediction Dataset</a> [Lyft Level 5, prediction dataset]</li>
</ul>

<h2 id="2023-04-1">2023-04 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2212.10156">UniAD: Planning-oriented Autonomous Driving</a> [<a href="paper_notes/uniad.md">Notes</a>] [BEV, e2e, Hongyang Li]</li>
</ul>

<h2 id="2023-03-5">2023-03 (5)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2303.08774">GPT-4 Technical Report</a> [<a href="/Learning-Deep-Learning/paper_notes/gpt4.html">Notes</a>] [OpenAI, GPT]</li>
  <li><a href="https://arxiv.org/abs/2303.03991">OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception</a> [<a href="/Learning-Deep-Learning/paper_notes/openoccupancy.html">Notes</a>] [Occupancy Network, Jiwen Lu]</li>
  <li><a href="https://arxiv.org/abs/2302.12251">VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion</a> [<a href="/Learning-Deep-Learning/paper_notes/voxformer.html">Note</a>] <kbd>CVPR 2023 highlight</kbd> [Occupancy Network, Nvidia]</li>
  <li><a href="https://arxiv.org/abs/2112.00726">MonoScene: Monocular 3D Semantic Scene Completion</a> <kbd>CVPR 2022</kbd> [<a href="/Learning-Deep-Learning/paper_notes/monoscene.html">Notes</a>] [Occupancy Network, single cam]</li>
  <li><a href="https://arxiv.org/abs/2004.12989">CoReNet: Coherent 3D scene reconstruction from a single RGB image</a> [<a href="/Learning-Deep-Learning/paper_notes/corenet.html">Notes</a>] <kbd>ECCV 2020 oral</kbd></li>
</ul>

<h2 id="2023-02-4">2023-02 (4)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2211.04325">Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/out_of_data.html">Notes</a>] [Epoch.ai industry report]</li>
  <li><a href="https://arxiv.org/abs/2107.03374">Codex: Evaluating Large Language Models Trained on Code</a> [<a href="/Learning-Deep-Learning/paper_notes/codex.html">Notes</a>] [GPT, OpenAI]</li>
  <li><a href="https://arxiv.org/abs/2203.02155">InstructGPT: Training language models to follow instructions with human feedback</a> [<a href="/Learning-Deep-Learning/paper_notes/instructgpt.html">Notes</a>] [GPT, OpenAI]</li>
  <li><a href="https://arxiv.org/abs/2302.07817">TPVFormer: Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/tpvformer.html">Notes</a>] <kbd>CVPR 2023</kbd> [Occupancy Network, Jiwen Lu]</li>
</ul>

<h2 id="2023-01-2">2023-01 (2)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2301.01006">PPGeo: Policy Pre-training for End-to-end Autonomous Driving via Self-supervised Geometric Modeling</a> [<a href="/Learning-Deep-Learning/paper_notes/ppgeo.html">Notes</a>] <kbd>ICLR 2023</kbd></li>
  <li><a href="https://arxiv.org/abs/2106.11810">nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles</a> [<a href="/Learning-Deep-Learning/paper_notes/nuplan.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/2207.02202">CoBEVT: Cooperative Bird’s Eye View Semantic Segmentation with Sparse Transformers</a> <kbd>CoRL 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2211.10439">BEVFormer v2: Adapting Modern Image Backbones to Bird’s-Eye-View Recognition via Perspective Supervision</a> [BEVNet, Jifeng Dai]</li>
  <li><a href="https://ml4ad.github.io/files/papers2022/Fast-BEV:%20Towards%20Real-time%20On-vehicle%20Bird's-Eye%20View%20Perception.pdf">Fast-BEV: Towards Real-time On-vehicle Bird’s-Eye View Perception</a> <kbd>NeurIPS 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.03639">Traj++: Human Trajectory Forecasting in Crowds: A Deep Learning Perspective</a> <kbd>TITS 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.06715">Data Driven Prediction Architecture for Autonomous Driving and its Application on Apollo Platform</a> <kbd>IV 2020</kbd> [Baidu]</li>
  <li><a href="https://arxiv.org/abs/2110.06607">THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling</a> <kbd>ICLR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.13732">Learning Lane Graph Representations for Motion Forecasting</a> <kbd>ECCV 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.09959">Identifying Driver Interactions via Conditional Behavior Prediction</a> <kbd>ICRA 2021</kbd> [Waymo]</li>
  <li><a href="https://arxiv.org/abs/2001.03093">Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.12255">TPNet: Trajectory Proposal Network for Motion Prediction</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2109.01827">GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation</a></li>
  <li><a href="https://arxiv.org/abs/2004.02025">PECNet: It Is Not the Journey but the Destination: Endpoint Conditioned Trajectory Prediction</a> <kbd>ECCV 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2012.01526">From Goals, Waypoints &amp; Paths To Long Term Human Trajectory Forecasting</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1905.01296">PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.11476">PiP: Planning-informed Trajectory Prediction for Autonomous Driving</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1910.05449">MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction</a> <kbd>CoRL 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.00249">LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.04027">PRIME: Learning to Predict Vehicle Trajectories with Model-based Planning</a> <kbd>CoRL 2021<kbd></kbd></kbd></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1109/TITS.2020.3016304">A Flexible and Explainable Vehicle Motion Prediction and Inference Framework Combining Semi-Supervised AOG and ST-LSTM</a> <kbd>TITS 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1805.05499">Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs</a> <kbd>IV 2018</kbd> [Trivedi]</li>
  <li><a href="https://arxiv.org/abs/2110.02344">HYPER: Learned Hybrid Trajectory Prediction via Factored Inference and Adaptive Sampling</a> <kbd>ICRA 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2110.09741">Trajectory Prediction with Linguistic Representations</a> <kbd>ICRA 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2008.10587">What-If Motion Prediction for Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/2008.05927">End-to-end Contextual Perception and Prediction with Interaction Transformer</a> <kbd>IROS 2020</kbd> [Auxiliary collision loss, scene compliant pred]</li>
  <li><a href="https://arxiv.org/abs/1910.06673">SafeCritic: Collision-Aware Trajectory Prediction</a> <kbd>BMVC 2019</kbd> [IRL, scene compliant pred]</li>
  <li><a href="https://arxiv.org/abs/2104.10133">Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset</a> <kbd>ICCV 2021</kbd> [Waymo]</li>
  <li><a href="https://arxiv.org/abs/2009.12916">Interaction-Based Trajectory Prediction Over a Hybrid Traffic Graph</a> <kbd>IROS 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.07882">Joint Interaction and Trajectory Prediction for Autonomous Driving using Graph Neural Networks</a> <kbd>NeurIPS 2019 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2005.13458">Fast Risk Assessment for Autonomous Vehicles Using Learned Models of Agent Futures</a> <kbd>Robotics: science and systems 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2106.15796">Monocular 3D Object Detection: An Extrinsic Parameter Free Approach</a> <kbd>CVPR 2021</kbd> [PJLab]</li>
  <li><a href="https://arxiv.org/abs/2207.08536">UniFormer: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird’s-Eye-View</a> [BEVFormer, BEVNet, Temporal]</li>
  <li><a href="">GitNet: geometric prior-baesd transformation for birds yee view segmentation</a></li>
  <li><a href="">WBF: weighted box fusion: ensembling boxes from differnt object detection modules</a></li>
  <li><a href="">NNI: auto parameter finding algorithm</a></li>
  <li><a href="https://storage.googleapis.com/waymo-uploads/files/research/3DCam/3DCam_BEVFormer.pdf">BEVFormer++: Improving BEVFormer for 3D Camera-only Object Detection</a> [Waymo open dataset challenge 1st place in mono3d]</li>
  <li><a href="https://arxiv.org/abs/2206.07705">LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for Camera-Only 3D Detection</a> [Waymo open dataset challenge official metric]</li>
  <li><a href="https://pdfs.semanticscholar.org/44ac/01c0d356f22e7ee883f8e4ac2cccf199f68d.pdf">High-Level Interpretation of Urban Road Maps Fusing Deep Learning-Based Pixelwise Scene Segmentation and Digital Navigation Maps</a> <kbd>Journal of Advanced Transportation 2018</kbd></li>
  <li><a href="https://downloads.hindawi.com/journals/jat/2017/7090549.pdf">A Hybrid Vision-Map Method for Urban Road Detection</a> <kbd>Journal of Advanced Transportation 2017</kbd></li>
  <li><a href="https://www.researchgate.net/profile/Christopher-Plachetka/publication/348367176_Terminology_and_Analysis_of_Map_Deviations_in_Urban_Domains_Towards_Dependability_for_HD_Maps_in_Automated_Vehicles/links/607d523f907dcf667babc06b/Terminology-and-Analysis-of-Map-Deviations-in-Urban-Domains-Towards-Dependability-for-HD-Maps-in-Automated-Vehicles.pdf">Terminology and Analysis of Map Deviations in Urban Domains: Towards Dependability for HD Maps in Automated Vehicles</a> <kbd>IV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2210.02443">TIME WILL TELL: NEW OUTLOOKS AND A BASELINE FOR TEMPORAL MULTI-VIEW 3D OBJECT DETECTION</a></li>
  <li><a href="https://arxiv.org/abs/2108.06152">Conditional DETR for Fast Training Convergence</a> <kbd>ICCV 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2201.12329">DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR</a> <kbd>ICLR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2203.01305">DN-DETR: Accelerate DETR Training by Introducing Query DeNoising</a> <kbd>CVPR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2203.03605">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2202.01478">Trajectory Forecasting from Detection with Uncertainty-Aware Motion Encoding</a> [Ouyang Wanli]</li>
  <li><a href="https://arxiv.org/abs/2207.01878">Vision-based Uneven BEV Representation Learning with Polar Rasterization and Surface Estimation</a> [BEVNet, polar]</li>
  <li><a href="https://arxiv.org/abs/2205.00613">MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries</a> [BEVNet, tracking] <kbd>CVPR 2022 workshop</kbd> [Hang Zhao]</li>
  <li><a href="https://arxiv.org/abs/2207.07601">ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning</a> <kbd>ECCV 2022</kbd> [Hongyang Li]</li>
  <li><a href="https://arxiv.org/abs/2206.04584">GKT: Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer</a> [BEVNet, Horizon]</li>
  <li><a href="https://yan-junjie.github.io/publication/dblp-confcvpr-li-ywzh-18/dblp-confcvpr-li-ywzh-18.pdf">SiamRPN: High Performance Visual Tracking with Siamese Region Proposal Network</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2112.10155">TPLR: Topology Preserving Local Road Network Estimation from Single Onboard Camera Image</a> <kbd>CVPR 2022</kbd> [STSU, Luc Van Gool]</li>
  <li><a href="https://arxiv.org/abs/2206.13294">LaRa: Latents and Rays for Multi-Camera Bird’s-Eye-View Semantic Segmentation</a> [Valeo, BEVNet, polar]</li>
  <li><a href="https://arxiv.org/abs/2206.10965">PolarDETR: Polar Parametrization for Vision-based Surround-View 3D Detection</a> [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2104.05858">Exploring Geometric Consistency for Monocular 3D Object Detection</a> <kbd>CVPR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2106.01178">ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection</a> <kbd>WACV 2022</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2112.15351">Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints</a> <kbd>AAAI 2022</kbd></li>
  <li><a href="https://ieeexplore.ieee.org/document/9294383">Detecting Lane and Road Markings at A Distance with Perspective Transformer Layers</a> <kbd>ICICN 2021</kbd> [BEVNet, lane line]</li>
  <li><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/CVRSUAD/Behrendt_Unsupervised_Labeled_Lane_Markers_Using_Maps_ICCVW_2019_paper.pdf">Unsupervised Labeled Lane Markers Using Maps</a> <kbd>ICCV 2019 workshop</kbd> [Bosch, 2D lane line]</li>
  <li><a href="https://arxiv.org/abs/2104.11896">M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers</a> [Lidar detection, Waymo open dataset] <kbd>WACV 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2110.11048">K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways</a> [lane line dataset]</li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9506296">Robust Monocular 3D Lane Detection With Dual Attention</a> <kbd>ICIP 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2203.07977">OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic 3D Reconstruction</a> <kbd>CVPR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2110.02178">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a> <kbd>ICLR 2022</kbd> [lightweight Transformers]</li>
  <li><a href="https://arxiv.org/abs/2207.07268">XFormer: Lightweight Vision Transformer with Cross Feature Attention</a> [Samsung]</li>
  <li><a href="https://arxiv.org/abs/2209.05588">CenterFormer: Center-based Transformer for 3D Object Detection</a> <kbd>ECCV 2022 oral</kbd> [TuSimple]</li>
  <li><a href="https://arxiv.org/abs/2209.09385">LidarMultiNet: Towards a Unified Multi-task Network for LiDAR Perception</a> [2022 Waymo Open Dataset, TuSimple]</li>
  <li><a href="https://storage.googleapis.com/waymo-uploads/files/research/MotionPred/MotionPrediction_MTRA.pdf">MTRA: 1st Place Solution for 2022 Waymo Open Dataset Challenge - Motion Prediction</a> [Waymo open dataset challenge 1st place in motion prediction]</li>
  <li><a href="https://arxiv.org/abs/2203.04050">BEVSegFormer: Bird’s Eye View Semantic Segmentation From Arbitrary Camera Rigs</a> [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2109.03814">Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</a> <kbd>CVPR 2022</kbd> [nVidia]</li>
  <li><a href="https://arxiv.org/abs/2109.04617">Efficiently Identifying Task Groupings for Multi-Task Learning</a> <kbd>NeurIPS 2021 spotlight</kbd> [MTL]</li>
  <li><a href="https://arxiv.org/abs/2203.05482">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</a> [Google, Golden Backbone]</li>
  <li><a href="https://arxiv.org/abs/2204.02944">“The Pedestrian next to the Lamppost” Adaptive Object Graphs for Better Instantaneous Mapping</a> <kbd>CVPR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2204.07733">GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation</a> [BEVNet, Baidu]</li>
  <li><a href="https://arxiv.org/abs/2203.10642">FUTR3D: A Unified Sensor Fusion Framework for 3D Detection</a> [Hang Zhao]</li>
  <li><a href="https://arxiv.org/abs/2204.07733">GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation</a> [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2205.11083">MonoFormer: Towards Generalization of self-supervised monocular depth estimation with Transformers</a> [monodepth]</li>
  <li><a href="https://arxiv.org/abs/2205.14882">Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/2202.08791">cosFormer: Rethinking Softmax in Attention</a> <kbd>ICLR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2203.13641">StretchBEV: Stretching Future Instance Prediction Spatially and Temporally</a> [BEVNet, prediction]</li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Zhao_Scene_Representation_in_Birds-Eye_View_From_Surrounding_Cameras_With_Transformers_CVPRW_2022_paper.pdf">Scene Representation in Bird’s-Eye View from Surrounding Cameras with Transformers</a> [BEVNet, LLD] <kbd>CVPR 2022 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2204.07616">Multi-Frame Self-Supervised Depth with Transformers</a> <kbd>CVPR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2111.09162">It’s About Time: Analog Clock Reading in the Wild</a> <kbd>CVPR 2022</kbd> [Andrew Zisserman]</li>
  <li><a href="https://arxiv.org/abs/2204.03636">SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation</a> [Jiwen Lu]</li>
  <li><a href="https://arxiv.org/abs/2205.00301">ONCE-3DLanes: Building Monocular 3D Lane Detection</a> <kbd>CVPR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2110.11048">K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways</a> <kbd>CVPR 2022 workshop</kbd> [3D LLD]</li>
  <li><a href="https://arxiv.org/abs/2112.12141">Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving</a> <kbd>CVPR 2022 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2206.07959">A Simple Baseline for BEV Perception Without LiDAR</a> [TRI, BEVNet, vision+radar]</li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Li_Reconstruct_From_Top_View_A_3D_Lane_Detection_Approach_Based_CVPRW_2022_paper.pdf">Reconstruct from Top View: A 3D Lane Detection Approach based on Geometry Structure Prior</a> <kbd>CVPR 2022 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2206.01738">RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding</a> <kbd>CVPR 2022</kbd> [Waymo, Charles Qi]</li>
  <li><a href="https://arxiv.org/abs/2203.03875">Occupancy Flow Fields for Motion Forecasting in Autonomous Driving</a> <kbd>RAL 2022</kbd> [Waymo occupancy flow challenge]</li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.pdf">Safe Local Motion Planning with Self-Supervised Freespace Forecasting</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://zhuanlan.zhihu.com/p/533907821">数据闭环的核心 - Auto-labeling 方案分享</a></li>
  <li><a href="https://arxiv.org/abs/2110.11048">K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways</a></li>
  <li><a href="https://arxiv.org/abs/2101.01909">LETR: Line Segment Detection Using Transformers without Edges</a> <kbd>CVPR 2021 oral</kbd></li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mi_HDMapGen_A_Hierarchical_Graph_Generative_Model_of_High_Definition_Maps_CVPR_2021_paper.pdf">HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps</a> <kbd>CVPR 2021</kbd> [HD mapping]</li>
  <li><a href="https://arxiv.org/abs/1704.03477">SketchRNN: A Neural Representation of Sketch Drawings</a> [David Ha]</li>
  <li><a href="https://arxiv.org/abs/2002.10880">PolyGen: An Autoregressive Generative Model of 3D Meshes</a> <kbd>ICML 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2106.02351">SOLQ: Segmenting Objects by Learning Queries</a> <kbd>NeurlPS 2021</kbd> [Megvii, end-to-end, instance segmentation]</li>
  <li><a href="https://arxiv.org/abs/2208.03543">MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer</a> <kbd>3DV 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2204.07346">MVSTER: Epipolar Transformer for Efficient Multi-View Stereo</a> <kbd>ECCV 2022&lt;/bd&gt;</kbd></li>
  <li><a href="https://arxiv.org/abs/2208.09170">MOVEDepth: Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning</a> [MVS + monodepth]</li>
  <li><a href="https://arxiv.org/abs/2204.03636">SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation</a></li>
  <li><a href="https://arxiv.org/abs/2106.08417">Scene Transformer: A unified architecture for predicting multiple agent trajectories</a> [prediction, Waymo] <kbd>ICLR 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.05821">SSIA: Monocular Depth Estimation with Self-supervised Instance Adaptation</a> [VGG team, TTR, test time refinement, CVD]</li>
  <li><a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kuznietsov_CoMoDA_Continuous_Monocular_Depth_Adaptation_Using_Past_Experiences_WACV_2021_paper.pdf">CoMoDA: Continuous Monocular Depth Adaptation Using Past Experiences</a> <kbd>WACV 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2011.11814">MonoRec: Semi-supervised dense reconstruction in dynamic environments from a single moving camera</a> <kbd>CVPR 2021</kbd> [Daniel Cremmers]</li>
  <li><a href="https://arxiv.org/abs/2112.05131">Plenoxels: Radiance Fields without Neural Networks</a></li>
  <li><a href="https://arxiv.org/abs/2111.09497">Lidar with Velocity: Motion Distortion Correction of Point Clouds from Oscillating Scanning Lidars</a> [Livox, ISEE]</li>
  <li><a href="https://arxiv.org/abs/2110.13389">NWD: A Normalized Gaussian Wasserstein Distance for Tiny Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2111.07971">Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation</a> <kbd>NeurIPS 2021</kbd> [Sanja Fidler]</li>
  <li><a href="https://arxiv.org/abs/2102.02629">Insta-DM: Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency</a> <kbd>AAAI 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.09351">Instance-wise Depth and Motion Learning from Monocular Videos</a> <kbd>NeurIPS 2020 workshop</kbd> [<a href="https://sites.google.com/site/seokjucv/home/instadm">website</a>]</li>
  <li><a href="https://arxiv.org/abs/2003.08934">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a> <kbd>ECCV 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.06405">BARF: Bundle-Adjusting Neural Radiance Fields</a> <kbd>ICCV 2021 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2109.01129">NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</a> <kbd>ICCV 2021 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.09224">Transfuser: Multi-Modal Fusion Transformer for End-to-End Autonomous Driving</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.14420">YOLinO: Generic Single Shot Polyline Detection in Real Time</a> <kbd>ICCV 2021 workshop</kbd> [lld]</li>
  <li><a href="https://arxiv.org/abs/2104.03775">MonoRCNN: Geometry-based Distance Decomposition for Monocular 3D Object Detection</a> <kbd>ICCV 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2110.00464">MonoCInIS: Camera Independent Monocular 3D Object Detection using Instance Segmentation</a> <kbd>ICCV 2021 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.13192">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</a> <kbd>CVPR 2020</kbd> [Waymo challenge 2nd place]</li>
  <li><a href="https://arxiv.org/abs/2104.03775">Geometry-based Distance Decomposition for Monocular 3D Object Detection</a> <kbd>ICCV 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2103.05073">Offboard 3D Object Detection from Point Cloud Sequences</a> <kbd>CVPR 2021</kbd> [Charles Qi]</li>
  <li><a href="https://arxiv.org/abs/1909.02466">FreeAnchor: Learning to Match Anchors for Visual Object Detection</a> <kbd>NeurIPS 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.03496">AutoAssign: Differentiable Label Assignment for Dense Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2007.08103">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2108.12102">FOVEA: Foveated Image Magnification for Autonomous Navigation</a> <kbd>ICCV 2021</kbd> [Argo]</li>
  <li><a href="https://arxiv.org/abs/1903.06593">PifPaf: Composite Fields for Human Pose Estimation</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://avvision.xyz/iccv21/papers/1/CameraReady/01.pdf">Monocular 3D Localization of Vehicles in Road Scenes</a> <kbd>ICCV 2021 workshop</kbd> [mono3D, tracking]</li>
  <li><a href="https://arxiv.org/abs/2107.02191">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</a></li>
  <li><a href="https://arxiv.org/abs/2108.06152">Conditional DETR for Fast Training Convergence</a></li>
  <li><a href="https://arxiv.org/abs/2109.07107">Anchor DETR: Query Design for Transformer-Based Detector</a> [megvii]</li>
  <li><a href="https://arxiv.org/abs/2107.14160">PGD: Probabilistic and Geometric Depth: Detecting Objects in Perspective</a> <kbd>CoRL 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.07399">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</a></li>
  <li><a href="https://proceedings.mlr.press/v139/sun21b.html">What Makes for End-to-End Object Detection?</a> <kbd>PMLR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2105.01928">Instances as Queries</a> <kbd>ICCV 2021</kbd> [instance segmentation]</li>
  <li><a href="https://arxiv.org/abs/2106.11037">One Million Scenes for Autonomous Driving: ONCE Dataset</a> [Huawei]</li>
  <li><a href="https://arxiv.org/abs/2112.12577">NVS-MonoDepth: Improving Monocular Depth Prediction with Novel View Synthesis</a> <kbd>3DV 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2107.03332">Is 2D Heatmap Representation Even Necessary for Human Pose Estimation?</a></li>
  <li><a href="https://arxiv.org/abs/2112.10155">Topology Preserving Local Road Network Estimation from Single Onboard Camera Image</a> [BEVNet, Luc Van Gool]</li>
</ul>

<h2 id="2022-11-1">2022-11 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2202.11884">M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/m2i.html">Notes</a>] <kbd>CVPR 2022</kbd></li>
</ul>

<h2 id="2022-10-1">2022-10 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2209.05324">Delving into the Devils of Bird’s-eye-view Perception: A Review, Evaluation and Recipe</a> [<a href="/Learning-Deep-Learning/paper_notes/delving_bev.html">Notes</a>] [PJLab]</li>
</ul>

<h2 id="2022-09-3">2022-09 (3)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2208.01582">ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries</a> [<a href="/Learning-Deep-Learning/paper_notes/vip3d.html">Notes</a>] [BEV, perception + prediction, Hang Zhao]</li>
  <li><a href="https://arxiv.org/abs/2208.14437">MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction</a> [<a href="/Learning-Deep-Learning/paper_notes/maptr.html">Notes</a>] [Horizon, BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2206.00991">StopNet: Scalable Trajectory and Occupancy Prediction for Urban Autonomous Driving</a> <kbd>ICRA 2022</kbd></li>
  <li><a href="https://arxiv.org/abs/2105.03247">MOTR: End-to-End Multiple-Object Tracking with Transformer</a> <kbd>ECCV 2022</kbd> [Megvii, MOT]</li>
  <li><a href="https://arxiv.org/abs/2109.07107">Anchor DETR: Query Design for Transformer-Based Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/anchor_detr.html">Notes</a>] <kbd>AAAI 2022</kbd> [Megvii]</li>
</ul>

<h2 id="2022-08-1">2022-08 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2105.10968">HOME: Heatmap Output for future Motion Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/home.html">Notes</a>] <kbd>ITSC 2021</kbd> [behavior prediction, Huawei Paris]</li>
</ul>

<h2 id="2022-07-8">2022-07 (8)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2203.11089">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</a> [<a href="/Learning-Deep-Learning/paper_notes/persformer.html">Notes</a>] [BEVNet, lane line]</li>
  <li><a href="https://arxiv.org/abs/2206.08920">VectorMapNet: End-to-end Vectorized HD Map Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/vectormapnet.html">Notes</a>] [BEVNet, LLD, Hang Zhao]</li>
  <li><a href="https://arxiv.org/abs/2203.05625">PETR: Position Embedding Transformation for Multi-View 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/petr.html">Notes</a>] <kbd>ECCV 2022</kbd> [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2206.01256">PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images</a> [<a href="/Learning-Deep-Learning/paper_notes/petrv2.html">Notes</a>] [BEVNet, MegVii]</li>
  <li><a href="https://arxiv.org/abs/2204.05088">M^2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation</a> [<a href="/Learning-Deep-Learning/paper_notes/m2bev.html">Notes</a>] [BEVNet, nvidia]</li>
  <li><a href="https://arxiv.org/abs/2206.10092">BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/bevdepth.html">Notes</a>] [BEVNet, NuScenes SOTA, Megvii]</li>
  <li><a href="https://arxiv.org/abs/2205.02833">CVT: Cross-view Transformers for real-time Map-view Semantic Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/cvt.html">Notes</a>] <kbd>CVPR 2022 oral</kbd> [UTAustin, Philipp]</li>
  <li><a href="https://arxiv.org/abs/2207.05844">Wayformer: Motion Forecasting via Simple &amp; Efficient Attention Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/wayformer.html">Notes</a>] [Behavior prediction, Waymo]</li>
</ul>

<h2 id="2022-06-3">2022-06 (3)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2203.17054">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/bevdet4d.html">Notes</a>] [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2205.09743">BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/beverse.html">Notes</a>] [Jiwen Lu, BEVNet, perception + prediction]</li>
  <li><a href="https://arxiv.org/abs/2205.13542">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation</a> [<a href="/Learning-Deep-Learning/paper_notes/bevfusion.html">Notes</a>] [BEVNet, Han Song]</li>
</ul>

<h2 id="2022-03-1">2022-03 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2203.17270">BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a> [<a href="/Learning-Deep-Learning/paper_notes/bevformer.html">Notes</a>] [BEVNet, Hongyang Li, Jifeng Dai]</li>
</ul>

<h2 id="2022-02-1">2022-02 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2008.08294">TNT: Target-driveN Trajectory Prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/tnt.html">Notes</a>] <kbd>CoRL 2020</kbd> [prediction, Waymo, Hang Zhao]</li>
  <li><a href="https://arxiv.org/abs/2108.09640">DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets</a> [<a href="paper_notes/dense_tnt.md">Notes</a>] <kbd>ICCV 2021</kbd> [prediction, Waymo, 1st place winner WOMD]</li>
</ul>

<h2 id="2022-01-1">2022-01 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2104.14540">Manydepth: The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth</a> [<a href="/Learning-Deep-Learning/paper_notes/manydepth.html">Notes</a>] <kbd>CVPR 2021</kbd> [monodepth, Niantic]</li>
  <li><a href="https://arxiv.org/abs/2104.02300">DEKR: Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression</a> [<a href="paper_notes/dekr.md">Notes</a>] <kbd>CVPR 2021</kbd></li>
</ul>

<h2 id="2021-12-5">2021-12 (5)</h2>
<ul>
  <li><a href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf">BN-FFN-BN: Leveraging Batch Normalization for Vision Transformers</a> [<a href="/Learning-Deep-Learning/paper_notes/bn_ffn_bn.html">Notes</a>] <kbd>ICCVW 2021</kbd> [BN, transformers]</li>
  <li><a href="https://arxiv.org/abs/2003.07845">PowerNorm: Rethinking Batch Normalization in Transformers</a> [<a href="/Learning-Deep-Learning/paper_notes/powernorm.html">Notes</a>] <kbd>ICML 2020</kbd> [BN, transformers]</li>
  <li><a href="https://arxiv.org/abs/2111.14973">MultiPath++: Efficient Information Fusion and Trajectory Aggregation for Behavior Prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/multipath++.html">Notes</a>] <kbd>ICRA 2022</kbd> [Waymo, behavior prediction]</li>
  <li><a href="https://arxiv.org/abs/2112.11790">BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View</a> [<a href="paper_note/bevdet.md">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/2110.00966">Translating Images into Maps</a> [<a href="/Learning-Deep-Learning/paper_notes/translating_images_to_maps.html">Notes</a>] <kbd>ICRA 2022</kbd> [BEVNet, transformers]</li>
</ul>

<h2 id="2021-11-4">2021-11 (4)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2110.06922">DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries</a> [<a href="/Learning-Deep-Learning/paper_notes/detr3d.html">Notes</a>] <kbd>CoRL 2021</kbd> [BEVNet, transformers]</li>
  <li><a href="https://arxiv.org/abs/2012.05901">Robust-CVD: Robust Consistent Video Depth Estimation</a> <kbd>CVPR 2021 oral</kbd> [<a href="https://robust-cvd.github.io/">website</a>]</li>
  <li><a href="https://arxiv.org/abs/2111.06377">MAE: Masked Autoencoders Are Scalable Vision Learners</a> [<a href="/Learning-Deep-Learning/paper_notes/mae.html">Notes</a>] [Kaiming He, unsupervised learning]</li>
  <li><a href="https://arxiv.org/abs/2111.09886">SimMIM: A Simple Framework for Masked Image Modeling</a> [<a href="/Learning-Deep-Learning/paper_notes/simmim.html">Notes</a>] [MSRA, unsupervised learning, MAE]</li>
  <li><a href="https://arxiv.org/abs/2111.07832">iBOT: Image BERT Pre-Training with Online Tokenizer</a></li>
</ul>

<h2 id="2021-10-3">2021-10 (3)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2110.01997">STSU: Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images</a> [<a href="/Learning-Deep-Learning/paper_notes/stsu.html">Notes</a>] <kbd>ICCV 2021</kbd> [BEV feat stitching, Luc Van Gool]</li>
  <li><a href="https://arxiv.org/abs/2108.03227">PanopticBEV: Bird’s-Eye-View Panoptic Segmentation Using Monocular Frontal View Images</a> [<a href="/Learning-Deep-Learning/paper_notes/panoptic_bev.html">Notes</a>] <kbd>RAL 2022</kbd> [BEVNet, vertical/horizontal features]</li>
  <li><a href="https://arxiv.org/abs/2109.04456">NEAT: Neural Attention Fields for End-to-End Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/neat.html">Notes</a>] <kbd>ICCV 2021</kbd> [<a href="http://www.cvlibs.net/publications/Chitta2021ICCV_supplementary.pdf">supplementary</a>] [BEVNet]</li>
</ul>

<h2 id="2021-09-11">2021-09 (11)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2108.06417">DD3D: Is Pseudo-Lidar needed for Monocular 3D Object detection?</a> [<a href="paper_notes/dd3d.md">Notes</a>] <kbd>ICCV 2021</kbd> [mono3D, Toyota]</li>
  <li><a href="https://arxiv.org/abs/1911.09070">EfficientDet: Scalable and Efficient Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/efficientdet.html">Notes</a>] <kbd>CVPR 2020</kbd> [BiFPN, Tesla AI day]</li>
  <li><a href="https://arxiv.org/abs/2005.14711">PnPNet: End-to-End Perception and Prediction with Tracking in the Loop</a> [<a href="/Learning-Deep-Learning/paper_notes/pnpnet.html">Notes</a>] <kbd>CVPR 2020</kbd> [Uber ATG]</li>
  <li><a href="https://arxiv.org/abs/2101.06806">MP3: A Unified Model to Map, Perceive, Predict and Plan</a> [<a href="/Learning-Deep-Learning/paper_notes/mp3.html">Notes</a>] <kbd>CVPR 2021</kbd> [Uber, planning]</li>
  <li><a href="http://arxiv.org/abs/2110.04931">BEV-Net: Assessing Social Distancing Compliance by Joint People Localization and Geometric Reasoning</a> [<a href="/Learning-Deep-Learning/paper_notes/bevnet_sdca.html">Notes</a>] <kbd>ICCV 2021</kbd> [BEVNet, surveillance]</li>
  <li><a href="https://arxiv.org/abs/2103.15297">LiDAR R-CNN: An Efficient and Universal 3D Object Detector</a> [<a href="/Learning-Deep-Learning/paper_notes/lidar_rcnn.html">Notes</a>] <kbd>CVPR 2021</kbd> [TuSimple, Naiyan Wang]</li>
  <li><a href="https://arxiv.org/abs/2102.05897">Corner Cases for Visual Perception in Automated Driving: Some Guidance on Detection Approaches</a> [<a href="/Learning-Deep-Learning/paper_notes/corner_case_vision_arxiv.html">Notes</a>] [corner cases]</li>
  <li><a href="https://ieeexplore.ieee.org/document/9304789">Systematization of Corner Cases for Visual Perception in Automated Driving</a> [<a href="paper_notes/corner_case_vision_iv.md">Notes</a>] <kbd>IV 2020</kbd> [corner cases]</li>
  <li><a href="https://arxiv.org/abs/2103.03678">An Application-Driven Conceptualization of Corner Cases for Perception in Highly Automated Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/corner_case_multisensor.html">Notes</a>] <kbd>IV 2021</kbd> [corner cases]</li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.html">PYVA: Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-view Transformation</a> [<a href="/Learning-Deep-Learning/paper_notes/pyva.html">Notes</a>] <kbd>CVPR 2021</kbd> [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yang_Projecting_Your_View_CVPR_2021_supplemental.zip">Supplementary</a>] [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2103.09460">YOLOF: You Only Look One-level Feature</a> [<a href="/Learning-Deep-Learning/paper_notes/yolof.html">Notes</a>] <kbd>CVPR 2021</kbd> [megvii]</li>
  <li><a href="https://arxiv.org/abs/2009.00984">Perceiving Humans: from Monocular 3D Localization to Social Distancing</a> [<a href="/Learning-Deep-Learning/paper_notes/perceiving_humans.html">Notes</a>] <kbd>TITS 2021</kbd> [monoloco++]</li>
  <li><a href="https://arxiv.org/abs/1903.06593">PifPaf: Composite Fields for Human Pose Estimation</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2108.03227">Bird’s-Eye-View Panoptic Segmentation Using Monocular Frontal View Images</a> [BEVNet]</li>
  <li><a href="https://arxiv.org/abs/2107.02191">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</a></li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf">Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-view Transformation</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.09224">Multi-Modal Fusion Transformer for End-to-End Autonomous Driving</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2108.06152">Conditional DETR for Fast Training Convergence</a></li>
  <li><a href="https://arxiv.org/abs/2107.14160">Probabilistic and Geometric Depth: Detecting Objects in Perspective</a> <kbd>CoRL 2021</kbd></li>
</ul>

<h2 id="2021-08-11">2021-08 (11)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2011.08464">EgoNet: Exploring Intermediate Representation for Monocular Vehicle Pose Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/egonet.html">Notes</a>] <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2106.15796">MonoEF: Monocular 3D Object Detection: An Extrinsic Parameter Free Approach</a> [<a href="/Learning-Deep-Learning/paper_notes/monoef.html">Notes</a>] <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2102.00690">GAC: Ground-aware Monocular 3D Object Detection for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/gac.html">Notes</a>] <kbd>RAL 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2104.10956">FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/fcos3d.html">Notes</a>] <kbd>NeurIPS 2020</kbd> [mono3D, senseTime]</li>
  <li><a href="https://arxiv.org/abs/2107.13774">GUPNet: Geometry Uncertainty Projection Network for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/gupnet.html">Notes</a>] <kbd>ICCV 2021</kbd> [mono3D, Wanli Ouyang]</li>
  <li><a href="https://arxiv.org/abs/1806.09055">DARTS: Differentiable Architecture Search</a> [<a href="/Learning-Deep-Learning/paper_notes/darts.html">Notes</a>] <kbd>ICLR 2019</kbd> [VGG author]</li>
  <li><a href="https://arxiv.org/abs/1812.03443">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</a> [<a href="/Learning-Deep-Learning/paper_notes/fbnet.html">Notes</a>] <kbd>CVPR 20219</kbd> [DARTS]</li>
  <li><a href="https://arxiv.org/abs/2004.05565">FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.02049">FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.03206">Perceiver: General Perception with Iterative Attention</a> [<a href="/Learning-Deep-Learning/paper_notes/perceiver.html">Notes</a>] <kbd>ICML 2021</kbd> [transformers, multimodal]</li>
  <li><a href="https://arxiv.org/abs/2107.14795">Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs</a> [<a href="/Learning-Deep-Learning/paper_notes/perceiver_io.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/2104.08683">PillarMotion: Self-Supervised Pillar Motion Learning for Autonomous Driving</a>  [<a href="/Learning-Deep-Learning/paper_notes/pillar_motion.html">Notes</a>] <kbd>CVPR 2021</kbd> [Qcraft, Alan Yuille]</li>
  <li><a href="https://arxiv.org/abs/2108.10312">SimTrack: Exploring Simple 3D Multi-Object Tracking for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/simtrack.html">Notes</a>] <kbd>ICCV 2019</kbd> [QCraft, Alan Yuille]</li>
</ul>

<h2 id="2021-07-1">2021-07 (1)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2107.06307">HDMapNet: An Online HD Map Construction and Evaluation Framework</a> [<a href="/Learning-Deep-Learning/paper_notes/hdmapnet.html">Notes</a>] <kbd>CVPR 2021 workshop</kbd> [youtube video only, Li Auto]</li>
</ul>

<h2 id="2021-06-2">2021-06 (2)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2104.10490">FIERY: Future Instance Prediction in Bird’s-Eye View from Surround Monocular Cameras</a> [<a href="/Learning-Deep-Learning/paper_notes/fiery.html">Notes</a>] <kbd>ICCV 2021</kbd> [BEVNet, perception + prediction]</li>
  <li><a href="https://zhuanlan.zhihu.com/p/35034215">Baidu’s CNN seg</a> [<a href="/Learning-Deep-Learning/paper_notes/cnn_seg.html">Notes</a>]</li>
</ul>

<h2 id="2021-04-5">2021-04 (5)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2012.15175">Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/swahr.html">Notes</a>] <kbd>CVPR 2021</kbd> [megvii]</li>
  <li><a href="https://arxiv.org/abs/1812.00324">CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://openaccess.thecvf.com/content_WACV_2020/html/Dhamija_The_Overlooked_Elephant_of_Object_Detection_Open_Set_WACV_2020_paper.html">The Overlooked Elephant of Object Detection: Open Set</a> <kbd>WACV 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2011.14204">Class-Agnostic Object Detection</a> <kbd>WACV 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.02603">OWOD: Towards Open World Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/owod.html">Notes</a>] <kbd>CVPR 2021 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.06957">FsDet: Frustratingly Simple Few-Shot Object Detection</a> <kbd>ICML 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.02323">MonoFlex: Objects are Different: Flexible Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/monoflex.html">Notes</a>] <kbd>CVPR 2021</kbd> [mono3D, Jiwen Lu, cropped]</li>
  <li><a href="https://arxiv.org/abs/2103.16237">monoDLE: Delving into Localization Errors for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/monodle.html">Notes</a>] <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2104.10786">Exploring 2D Data Augmentation for 3D Monocular Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2104.06041">OCM3D: Object-Centric Monocular 3D Object Detection</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2104.00152">FSM: Full Surround Monodepth from Multiple Cameras</a> [<a href="/Learning-Deep-Learning/paper_notes/fsm.html">Notes</a>] <kbd>ICRA 2021</kbd> [monodepth, Xnet]</li>
</ul>

<h2 id="2021-03-4">2021-03 (4)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2103.01100">CaDDN: Categorical Depth Distribution Network for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/caddn.html">Notes</a>] <kbd>CVPR 2021 oral</kbd> [mono3D, BEVNet]</li>
  <li><a href="https://arxiv.org/abs/1801.07372">DSNT: Numerical Coordinate Regression with Convolutional Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/dsnt.html">Notes</a>] [differentiable spatial to numerical transform]</li>
  <li><a href="https://arxiv.org/abs/1710.02322">Soft-Argmax: Human pose regression by combining indirect part detection and contextual information</a></li>
  <li><a href="https://arxiv.org/abs/2102.06777">INSTA-YOLO: Real-Time Instance Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/insta_yolo.html">Notes</a>] <kbd>ICML workshop 2020</kbd> [single stage instance segmentation]</li>
  <li><a href="https://arxiv.org/abs/2103.07461">CenterNet2: Probabilistic two-stage detection</a> [<a href="/Learning-Deep-Learning/paper_notes/centernet2.html">Notes</a>] [CenterNet, two-stage]</li>
</ul>

<h2 id="2021-01-7">2021-01 (7)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2012.00257">Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/confluence.html">Notes</a>] [NMS]</li>
  <li><a href="https://arxiv.org/abs/2012.02310">BoxInst: High-Performance Instance Segmentation with Box Annotations</a> [<a href="/Learning-Deep-Learning/paper_notes/boxinst.html">Notes</a>] <kbd>CVPR 2021</kbd> [Chunhua Shen, Tian Zhi]</li>
  <li><a href="https://arxiv.org/abs/2002.10187">3DSSD: Point-based 3D Single Stage Object Detector</a> [<a href="/Learning-Deep-Learning/paper_notes/3dssd.html">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2101.03697">RepVGG: Making VGG-style ConvNets Great Again</a> [<a href="/Learning-Deep-Learning/paper_notes/repvgg.html">Notes</a>] [Megvii, Xiangyu Zhang, ACNet]</li>
  <li><a href="https://arxiv.org/abs/1908.03930">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a> [<a href="/Learning-Deep-Learning/paper_notes/acnet.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2012.03040">BEV-Feat-Stitching: Understanding Bird’s-Eye View Semantic HD-Maps Using an Onboard Monocular Camera</a> [<a href="/Learning-Deep-Learning/paper_notes/bev_feat_stitching.html">Notes</a>] [BEVNet, mono3D, Luc Van Gool]</li>
  <li><a href="https://arxiv.org/abs/2101.11782">PSS: Object Detection Made Simpler by Eliminating Heuristic NMS</a> [<a href="/Learning-Deep-Learning/paper_notes/pss.html">Notes</a>] [Transformer, DETR]</li>
</ul>

<h2 id="2020-12-17">2020-12 (17)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2012.03544">DeFCN: End-to-End Object Detection with Fully Convolutional Network</a> [<a href="/Learning-Deep-Learning/paper_notes/defcn.html">Notes</a>] [Transformer, DETR]</li>
  <li><a href="https://arxiv.org/abs/2012.05780">OneNet: End-to-End One-Stage Object Detection by Classification Cost</a> [<a href="/Learning-Deep-Learning/paper_notes/onenet.html">Notes</a>] [Transformer, DETR]</li>
  <li><a href="http://driving.stanford.edu/papers/ICRA2011.pdf">Traffic Light Mapping, Localization, and State Detection for Autonomous Vehicles</a> [<a href="/Learning-Deep-Learning/paper_notes/tfl_stanford.html">Notes</a>] <kbd>ICRA 2011</kbd> [traffic light, Sebastian Thrun]</li>
  <li><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43966.pdf">Towards lifelong feature-based mapping in semi-static environments</a> [<a href="/Learning-Deep-Learning/paper_notes/lifelong_feature_mapping_google.html">Notes</a>] <kbd>ICRA 2016</kbd></li>
  <li><a href="http://www.lewissoft.com/pdf/ICRA2020/1484.pdf">How to Keep HD Maps for Automated Driving Up To Date</a> [<a href="/Learning-Deep-Learning/paper_notes/keep_hd_maps_updated_bmw.html">Notes</a>] <kbd>ICRA 2020</kbd> [BMW]</li>
  <li><a href="https://arxiv.org/abs/2011.12885">Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/gfocalv2.html">Notes</a>] <kbd>CVPR 2021</kbd> [focal loss]</li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/Milz_Visual_SLAM_for_CVPR_2018_paper.pdf">Visual SLAM for Automated Driving: Exploring the Applications of Deep Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/vslam_for_ad.html">Notes</a>] <kbd>CVPR 2018 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.09836">Centroid Voting: Object-Aware Centroid Voting for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/centroid_voting.html">Notes</a>] <kbd>IROS 2020</kbd> [mono3D, geometry + appearance = distance]</li>
  <li><a href="https://arxiv.org/abs/2003.03759">Monocular 3D Object Detection in Cylindrical Images from Fisheye Cameras</a> [<a href="/Learning-Deep-Learning/paper_notes/mono3d_fisheye.html">Notes</a>] [GM Israel, mono3D]</li>
  <li><a href="https://cslinzhang.github.io/deepps/parkingslot.pdf">DeepPS: Vision-Based Parking-Slot Detection: A DCNN-Based Approach and a Large-Scale Benchmark Dataset</a> <kbd>TIP 2018</kbd> [Parking slot detection, PS2.0 dataset]</li>
  <li><a href="https://arxiv.org/abs/2005.05528">PSDet: Efficient and Universal Parking Slot Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/psdet.html">Notes</a>] <kbd>IV 2020</kbd> [Zongmu, Parking slot detection]</li>
  <li><a href="https://arxiv.org/abs/2001.00138">PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning</a> [<a href="/Learning-Deep-Learning/paper_notes/patdnn.html">Notes</a>] <kbd>ASPLOS 2020</kbd> [pruning]</li>
  <li><a href="https://arxiv.org/abs/2011.08036">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a> [<a href="/Learning-Deep-Learning/paper_notes/scaled_yolov4.html">Notes</a>] [yolo]</li>
  <li><a href="https://github.com/ultralytics/yolov5">Yolov5 by Ultralytics</a> [<a href="/Learning-Deep-Learning/paper_notes/yolov5.html">Notes</a>] [yolo, spatial2channel]</li>
  <li><a href="https://arxiv.org/abs/2007.12099">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a> [<a href="/Learning-Deep-Learning/paper_notes/pp_yolo.html">Notes</a>] [yolo, paddle-paddle, baidu]</li>
  <li><a href="https://arxiv.org/pdf/1911.10150.pdf">PointPainting: Sequential Fusion for 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/point_painting.html">Notes</a>] [nuscenece]</li>
  <li><a href="https://arxiv.org/abs/2003.06754">MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird’s Eye View Maps</a> [<a href="/Learning-Deep-Learning/paper_notes/motionnet.html">Notes</a>] <kbd>CVPR 2020</kbd> [Unseen moving objects, BEV]</li>
  <li><a href="https://arxiv.org/abs/1806.07564">Locating Objects Without Bounding Boxes</a> [<a href="/Learning-Deep-Learning/paper_notes/objects_without_bboxes.html">Notes</a>] <kbd>CVPR 2019</kbd> [weighted Haussdorf distance, NMS-free]</li>
</ul>

<h2 id="2020-11-18">2020-11 (18)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2011.10881">TSP: Rethinking Transformer-based Set Prediction for Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/tsp.html">Notes</a>] <kbd>ICCV 2021</kbd> [DETR, transformers, Kris Kitani]</li>
  <li><a href="https://arxiv.org/abs/2011.12450">Sparse R-CNN: End-to-End Object Detection with Learnable Proposals</a> [<a href="/Learning-Deep-Learning/paper_notes/sparse_rcnn.html">Notes</a>] <kbd>CVPR 2020</kbd> [DETR, Transformer]</li>
  <li><a href="https://arxiv.org/abs/2010.16404">Unsupervised Monocular Depth Learning in Dynamic Scenes</a> [<a href="/Learning-Deep-Learning/paper_notes/learn_depth_and_motion.html">Notes</a>] <kbd>CoRL 2020</kbd> [LearnK improved ver, Google]</li>
  <li><a href="https://arxiv.org/abs/2006.16007">MoNet3D: Towards Accurate Monocular 3D Object Localization in Real Time</a> [<a href="/Learning-Deep-Learning/paper_notes/monet3d.html">Notes</a>] <kbd>ICML 2020</kbd> [Mono3D, pairwise relationship]</li>
  <li><a href="https://arxiv.org/abs/1911.02620">Argoverse: 3D Tracking and Forecasting with Rich Maps</a> [<a href="/Learning-Deep-Learning/paper_notes/argoverse.html">Notes</a>] <kbd>CVPR 2019</kbd> [HD maps, dataset, CV lidar]</li>
  <li><a href="https://arxiv.org/abs/1903.01568">The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes</a> [<a href="/Learning-Deep-Learning/paper_notes/h3d.html">Notes</a>] <kbd>ICRA 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.07864">Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection</a> <kbd>CVPRW 2020</kbd> [dataset, Daimler, mono3D]</li>
  <li><a href="https://www.cs.cornell.edu/~snavely/publications/papers/nyc3dcars_iccv13.pdf">NYC3DCars: A Dataset of 3D Vehicles in Geographic Context</a> <kbd>ICCV 2013</kbd></li>
  <li><a href="https://www.ri.cmu.edu/wp-content/uploads/2017/12/levinson-iv2011.pdf">Towards Fully Autonomous Driving: Systems and Algorithms</a> <kbd>IV 2011</kbd></li>
  <li><a href="https://arxiv.org/abs/2005.13423">Center3D: Center-based Monocular 3D Object Detection with Joint Depth Understanding</a> [<a href="/Learning-Deep-Learning/paper_notes/center3d.html">Notes</a>] [mono3D, LID+DepJoint]</li>
  <li><a href="https://arxiv.org/abs/2003.00529">ZoomNet: Part-Aware Adaptive Zooming Neural Network for 3D Object Detection</a> <kbd>AAAI 2020 oral</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2011.04841">CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/centerfusion.html">Notes</a>] <kbd>WACV 2021</kbd> [early fusion, camera, radar]</li>
  <li><a href="https://arxiv.org/abs/2011.01535">3D-LaneNet+: Anchor Free Lane Detection using a Semi-Local Representation</a> [<a href="/Learning-Deep-Learning/paper_notes/3d_lanenet+.html">Notes</a>] <kbd>NeurIPS 2020 workshop</kbd> [GM Israel, 3D LLD]</li>
  <li><a href="https://arxiv.org/abs/2011.04233">LSTR: End-to-end Lane Shape Prediction with Transformers</a> [<a href="/Learning-Deep-Learning/paper_notes/lstr.html">Notes</a>] <kbd>WACV 2021</kbd> [LLD, transformers]</li>
  <li><a href="https://arxiv.org/abs/1902.06326">PIXOR: Real-time 3D Object Detection from Point Clouds</a> [<a href="/Learning-Deep-Learning/paper_notes/pixor.html">Notes</a>] <kbd>CVPR 2018</kbd> (birds eye view)</li>
  <li><a href="http://proceedings.mlr.press/v87/yang18b/yang18b.pdf">HDNET/PIXOR++: Exploiting HD Maps for 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/pixor++.html">Notes</a>] <kbd>CoRL 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.13816">CPNDet: Corner Proposal Network for Anchor-free, Two-stage Object Detection</a> <kbd>ECCV 2020</kbd> [anchor free, two stage]</li>
  <li><a href="https://arxiv.org/abs/1910.06528">MVF: End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds</a> [<a href="/Learning-Deep-Learning/paper_notes/mvf.html">Notes</a>] <kbd>CoRL 2019</kbd> [Waymo, VoxelNet 1st author]</li>
  <li><a href="https://arxiv.org/abs/2007.10323">Pillar-based Object Detection for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/pillar_od.html">Notes</a>] <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1909.00700">Training-Time-Friendly Network for Real-Time Object Detection</a> <kbd>AAAI 2020</kbd> [anchor-free, fast training]</li>
  <li><a href="https://arxiv.org/abs/2006.06091">Autonomous Driving with Deep Learning: A Survey of State-of-Art Technologies</a> [Review of autonomous stack, Yu Huang]</li>
  <li><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Ranftl_Dense_Monocular_Depth_CVPR_2016_paper.pdf">Dense Monocular Depth Estimation in Complex Dynamic Scenes</a> <kbd>CVPR 2016</kbd></li>
  <li><a href="https://anthonyhu.github.io/research/probabilistic-future-prediction/">Probabilistic Future Prediction for Video Scene Understanding</a></li>
  <li><a href="https://arxiv.org/abs/1907.03961">AB3D: A Baseline for 3D Multi-Object Tracking</a> <kbd>IROS 2020</kbd> [3D MOT]</li>
  <li><a href="https://arxiv.org/abs/1904.11489">Spatial-Temporal Relation Networks for Multi-Object Tracking</a> <kbd>ICCV 2019</kbd> [MOT, feature location over time]</li>
  <li><a href="https://arxiv.org/abs/1802.09298">Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking</a> <kbd>ICRA 2018</kbd> [MOT, IIT, 3D shape]</li>
  <li><a href="https://arxiv.org/abs/2004.09305">ST-3D: Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking</a> <kbd>CVPR 2020</kbd> [Peilinag LI, author of VINS and S3DOT]</li>
  <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Hoffer_Augment_Your_Batch_Improving_Generalization_Through_Instance_Repetition_CVPR_2020_paper.pdf">Augment Your Batch: Improving Generalization Through Instance Repetition</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.13870">RetinaTrack: Online Single Stage Joint Detection and Tracking</a> <kbd>CVPR 2020</kbd> [MOT]</li>
  <li><a href="https://arxiv.org/abs/1912.12791">Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots</a></li>
  <li><a href="https://arxiv.org/abs/2004.01461">Gradient Centralization: A New Optimization Technique for Deep Neural Networks</a> <kbd>ECCV 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.10336">Depth Completion via Deep Basis Fitting</a> <kbd>WACV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1907.10326">BTS: From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation</a> [monodepth, supervised]</li>
  <li><a href="https://arxiv.org/abs/2004.00171">The Edge of Depth: Explicit Constraints between Segmentation and Depth</a> <kbd>CVPR 2020</kbd> [monodepth, Xiaoming Liu]</li>
  <li><a href="https://arxiv.org/abs/1812.07035">On the Continuity of Rotation Representations in Neural Networks</a> <kbd>CVPR 2019</kbd> [rotational representation]</li>
  <li><a href="https://arxiv.org/abs/2005.11052">VDO-SLAM: A Visual Dynamic Object-aware SLAM System</a> <kbd>IJRR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2002.08584">Dynamic SLAM: The Need For Speed</a></li>
  <li><a href="https://arxiv.org/abs/2004.10681">Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37259.pdf">Traffic Light Mapping and Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/tfl_mapping_google.html">Notes</a>] <kbd>ICRA 2011</kbd> [traffic light, Google, Chris Urmson]</li>
  <li><a href="https://web.yonsei.ac.kr/jksuhr/papers/Traffic%20light%20recognition%20exploiting%20map%20and%20localization%20at%20every%20stage.pdf">Traffic light recognition exploiting map and localization at every stage</a> [<a href="/Learning-Deep-Learning/paper_notes/tfl_exploting_map_korea.html">Notes</a>] <kbd>Expert Systems 2017</kbd> [traffic light, 鲜于明镐，徐在圭，郑浩奇]</li>
  <li><a href="https://arxiv.org/abs/1906.11886">Traffic Light Recognition Using Deep Learning and Prior Maps for Autonomous Cars</a> [<a href="/Learning-Deep-Learning/paper_notes/tfl_lidar_map_building_brazil.html">Notes</a>] <kbd> IJCNN 2019</kbd> [traffic light, Espirito Santo Brazil]</li>
</ul>

<h2 id="2020-10-14">2020-10 (14)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1811.08383">TSM: Temporal Shift Module for Efficient Video Understanding</a> [<a href="/Learning-Deep-Learning/paper_notes/tsm.html">Notes</a>] <kbd>ICCV 2019</kbd> [Song Han, video, object detection]</li>
  <li><a href="https://arxiv.org/abs/1912.04838">WOD: Waymo Dataset: Scalability in Perception for Autonomous Driving: Waymo Open Dataset</a> [<a href="paper_notes/wod.md">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.04388">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/gfocal.html">Notes</a>] <kbd>NeurIPS 2020</kbd> [classification as regression]</li>
  <li><a href="https://arxiv.org/abs/2009.13592">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</a> <kbd>NeurIPS 2020 spotlight</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.07529">Rethinking the Value of Labels for Improving Class-Imbalanced Learning</a> <kbd>NeurIPS 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1711.07752">RepLoss: Repulsion Loss: Detecting Pedestrians in a Crowd</a> [<a href="/Learning-Deep-Learning/paper_notes/rep_loss.html">Notes</a>] <kbd>CVPR 2018</kbd> [crowd detection, Megvii]</li>
  <li><a href="https://arxiv.org/abs/1904.03629">Adaptive NMS: Refining Pedestrian Detection in a Crowd</a> [<a href="/Learning-Deep-Learning/paper_notes/adaptive_nms.html">Notes</a>] <kbd>CVPR 2019 oral</kbd> [crowd detection, NMS]</li>
  <li><a href="https://arxiv.org/abs/1807.08407">AggLoss: Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd</a> [<a href="/Learning-Deep-Learning/paper_notes/agg_loss.html">Notes</a>] <kbd>ECCV 2018</kbd> [crowd detection]</li>
  <li><a href="https://arxiv.org/abs/2003.09163">CrowdDet: Detection in Crowded Scenes: One Proposal, Multiple Predictions</a> [<a href="/Learning-Deep-Learning/paper_notes/crowd_det.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [crowd detection, Megvii, Earth mover’s distance]</li>
  <li><a href="https://arxiv.org/abs/2003.12729">R2-NMS: NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing</a> [<a href="/Learning-Deep-Learning/paper_notes/r2_nms.html">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1909.09998">Double Anchor R-CNN for Human Detection in a Crowd</a> [<a href="/Learning-Deep-Learning/paper_notes/double_anchor.html">Notes</a>] [head-body bundle]</li>
  <li><a href="/Learning-Deep-Learning/paper_notes/ap_mr.html">Review: AP vs MR</a></li>
  <li><a href="https://arxiv.org/abs/1904.00853">SKU110K: Precise Detection in Densely Packed Scenes</a> [<a href="/Learning-Deep-Learning/paper_notes/sku110k.html">Notes</a>] <kbd>CVPR 2019</kbd> [crowd detection, no occlusion]</li>
  <li><a href="https://arxiv.org/abs/1705.02950">GossipNet: Learning non-maximum suppression</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.01438">TLL: Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation</a> <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2010.03506">Learning Monocular 3D Vehicle Detection without 3D Bounding Box Labels</a> <kbd>GCPR 2020</kbd> [mono3D, Daniel Cremers, TUM]</li>
  <li><a href="https://arxiv.org/abs/2006.04080">CubifAE-3D: Monocular Camera Space Cubification on Autonomous Vehicles for Auto-Encoder based 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/cubifae_3d.html">Notes</a>] [mono3D, depth AE pretraining]</li>
  <li><a href="https://arxiv.org/abs/2010.04159">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/deformable_detr.html">Notes</a>] <kbd>ICLR 2021</kbd> [Jifeng Dai, DETR]</li>
  <li><a href="https://arxiv.org/abs/2010.11929">ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> [<a href="/Learning-Deep-Learning/paper_notes/vit.html">Notes</a>] <kbd>ICLR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.07733">BYOL: Bootstrap your own latent: A new approach to self-supervised Learning</a> [self-supervised]</li>
</ul>

<h2 id="2020-09-15">2020-09 (15)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1911.11288">SDFLabel: Autolabeling 3D Objects With Differentiable Rendering of SDF Shape Priors</a> [<a href="/Learning-Deep-Learning/paper_notes/sdflabel.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [TRI, differentiable rendering]</li>
  <li><a href="https://arxiv.org/abs/1802.00434">DensePose: Dense Human Pose Estimation In The Wild</a> [<a href="/Learning-Deep-Learning/paper_notes/densepose.html">Notes</a>] <kbd>CVPR 2018 oral</kbd> [FAIR]</li>
  <li><a href="https://arxiv.org/abs/1901.02970">NOCS: Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2009.14524">monoDR: Monocular Differentiable Rendering for Self-Supervised 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/monodr.html">Notes</a>] <kbd>ECCV 2020</kbd> [TRI, mono3D]</li>
  <li><a href="https://arxiv.org/abs/2008.05711">Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</a> [<a href="/Learning-Deep-Learning/paper_notes/lift_splat_shoot.html">Notes</a>] <kbd>ECCV 2020</kbd> [BEV-Net, Utoronto, Sanja Fidler]</li>
  <li><a href="https://arxiv.org/abs/2007.12036">Implicit Latent Variable Model for Scene-Consistent Motion Forecasting</a> <kbd>ECCV 2020</kbd> [Uber ATG, Rachel Urtasun]</li>
  <li><a href="https://arxiv.org/abs/2006.09917">FISHING Net: Future Inference of Semantic Heatmaps In Grids</a> [<a href="/Learning-Deep-Learning/paper_notes/fishing_net.html">Notes</a>] <kbd>CVPRW 2020</kbd> [BEV-Net, Mapping, Zoox]</li>
  <li><a href="https://arxiv.org/abs/1906.03560">VPN: Cross-view Semantic Segmentation for Sensing Surroundings</a> [<a href="/Learning-Deep-Learning/paper_notes/vpn.html">Notes</a>] <kbd>RAL 2020</kbd> [Bolei Zhou, BEV-Net]</li>
  <li><a href="https://arxiv.org/abs/1804.02176">VED: Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/ved.html">Notes</a>] <kbd>ICRA 2019</kbd> [BEV-Net]</li>
  <li><a href="https://arxiv.org/abs/2005.04078">Cam2BEV: A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird’s Eye View</a> [<a href="/Learning-Deep-Learning/paper_notes/cam2bev.html">Notes</a>] <kbd>ITSC 2020</kbd> [BEV-Net]</li>
  <li><a href="https://arxiv.org/abs/1803.10870">Learning to Look around Objects for Top-View Representations of Outdoor Scenes</a> [<a href="/Learning-Deep-Learning/paper_notes/learning_to_look_around_objects.html">Notes</a>] <kbd>ECCV 2018</kbd> [BEV-Net, UCSD, Manmohan Chandraker]</li>
  <li><a href="https://arxiv.org/abs/1812.06152">A Parametric Top-View Representation of Complex Road Scenes</a> <kbd>CVPR 2019</kbd> [BEV-Net, UCSD, Manmohan Chandraker]</li>
  <li><a href="https://arxiv.org/abs/2007.00822">FTM: Understanding Road Layout from Videos as a Whole</a> <kbd>CVPR 2020</kbd> [BEV-Net, UCSD, Manmohan Chandraker]</li>
  <li><a href="https://arxiv.org/abs/2009.00764">KM3D-Net: Monocular 3D Detection with Geometric Constraints Embedding and Semi-supervised Training</a> [<a href="/Learning-Deep-Learning/paper_notes/km3d_net.html">Notes</a>] <kbd>RAL 2021</kbd> [RTM3D, Peixuan Li]</li>
  <li><a href="https://arxiv.org/abs/2008.07008">InstanceMotSeg: Real-time Instance Motion Segmentation for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/instance_mot_seg.html">Notes</a>] <kbd>IROS 2020</kbd> [motion segmentation]</li>
  <li><a href="https://arxiv.org/abs/1905.06937">MPV-Nets: Monocular Plan View Networks for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/mpv_nets.html">Notes</a>] <kbd>IROS 2019</kbd> [BEV-Net]</li>
  <li><a href="https://arxiv.org/abs/1901.05555">Class-Balanced Loss Based on Effective Number of Samples</a> [<a href="/Learning-Deep-Learning/paper_notes/class_balanced_loss.html">Notes</a>] <kbd>CVPR 2019</kbd> [Focal loss authors]</li>
  <li><a href="http://lewissoft.com/pdf/ICRA2020/0035.pdf">Geometric Pretraining for Monocular Depth Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/geometric_pretraining.html">Notes</a>] <kbd>ICRA 2020</kbd></li>
  <li><a href="https://www.mdpi.com/1424-8220/20/4/1181">Robust Traffic Light and Arrow Detection Using Digital Map with Spatial Prior Information for Automated Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/tfl_robust_japan.html">Notes</a>] <kbd>Sensors 2020</kbd> [traffic light, 金沢]</li>
</ul>

<h2 id="2020-08-26">2020-08 (26)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2007.10603">Feature-metric Loss for Self-supervised Learning of Depth and Egomotion</a> [<a href="/Learning-Deep-Learning/paper_notes/feature_metric.html">Notes</a>] <kbd>ECCV 2020</kbd> [feature-metric, local minima, monodepth]</li>
  <li><a href="https://arxiv.org/abs/1803.03893">Depth-VO-Feat: Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a> <kbd>CVPR 2018</kbd> [feature-metric, monodepth]</li>
  <li><a href="https://arxiv.org/abs/1904.04144">MonoResMatch: Learning monocular depth estimation infusing traditional stereo knowledge</a> [<a href="/Learning-Deep-Learning/paper_notes/monoresmatch.html">Notes</a>] <kbd>CVPR 2019</kbd> [monodepth, local minima, cheap stereo GT]</li>
  <li><a href="https://arxiv.org/abs/2007.06936">SGDepth: Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance</a> [<a href="/Learning-Deep-Learning/paper_notes/sgdepth.html">Notes</a>] <kbd>ECCV 2020</kbd> [Moving objects]</li>
  <li><a href="https://arxiv.org/abs/1806.10556">Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D Motion Understanding</a> <kbd>ECCV 2018</kbd> [dynamic objects, rigid and dynamic motion]</li>
  <li><a href="https://arxiv.org/abs/1810.06125">Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D Holistic Understanding</a> <kbd>TPAMI 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1805.09806">CC: Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/cc.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.04250">ObjMotionNet: Self-supervised Object Motion and Depth Estimation from Video</a> [<a href="/Learning-Deep-Learning/paper_notes/obj_motion_net.html">Notes</a>] <kbd>CVPRW 2020</kbd> [object motion prediction, velocity prediction]</li>
  <li><a href="https://arxiv.org/abs/1912.09351">Instance-wise Depth and Motion Learning from Monocular Videos</a></li>
  <li><a href="https://arxiv.org/abs/2006.04371">Semantics-Driven Unsupervised Learning for Monocular Depth and Ego-Motion Estimation</a></li>
  <li><a href="https://arxiv.org/abs/2006.09876">Self-Supervised Joint Learning Framework of Depth Estimation via Implicit Cues</a></li>
  <li><a href="https://arxiv.org/abs/1809.01649">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a> <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.05696">LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling in Urban Environments</a> [mapping]</li>
  <li><a href="https://www.naverlabs.com/img/autonomousDriving/intelligence/dissertation/Road-SLAM_Road%20Marking%20based%20SLAM%20with%20Lane-level%20Accuracy.pdf">Road-SLAM: Road Marking based SLAM with Lane-level Accuracy</a> [<a href="/Learning-Deep-Learning/paper_notes/road_slam.html">Notes</a>] [HD mapping]</li>
  <li><a href="https://arxiv.org/abs/2007.01813">AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot</a> [<a href="/Learning-Deep-Learning/paper_notes/avp_slam.html">Notes</a>] <kbd>IROS 2020</kbd> [Huawei, HD mapping, Tong Qin, VINS author, autonomous valet parking]</li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8917529">AVP-SLAM-Late-Fusion: Mapping and Localization using Semantic Road Marking with Centimeter-level Accuracy in Indoor Parking Lots</a> [<a href="/Learning-Deep-Learning/paper_notes/avp_slam_late_fusion.html">Notes</a>] <kbd>ITSC 2019</kbd></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8917254">Lane markings-based relocalization on highway</a> <kbd>ITSC 2019</kbd></li>
  <li><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Mattyus_DeepRoadMapper_Extracting_Road_ICCV_2017_paper.pdf">DeepRoadMapper: Extracting Road Topology from Aerial Images</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_road_mapper.html">Notes</a>] <kbd>ICCV 2017</kbd> [Uber ATG, NOT HD maps]</li>
  <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Bastani_RoadTracer_Automatic_Extraction_CVPR_2018_paper.pdf">RoadTracer: Automatic Extraction of Road Networks from Aerial Images</a> <kbd>CVPR 2018</kbd> [NOT HD maps]</li>
  <li><a href="https://arxiv.org/abs/1812.01497">PolyMapper: Topological Map Extraction From Overhead Images</a> [<a href="/Learning-Deep-Learning/paper_notes/polymapper.html">Notes</a>] <kbd>ICCV 2019</kbd> [mapping, polygon, NOT HD maps]</li>
  <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.pdf">HRAN: Hierarchical Recurrent Attention Networks for Structured Online Maps</a> [<a href="/Learning-Deep-Learning/paper_notes/hran.html">Notes</a>] <kbd>CVPR 2018</kbd> [HD mapping, highway, polyline loss, Chamfer distance]</li>
  <li><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Justin_Liang_End-to-End_Deep_Structured_ECCV_2018_paper.pdf">Deep Structured Crosswalk: End-to-End Deep Structured Models for Drawing Crosswalks</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_structured_crosswalk.html">Notes</a>] <kbd>ECCV 2018</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Convolutional_Recurrent_Network_for_Road_Boundary_Extraction_CVPR_2019_paper.html">DeepBoundaryExtractor: Convolutional Recurrent Network for Road Boundary Extraction</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_boundary_extractor.html">Notes</a>] <kbd>CVPR 2019</kbd> [HD mapping, boundary, polyline loss]</li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Homayounfar_DAGMapper_Learning_to_Map_by_Discovering_Lane_Topology_ICCV_2019_paper.pdf">DAGMapper: Learning to Map by Discovering Lane Topology</a> [<a href="/Learning-Deep-Learning/paper_notes/dagmapper.html">Notes</a>] <kbd>ICCV 2019</kbd> [HD mapping, highway, forks and merges, polyline loss]</li>
  <li><a href="https://arxiv.org/abs/1908.03274">Sparse-HD-Maps: Exploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization</a> [<a href="/Learning-Deep-Learning/paper_notes/sparse_hd_maps.html">Notes</a>] <kbd>IROS 2019 oral</kbd> [Uber ATG, metadata, mapping, localization]</li>
  <li><a href="https://arxiv.org/abs/1803.06904">Aerial LaneNet: Lane Marking Semantic Segmentation in Aerial Imagery using Wavelet-Enhanced Cost-sensitive Symmetric Fully Convolutional Neural Networks</a> <kbd>IEEE TGRS 2018</kbd></li>
  <li><a href="https://www.mdpi.com/1424-8220/20/7/1870/htm">Monocular Localization with Vector HD Map (MLVHM): A Low-Cost Method for Commercial IVs</a> <kbd>Sensors 2020</kbd> [Tsinghua, 3D HD maps]</li>
  <li><a href="https://arxiv.org/abs/2008.04582">PatchNet: Rethinking Pseudo-LiDAR Representation</a> [<a href="/Learning-Deep-Learning/paper_notes/patchnet.html">Notes</a>] <kbd>ECCV 2020</kbd> [SenseTime, Wanli Ouyang]</li>
  <li><a href="https://arxiv.org/abs/1912.04799">D4LCN: Learning Depth-Guided Convolutions for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/d4lcn.html">Notes</a>] <kbd>CVPR 2020</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2008.01484">MfS: Learning Stereo from Single Images</a> [<a href="/Learning-Deep-Learning/paper_notes/mfs.html">Notes</a>] <kbd>ECCV 2020</kbd> [mono for stereo, learn stereo matching with mono]</li>
  <li><a href="https://arxiv.org/abs/2007.11056">BorderDet: Border Feature for Dense Object Detection</a> <kbd>ECCV 2020 oral</kbd> [Megvii]</li>
  <li><a href="https://arxiv.org/abs/1901.01892">Scale-Aware Trident Networks for Object Detection</a> <kbd>ICCV 2019</kbd> [different heads for different scales]</li>
  <li><a href="https://arxiv.org/abs/1712.00175">Learning Depth from Monocular Videos using Direct Methods</a></li>
  <li><a href="https://arxiv.org/abs/1802.05522">Vid2Depth: Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</a> <kbd>CVPR 2018</kbd> [Google]</li>
  <li><a href="https://arxiv.org/abs/2008.02268">NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</a></li>
  <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Maria_Klodt_Supervising_the_new_ECCV_2018_paper.pdf">Supervising the new with the old: learning SFM from SFM</a> [<a href="paper_notes/learn_sfm_from_sfm.md">Notes</a>] <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1901.02571">Neural RGB-&gt;D Sensing: Depth and Uncertainty from a Video Camera</a> <kbd>CVPR 2019</kbd> [multi-frame monodepth]</li>
  <li><a href="https://arxiv.org/abs/2001.02613">Don’t Forget The Past: Recurrent Depth Estimation from Monocular Video</a> [multi-frame monodepth, RNN]</li>
  <li><a href="https://arxiv.org/abs/1904.07087">Recurrent Neural Network for (Un-)supervised Learning of Monocular VideoVisual Odometry and Depth</a> [multi-frame monodepth, RNN]</li>
  <li><a href="https://arxiv.org/abs/1908.03706">Exploiting temporal consistency for real-time video depth estimation</a> <kbd>ICCV 2019</kbd> [multi-frame monodepth, RNN, indoor]</li>
  <li><a href="https://arxiv.org/abs/1704.07804">SfM-Net: Learning of Structure and Motion from Video</a> [dynamic object, SfM]</li>
  <li><a href="https://ieeexplore.ieee.org/document/8500395">MB-Net: MergeBoxes for Real-Time 3D Vehicles Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/mb_net.html">Notes</a>] <kbd>IV 2018</kbd> [mono3D: Daimler]</li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8814036/">BS3D: Beyond Bounding Boxes: Using Bounding Shapes for Real-Time 3D Vehicle Detection from Monocular RGB Images</a> [<a href="/Learning-Deep-Learning/paper_notes/bs3d.html">Notes</a>] <kbd>IV 2019</kbd> [mono3D, Daimler]</li>
  <li><a href="https://arxiv.org/abs/2006.13084">3D-GCK: Single-Shot 3D Detection of Vehicles from Monocular RGB Images via
Geometrically Constrained Keypoints in Real-Time</a> [<a href="/Learning-Deep-Learning/paper_notes/3d_gck.html">Notes</a>] <kbd>IV 2020</kbd> [[mono3D, Daimler]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6559_ECCV_2020_paper.php">UR3D: Distance-Normalized Unified Representation for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/ur3d.html">Notes</a>] <kbd>ECCV 2020</kbd> [mono3D]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540018.pdf">DA-3Det: Monocular 3D Object Detection via Feature Domain Adaptation</a> [<a href="/Learning-Deep-Learning/paper_notes/da_3det.html">Notes</a>] <kbd>ECCV 2020</kbd> [mono3D]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2822_ECCV_2020_paper.php">RAR-Net: Reinforced Axial Refinement Network for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/rarnet.html">Notes</a>] <kbd>ECCV 2020</kbd> [mono3D]</li>
</ul>

<h2 id="2020-07-25">2020-07 (25)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2004.01177">CenterTrack: Tracking Objects as Points</a> [<a href="/Learning-Deep-Learning/paper_notes/centertrack.html">Notes</a>] <kbd>ECCV 2020 spotlight</kbd> [camera based 3D MOD, MOT SOTA, CenterNet, video based object detection]</li>
  <li><a href="https://arxiv.org/abs/2006.11275">CenterPoint: Center-based 3D Object Detection and Tracking</a> [<a href="/Learning-Deep-Learning/paper_notes/centerpoint.html">Notes</a>] <kbd>CVPR 2021</kbd> [lidar based 3D MOD, CenterNet]</li>
  <li><a href="https://arxiv.org/abs/1903.05625">Tracktor: Tracking without bells and whistles</a> [<a href="/Learning-Deep-Learning/paper_notes/tracktor.html">Notes</a>] <kbd>ICCV 2019</kbd> [Tracktor/Tracktor++, Laura Leal-Taixe@TUM]</li>
  <li><a href="https://arxiv.org/abs/2004.01888">FairMOT: A Simple Baseline for Multi-Object Tracking</a> [<a href="/Learning-Deep-Learning/paper_notes/fairmot.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1906.06618">DeepMOT: A Differentiable Framework for Training Multiple Object Trackers</a> [<a href="/Learning-Deep-Learning/paper_notes/deepmot.html">Notes</a>] <kbd>CVPR 2020</kbd> [trainable Hungarian, Laura Leal-Taixe@TUM]</li>
  <li><a href="https://arxiv.org/abs/1912.07515">MPNTracker: Learning a Neural Solver for Multiple Object Tracking</a> <kbd>CVPR 2020 oral</kbd> [trainable Hungarian, Laura Leal-Taixe@TUM]</li>
  <li><a href="https://arxiv.org/abs/1903.11027">nuScenes: A multimodal dataset for autonomous driving</a> [<a href="/Learning-Deep-Learning/paper_notes/nuscenes.html">Notes</a>] <kbd>CVPR 2020</kbd> [dataset, point cloud, radar]</li>
  <li><a href="https://arxiv.org/abs/1908.09492">CBGS: Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/cbgs.html">Notes</a>] <kbd>CVPRW 2019</kbd> [Megvii, lidar, WAD challenge winner]</li>
  <li><a href="https://arxiv.org/abs/2006.12671">AFDet: Anchor Free One Stage 3D Object Detection</a> and <a href="https://arxiv.org/pdf/2006.15505.pdf">Competition solution</a> [<a href="/Learning-Deep-Learning/paper_notes/afdet.html">Notes</a>]  <kbd>CVPRW 2020</kbd> [Horizon robotics, lidar, winning for Waymo challenge]</li>
  <li>Review of MOT and SOT [<a href="/Learning-Deep-Learning/paper_notes/mot_and_sot.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1805.00123">CrowdHuman: A Benchmark for Detecting Human in a Crowd</a> [<a href="/Learning-Deep-Learning/paper_notes/crowdhuman.html">Notes</a>] [megvii, pedestrian, dataset]</li>
  <li><a href="https://arxiv.org/abs/1909.12118">WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild</a> [<a href="/Learning-Deep-Learning/paper_notes/widerperson.html">Notes</a>] <kbd>TMM 2019</kbd> [dataset, pedestrian]</li>
  <li><a href="http://www.gavrila.net/Publications/iv16_cyclist_benchmark.pdf">Tsinghua-Daimler Cyclists: A New Benchmark for Vison-Based Cyclist Detection</a> [<a href="paper_notes/tsinghua_daimler_cyclist.md">Notes</a>] <kbd>IV 2016</kbd> [dataset, cyclist Detection]</li>
  <li><a href="https://drive.google.com/drive/u/0/folders/1inawrX9NVcchDQZepnBeJY4i9aAI5mg9">Specialized Cyclist Detection Dataset: Challenging Real-World Computer Vision Dataset for Cyclist Detection Using a Monocular RGB Camera</a> [<a href="[paper_notes/specialized_cyclists.md">Notes</a>] <kbd>IV 2019</kbd> [Extention to KITTI]</li>
  <li><a href="https://arxiv.org/abs/2007.01550">PointTrack: Segment as Points for Efficient Online Multi-Object Tracking and Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/pointtrack.html">Notes</a>] <kbd>ECCV 2020 oral</kbd> [MOTS]</li>
  <li><a href="https://arxiv.org/abs/2007.01549">PointTrack++ for Effective Online Multi-Object Tracking and Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/pointtrack++.html">Notes</a>] <kbd>CVPR 2020 workshop</kbd> [CVPR2020 MOTS Challenge Winner. PointTrack++ ranks first on KITTI MOTS]</li>
  <li><a href="https://arxiv.org/abs/1906.11109">SpatialEmbedding: Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</a> [<a href="/Learning-Deep-Learning/paper_notes/spatial_embedding.html">Notes</a>] <kbd>ICCV 2019</kbd> [one-stage, instance segmentation]</li>
  <li><a href="https://arxiv.org/abs/1806.04807">BA-Net: Dense Bundle Adjustment Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/banet.html">Notes</a>] <kbd>ICLR 2019</kbd> [Bundle adjustment, multi-frame monodepth, feature-metric]</li>
  <li><a href="https://arxiv.org/abs/1912.09697">DeepSFM: Structure From Motion Via Deep Bundle Adjustment</a> <kbd>ECCV 2020 oral</kbd> [multi-frame monodepth, indoor scene]</li>
  <li><a href="https://arxiv.org/abs/2004.15021">CVD: Consistent Video Depth Estimation</a> [<a href="paper_notes/cvd.md">Notes</a>] <kbd>SIGGRAPH 2020</kbd> [multi-frame monodepth, online finetune]</li>
  <li><a href="https://arxiv.org/abs/1812.04605">DeepV2D: Video to Depth with Differentiable Structure from Motion</a> [<a href="/Learning-Deep-Learning/paper_notes/deepv2d.html">Notes</a>] <kbd>ICLR 2020</kbd> [multi-frame monodepth, Jia Deng]</li>
  <li><a href="https://arxiv.org/abs/1803.02276">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a> [<a href="/Learning-Deep-Learning/paper_notes/geonet.html">Notes</a>] <kbd>CVPR 2018</kbd> [residual optical flow, monodepth, rigid and dynamic motion]</li>
  <li><a href="https://arxiv.org/abs/1907.05820">GLNet: Self-supervised Learning with Geometric Constraints in Monocular Video: Connecting Flow, Depth, and Camera</a> [<a href="/Learning-Deep-Learning/paper_notes/glnet.html">Notes</a>] <kbd>ICCV 2019</kbd> [online finetune, rigid and dynamic motion]</li>
  <li><a href="https://arxiv.org/abs/1909.09051">Depth Hints: Self-Supervised Monocular Depth Hints</a> [<a href="/Learning-Deep-Learning/paper_notes/depth_hints.html">Notes</a>] <kbd>ICCV 2019</kbd> [monodepth, local minima, cheap stereo GT]</li>
  <li><a href="https://arxiv.org/abs/2005.06209">MonoUncertainty: On the uncertainty of self-supervised monocular depth estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/mono_uncertainty.html">Notes</a>] <kbd>CVPR 2020</kbd> [depth uncertainty]</li>
  <li><a href="https://arxiv.org/abs/1909.13163">Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment</a> [<a href="/Learning-Deep-Learning/paper_notes/ba_sfm_learner.html">Notes</a>] [Bundle adjustment, xmotors.ai, multi-frame monodepth]</li>
  <li><a href="https://arxiv.org/abs/2007.09548">Kinematic 3D Object Detection in Monocular Video</a> [<a href="/Learning-Deep-Learning/paper_notes/kinematic_mono3d.html">Notes</a>] <kbd>ECCV 2020</kbd> [multi-frame mono3D, Xiaoming Liu]</li>
  <li><a href="https://arxiv.org/abs/1802.07094">VelocityNet: Camera-based vehicle velocity estimation from monocular video</a> [<a href="/Learning-Deep-Learning/paper_notes/velocity_net.html">Notes</a>] <kbd>CVPR 2017 workshop</kbd> [monocular velocity estimation, CVPR 2017 challenge winner]</li>
  <li><a href="https://arxiv.org/abs/2006.04082">Vehicle Centric VelocityNet: End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera</a> [<a href="/Learning-Deep-Learning/paper_notes/vehicle_centric_velocity_net.html">Notes</a>] [monocular velocity estimation, monocular distance, SOTA]</li>
</ul>

<h2 id="2020-06-20">2020-06 (20)</h2>
<ul>
  <li><a href="http://personal.stevens.edu/~benglot/Shan_Englot_IROS_2018_Preprint.pdf">LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain</a> [<a href="/Learning-Deep-Learning/paper_notes/lego_loam.html">Notes</a>] <kbd>IROS 2018</kbd> [lidar, mapping]</li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf">PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/pie.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf">JAAD: Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian
Crosswalk Behavior</a> <kbd>ICCV 2017</kbd></li>
  <li><a href="https://bmvc2019.org/wp-content/uploads/papers/0283-paper.pdf">Pedestrian Action Anticipation using Contextual Feature Fusion in Stacked RNNs</a> <kbd>BMVC 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.10580">Is the Pedestrian going to Cross? Answering by 2D Pose Estimation</a> <kbd>IV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1910.03858">Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation</a> <kbd>ITSC 2019</kbd> [skeleton, pedestrian, cyclist intention]</li>
  <li><a href="https://arxiv.org/abs/1904.08918">Attentive Single-Tasking of Multiple Tasks</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2005.12872">DETR: End-to-End Object Detection with Transformers</a> [<a href="/Learning-Deep-Learning/paper_notes/detr.html">Notes</a>] <kbd>ECCV 2020 oral</kbd> [FAIR]</li>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer: Attention Is All You Need</a> [<a href="/Learning-Deep-Learning/paper_notes/transformer.html">Notes</a>] <kbd>NIPS 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.06130">SpeedNet: Learning the Speediness in Videos</a> [<a href="/Learning-Deep-Learning/paper_notes/speednet.html">Notes</a>] <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.00504">MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships</a> [<a href="/Learning-Deep-Learning/paper_notes/monopair.html">Notes</a>] <kbd>CVPR 2020</kbd> [Mono3D, pairwise relationship]</li>
  <li><a href="https://arxiv.org/abs/2002.10111">SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/smoke.html">Notes</a>] <kbd>CVPRW 2020</kbd> [Mono3D, Zongmu]</li>
  <li><a href="https://drive.google.com/file/d/1e6y8wtHAricaEHS9CpasSGOx0aAxCGib/view">Vehicle Re-ID for Surround-view Camera System</a> [<a href="/Learning-Deep-Learning/paper_notes/reid_surround_fisheye.html">Notes</a>] <kbd>CVPRW 2020</kbd> [tireline, vehicle ReID, Zongmu]</li>
  <li><a href="https://arxiv.org/abs/2005.08630">End-to-End Lane Marker Detection via Row-wise Classification</a> [<a href="/Learning-Deep-Learning/paper_notes/e2e_lmd.html">Notes</a>] [Qualcomm Korea, LLD as cls]</li>
  <li><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Chougule_Reliable_multilane_detection_and_classification_by_utilizing_CNN_as_a_ECCVW_2018_paper.pdf">Reliable multilane detection and classification by utilizing CNN as a regression network</a> <kbd>ECCV 2018</kbd> [LLD as reg]</li>
  <li><a href="https://arxiv.org/abs/2005.07277">SUPER: A Novel Lane Detection System</a> [<a href="/Learning-Deep-Learning/paper_notes/super.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1908.00821">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf">StixelNet: A Deep Convolutional Network for Obstacle Detection and Road Segmentation</a> <kbd>BMVC 2015</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Garnett_Real-Time_Category-Based_and_ICCV_2017_paper.pdf">StixelNetV2: Real-time category-based and general obstacle detection for autonomous driving</a> [<a href="paper_notes/stixelnetv2.md">Notes</a>] <kbd>ICCV 2017</kbd> [DS]</li>
  <li><a href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a> [<a href="/Learning-Deep-Learning/paper_notes/subpixel_conv.html">Notes</a>] <kbd>CVPR 2016</kbd> [channel-to-pixel]</li>
  <li><a href="https://arxiv.org/abs/1912.04363">Car Pose in Context: Accurate Pose Estimation with Ground Plane Constraints</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2004.04143">Self-Mono-SF: Self-Supervised Monocular Scene Flow Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/self_mono_sf.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [scene-flow, Stereo input]</li>
  <li><a href="https://arxiv.org/abs/2011.13688">MEBOW: Monocular Estimation of Body Orientation In the Wild</a> [<a href="/Learning-Deep-Learning/paper_notes/mebow.html">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2006.08547">VG-NMS: Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in Crowded Traffic Scenes</a> [<a href="/Learning-Deep-Learning/paper_notes/vg_nms.html">Notes</a>] <kbd>NeurIPS 2019 workshop</kbd> [Crowded scene, NMS, Daimler]</li>
  <li><a href="https://arxiv.org/abs/1912.04986">WYSIWYG: What You See is What You Get: Exploiting Visibility for 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/wysiwyg.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [occupancy grid]</li>
  <li><a href="https://arxiv.org/abs/1912.01202">Real-Time Panoptic Segmentation From Dense Detections</a> [<a href="/Learning-Deep-Learning/paper_notes/realtime_panoptic.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [bbox + semantic segmentation = panoptic segmentation, Toyota]</li>
  <li><a href="https://drive.google.com/file/d/1DY95vfWBLKOOZZyq8gLDd0heZ6aBSdji/view">Human-Centric Efficiency Improvements in Image Annotation for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/human_centric_annotation.html">Notes</a>] <kbd>CVPRW 2020</kbd> [efficient annotation]</li>
  <li><a href="https://arxiv.org/abs/2005.03844">SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/surfel_gan.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [Waymo, auto data generation, surfel]</li>
  <li><a href="https://arxiv.org/abs/2006.09348">LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World</a> [<a href="paper_notes/lidarsim.md">Notes</a>] <kbd>CVPR 2020 oral</kbd> [Uber ATG, auto data generation, surfel]</li>
  <li><a href="http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2019iros.pdf">SuMa++: Efficient LiDAR-based Semantic SLAM</a> <kbd>IROS 2019</kbd> [semantic segmentation, lidar, SLAM]</li>
  <li><a href="https://arxiv.org/abs/2003.13402">PON/PyrOccNet: Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/pyroccnet.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [BEV-Net, OFT]</li>
  <li><a href="https://arxiv.org/abs/2002.08394">MonoLayout: Amodal scene layout from a single image</a> [<a href="/Learning-Deep-Learning/paper_notes/monolayout.html">Notes</a>] <kbd>WACV 2020</kbd> [BEV-Net]</li>
  <li><a href="https://arxiv.org/abs/2006.11436">BEV-Seg: Bird’s Eye View Semantic Segmentation Using Geometry and Semantic Point Cloud</a> [<a href="/Learning-Deep-Learning/paper_notes/bev_seg.html">Notes</a>] <kbd>CVPR 2020 workshop</kbd> [BEV-Net, Mapping]</li>
  <li><a href="https://arxiv.org/abs/1905.02231">A Geometric Approach to Obtain a Bird’s Eye View from an Image</a> <kbd>ICCVW 2019</kbd> [mapping, geometry, Andrew Zisserman]</li>
  <li><a href="https://arxiv.org/abs/1904.11111">FrozenDepth: Learning the Depths of Moving People by Watching Frozen People</a> [<a href="/Learning-Deep-Learning/paper_notes/frozen_depth.html">Notes</a>] <kbd>CVPR 2019 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1502.00956">ORB-SLAM: a Versatile and Accurate Monocular SLAM System</a> <kbd>TRO 2015</kbd></li>
  <li><a href="https://arxiv.org/abs/1610.06475">ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras</a> <kbd>TRO 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1806.00557">CubeSLAM: Monocular 3D Object SLAM</a> [<a href="/Learning-Deep-Learning/paper_notes/cube_slam.html">Notes</a>] <kbd>TRO 2019</kbd> [dynamic SLAM, orb slam + mono3D]</li>
  <li><a href="https://arxiv.org/abs/2003.12980">ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings</a> [<a href="/Learning-Deep-Learning/paper_notes/cluster_vo.html">Notes</a>] <kbd>CVPR 2020</kbd> [general dynamic SLAM]</li>
  <li><a href="https://arxiv.org/abs/1807.02062">S3DOT: Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/s3dot.html">Notes</a>] <kbd>ECCV 2018</kbd> [Peiliang Li]</li>
  <li><a href="https://arxiv.org/abs/2002.03528">Multi-object Monocular SLAM for Dynamic Environments</a> [<a href="/Learning-Deep-Learning/paper_notes/multi_object_mono_slam.html">Notes</a>] <kbd>IV 2020</kbd> [monolayout authors]</li>
  <li><a href="https://arxiv.org/abs/1709.02371">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</a> [<a href="/Learning-Deep-Learning/paper_notes/pwc_net.html">Notes</a>] <kbd>CVPR 2018 oral</kbd> [Optical flow]</li>
  <li><a href="https://arxiv.org/abs/1805.07036">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</a> <kbd>CVPR 2018</kbd> [Optical flow]</li>
  <li><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf">FlowNet: Learning Optical Flow With Convolutional Networks</a> <kbd>ICCV 2015</kbd> [Optical flow]</li>
  <li><a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a> <kbd>CVPR 2017</kbd> [Optical flow]</li>
  <li><a href="https://arxiv.org/abs/1811.11431">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</a> <kbd>CVPR 2019</kbd> [semantic segmentation, lightweight]</li>
  <li><a href="https://arxiv.org/abs/1908.06316">Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes</a> <kbd>ICCV 2019</kbd> [depth uncertainty]</li>
</ul>

<h2 id="2020-05-19">2020-05 (19)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1809.07408">Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems</a> [<a href="/Learning-Deep-Learning/paper_notes/hevi.html">Notes</a>] [Honda] <kbd>ICRA 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1905.02693">PackNet: 3D Packing for Self-Supervised Monocular Depth Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/packnet.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [Scale aware depth]</li>
  <li><a href="https://arxiv.org/abs/2002.12319">PackNet-SG: Semantically-Guided Representation Learning for Self-Supervised Monocular Depth</a> [<a href="/Learning-Deep-Learning/paper_notes/packnet_sg.html">Notes</a>] <kbd>ICLR 2020</kbd> [TRI, infinite-depth problem]</li>
  <li><a href="https://arxiv.org/abs/2004.01314">TrianFlow: Towards Better Generalization: Joint Depth-Pose Learning without PoseNet</a> [<a href="/Learning-Deep-Learning/paper_notes/trianflow.html">Notes</a>] <kbd>CVPR 2020</kbd> [Scale aware]</li>
  <li><a href="https://arxiv.org/abs/1903.07504">Understanding the Limitations of CNN-based Absolute Camera Pose Regression</a> [<a href="/Learning-Deep-Learning/paper_notes/understanding_apr.html">Notes</a>] <kbd>CVPR 2019</kbd> [Drawbacks of PoseNet, MapNet, Laura Leal-Taixe@TUM]</li>
  <li><a href="https://arxiv.org/abs/1908.01293">To Learn or Not to Learn: Visual Localization from Essential Matrices</a> [<a href="/Learning-Deep-Learning/paper_notes/to_learn_or_not.html">Notes</a>] <kbd>ICRA 2020</kbd> [SIFT + 5 pt solver » others for VO, Laura Leal-Taixe@TUM]</li>
  <li><a href="https://arxiv.org/abs/1909.09803">DF-VO: Visual Odometry Revisited: What Should Be Learnt?</a> [<a href="/Learning-Deep-Learning/paper_notes/df_vo.html">Notes</a>] <kbd>ICRA 2020</kbd> [Depth and Flow for accurate VO]</li>
  <li><a href="https://arxiv.org/abs/2003.01060">D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry</a> [<a href="/Learning-Deep-Learning/paper_notes/d3vo.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [Daniel Cremers, TUM, depth uncertainty]</li>
  <li><a href="https://arxiv.org/abs/1708.06519">Network Slimming: Learning Efficient Convolutional Networks through Network Slimming</a> [<a href="/Learning-Deep-Learning/paper_notes/network_slimming.html">Notes</a>] <kbd>ICCV 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1802.00124">BatchNorm Pruning: Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers</a> [<a href="/Learning-Deep-Learning/paper_notes/batchnorm_pruning.html">Notes</a>] <kbd>ICLR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1607.02565">Direct Sparse Odometry</a> <kbd>PAMI 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2005.08139">Train in Germany, Test in The USA: Making 3D Object Detectors Generalize</a> [<a href="/Learning-Deep-Learning/paper_notes/train_in_germany.html">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.03080">PseudoLidarV3: End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/pseudo_lidar_v3.html">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.02424">ATSS: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a> [<a href="/Learning-Deep-Learning/paper_notes/atss.html">Notes</a>] <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1911.08287">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a> <kbd>AAAI 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2005.03572">Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation</a> [Journal version]</li>
  <li><a href="https://arxiv.org/abs/2004.10934">YOLOv4: Optimal Speed and Accuracy of Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/yolov4.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/2002.05712">CBN: Cross-Iteration Batch Normalization</a> [<a href="/Learning-Deep-Learning/paper_notes/cbn.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/2004.12432">Stitcher: Feedback-driven Data Provider for Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/stitcher.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1903.06586">SKNet: Selective Kernel Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/sknet.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.06521">CBAM: Convolutional Block Attention Module</a> [<a href="/Learning-Deep-Learning/paper_notes/cbam.html">Notes</a>] <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.08955">ResNeSt: Split-Attention Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/resnest.html">Notes</a>]</li>
</ul>

<h2 id="2020-04-14">2020-04 (14)</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/1812.03079.pdf">ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst</a> [<a href="/Learning-Deep-Learning/paper_notes/chauffeurnet.html">Notes</a>] <kbd>RSS 2019</kbd> [Waymo]</li>
  <li><a href="http://www.cs.toronto.edu/~wenjie/papers/intentnet_corl18.pdf">IntentNet: Learning to Predict Intention from Raw Sensor Data</a> [<a href="/Learning-Deep-Learning/paper_notes/intentnet.html">Notes</a>] <kbd>CoRL 2018</kbd> [Uber ATG, perception and prediction, Lidar+Map]</li>
  <li><a href="https://arxiv.org/abs/1906.08945">RoR: Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions</a> [<a href="/Learning-Deep-Learning/paper_notes/ror.html">Notes</a>] <kbd>CVPR 2019</kbd> [Zoox]</li>
  <li><a href="https://arxiv.org/abs/1910.05449">MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/multipath.html">Notes</a>] <kbd>CoRL 2019</kbd> [Waymo, authors from RoR and ChauffeurNet]</li>
  <li><a href="http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf">NMP: End-to-end Interpretable Neural Motion Planner</a> [<a href="/Learning-Deep-Learning/paper_notes/nmp.html">Notes</a>] <kbd>CVPR 2019 oral</kbd> [Uber ATG]</li>
  <li><a href="https://arxiv.org/abs/1809.10732">Multimodal Trajectory Predictions for Autonomous Driving using Deep Convolutional Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/multipath_uber.html">Notes</a>] <kbd>ICRA 2019</kbd> [Henggang Cui, Multimodal, Uber ATG Pittsburgh]</li>
  <li><a href="https://arxiv.org/abs/1808.05819">Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving</a> <kbd>WACV 2020</kbd> [Uber ATG Pittsburgh]</li>
  <li><a href="https://arxiv.org/abs/1910.04586">Jointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles</a> <kbd>IROS 2019 Oral</kbd> [Uber ATG, behavioral planning, motion planning]</li>
  <li><a href="https://arxiv.org/abs/1903.12174">TensorMask: A Foundation for Dense Object Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/tensormask.html">Notes</a>] <kbd>ICCV 2019</kbd> [single-stage instance seg]</li>
  <li><a href="https://arxiv.org/abs/2001.00309">BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/blendmask.html">Notes</a>] <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.11712">Mask Encoding for Single Shot Instance Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/meinst.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [single-stage instance seg, Chunhua Shen]</li>
  <li><a href="https://arxiv.org/abs/1909.13226">PolarMask: Single Shot Instance Segmentation with Polar Representation</a> [<a href="/Learning-Deep-Learning/paper_notes/polarmask.html">Notes</a>] <kbd>CVPR 2020 oral</kbd> [single-stage instance seg]</li>
  <li><a href="https://arxiv.org/abs/1912.04488">SOLO: Segmenting Objects by Locations</a> [<a href="/Learning-Deep-Learning/paper_notes/solo.html">Notes</a>] <kbd>ECCV 2020</kbd> [single-stage instance seg, Chunhua Shen]</li>
  <li><a href="https://arxiv.org/abs/2003.10152">SOLOv2: Dynamic, Faster and Stronger</a> [<a href="/Learning-Deep-Learning/paper_notes/solov2.html">Notes</a>] [single-stage instance seg, Chunhua Shen]</li>
  <li><a href="https://arxiv.org/abs/2003.05664">CondInst: Conditional Convolutions for Instance Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/condinst.html">Notes</a>] <kbd>ECCV 2020 oral</kbd> [single-stage instance seg, Chunhua Shen]</li>
  <li><a href="https://arxiv.org/abs/2004.04446">CenterMask: Single Shot Instance Segmentation With Point Representation</a> [<a href="/Learning-Deep-Learning/paper_notes/centermask.html">Notes</a>]<kbd>CVPR 2020</kbd></li>
</ul>

<h2 id="2020-03-15">2020-03 (15)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1710.06288">VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</a> [<a href="/Learning-Deep-Learning/paper_notes/vpgnet.html">Notes</a>] <kbd>ICCV 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1905.07553">Which Tasks Should Be Learned Together in Multi-task Learning?</a> [<a href="/Learning-Deep-Learning/paper_notes/task_grouping.html">Notes</a>] [Stanford, MTL] <kbd>ICML 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1810.04650">MGDA: Multi-Task Learning as Multi-Objective Optimization</a> <kbd>NeurIPS 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1804.08328">Taskonomy: Disentangling Task Transfer Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/taskonomy.html">Notes</a>] <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.08883">Rethinking ImageNet Pre-training</a> [<a href="/Learning-Deep-Learning/paper_notes/rethinking_pretraining.html">Notes</a>] <kbd>ICCV 2019</kbd> [Kaiming He]</li>
  <li><a href="https://arxiv.org/abs/1907.04011">UnsuperPoint: End-to-end Unsupervised Interest Point Detector and Descriptor</a> [<a href="/Learning-Deep-Learning/paper_notes/unsuperpoint.html">Notes</a>] [superpoint]</li>
  <li><a href="https://arxiv.org/abs/1912.10615">KP2D: Neural Outlier Rejection for Self-Supervised Keypoint Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/kp2d.html">Notes</a>] <kbd>ICLR 2020</kbd> (pointNet)</li>
  <li><a href="https://arxiv.org/abs/1912.03426">KP3D: Self-Supervised 3D Keypoint Learning for Ego-motion Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/kp3d.html">Notes</a>] <kbd>CoRL 2020</kbd> [Toyota, superpoint]</li>
  <li><a href="https://arxiv.org/abs/1905.04132">NG-RANSAC: Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses</a> [<a href="/Learning-Deep-Learning/paper_notes/ng_ransac.html">Notes</a>] <kbd>ICCV 2019</kbd> [pointNet]</li>
  <li><a href="https://arxiv.org/abs/1711.05971">Learning to Find Good Correspondences</a> [<a href="/Learning-Deep-Learning/paper_notes/learning_correspondence.html">Notes</a>] <kbd>CVPR 2018 Oral</kbd> (pointNet)</li>
  <li><a href="https://arxiv.org/abs/1911.09712">RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/refined_mpl.html">Notes</a>] [Huawei, Mono3D]</li>
  <li><a href="https://arxiv.org/abs/2002.01619">DSP: Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/dsp.html">Notes</a>] <kbd>AAAI 2020</kbd> (SenseTime, Mono3D)</li>
  <li><a href="https://arxiv.org/abs/1903.02193">Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks</a> (LLD, LSTM)</li>
  <li><a href="https://arxiv.org/abs/1802.05591">LaneNet: Towards End-to-End Lane Detection: an Instance Segmentation Approach</a> [<a href="/Learning-Deep-Learning/paper_notes/lanenet.html">Notes</a>] <kbd>IV 2018</kbd> (LaneNet)</li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Garnett_3D-LaneNet_End-to-End_3D_Multiple_Lane_Detection_ICCV_2019_paper.pdf">3D-LaneNet: End-to-End 3D Multiple Lane Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/3d_lanenet.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.05257">Semi-Local 3D Lane Detection and Uncertainty Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/semilocal_3d_lanenet.html">Notes</a>] [GM Israel, 3D LLD]</li>
  <li><a href="https://arxiv.org/abs/2003.10656">Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/gen_lanenet.html">Notes</a>] <kbd>ECCV 2020</kbd> [Apollo, 3D LLD]</li>
  <li><a href="https://arxiv.org/abs/1711.09026">Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty</a> <kbd>CVPR 2018</kbd> [Egocentric prediction]</li>
  <li><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Rasouli_Its_Not_All_About_Size_On_the_Role_of_Data_ECCVW_2018_paper.pdf">It’s Not All About Size: On the Role of Data Properties in Pedestrian Detection</a> <kbd>ECCV 2018</kbd> [pedestrian]</li>
</ul>

<h2 id="2020-02-12">2020-02 (12)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1611.05424">Associative Embedding: End-to-End Learning for Joint Detection and Grouping</a> [<a href="/Learning-Deep-Learning/paper_notes/associative_embedding.html">Notes</a>] <kbd>NIPS 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1706.07365">Pixels to Graphs by Associative Embedding</a> [<a href="/Learning-Deep-Learning/paper_notes/pixels_to_graphs.html">Notes</a>] <kbd>NIPS 2017</kbd></li>
  <li><a href="http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf">Social LSTM: Human Trajectory Prediction in Crowded Spaces</a> [<a href="/Learning-Deep-Learning/paper_notes/social_lstm.html">Notes</a>] <kbd>CVPR 2017</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lu__Online_Video_ICCV_2017_paper.pdf">Online Video Object Detection using Association LSTM</a> [<a href="/Learning-Deep-Learning/paper_notes/association_lstm.html">Notes</a>] [single stage, recurrent]</li>
  <li><a href="https://arxiv.org/abs/1712.07629">SuperPoint: Self-Supervised Interest Point Detection and Description</a> [<a href="/Learning-Deep-Learning/paper_notes/superpoint.html">Notes</a>] <kbd>CVPR 2018</kbd> (channel-to-pixel, deep SLAM, Magic Leap)</li>
  <li><a href="https://arxiv.org/abs/1912.08193">PointRend: Image Segmentation as Rendering</a> [<a href="/Learning-Deep-Learning/paper_notes/pointrend.html">Notes</a>] <kbd>CVPR 2020 Oral</kbd> [Kaiming He, FAIR]</li>
  <li><a href="https://arxiv.org/abs/1912.00998">Multigrid: A Multigrid Method for Efficiently Training Video Models</a> [<a href="/Learning-Deep-Learning/paper_notes/multigrid_training.html">Notes</a>] <kbd>CVPR 2020 Oral</kbd> [Kaiming He, FAIR]</li>
  <li><a href="https://arxiv.org/abs/1911.11907">GhostNet: More Features from Cheap Operations</a> [<a href="/Learning-Deep-Learning/paper_notes/ghostnet.html">Notes</a>] <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1906.06423">FixRes: Fixing the train-test resolution discrepancy</a> [<a href="/Learning-Deep-Learning/paper_notes/fixres.html">Notes</a>] <kbd>NIPS 2019</kbd> [FAIR]</li>
  <li><a href="https://arxiv.org/abs/1912.08035">MoVi-3D: Towards Generalization Across Depth for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/movi_3d.html">Notes</a>] <kbd>ECCV 2020</kbd> [Virtual Cam, viewport, Mapillary/Facebook, Mono3D]</li>
  <li><a href="https://arxiv.org/abs/1509.08147">Amodal Completion and Size Constancy in Natural Scenes</a> [<a href="/Learning-Deep-Learning/paper_notes/amodal_completion.html">Notes</a>] <kbd>ICCV 2015</kbd> (Amodal completion)</li>
  <li><a href="https://arxiv.org/abs/1911.05722">MoCo: Momentum Contrast for Unsupervised Visual Representation Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/moco.html">Notes</a>] <kbd>CVPR 2020 Oral</kbd> [FAIR, Kaiming He]</li>
</ul>

<h2 id="2020-01-19">2020-01 (19)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1812.11118">Double Descent: Reconciling modern machine learning practice and the bias-variance trade-of</a> [<a href="/Learning-Deep-Learning/paper_notes/double_descent.html">Notes</a>] <kbd>PNAS 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.02292">Deep Double Descent: Where Bigger Models and More Data Hurt</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_double_descent.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a> <kbd>NIPS 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1803.06184.pdf">The ApolloScape Open Dataset for Autonomous Driving and its Application</a> <kbd>CVPR 2018</kbd> (dataset)</li>
  <li><a href="https://arxiv.org/abs/1811.12222">ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/apollocar3d.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.10837">Part-level Car Parsing and Reconstruction from a Single Street View</a> [<a href="/Learning-Deep-Learning/paper_notes/apollo_car_parts.html">Notes</a>] [Baidu]</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Autonomous%20Driving/Wu_6D-VNet_End-to-End_6-DoF_Vehicle_Pose_Estimation_From_Monocular_RGB_Images_CVPRW_2019_paper.pdf">6D-VNet: End-to-end 6DoF Vehicle Pose Estimation from Monocular RGB Images</a> [<a href="/Learning-Deep-Learning/paper_notes/6d_vnet.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2001.03343">RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/rtm3d.html">Notes</a>] <kbd>ECCV 2020 spotlight</kbd></li>
  <li><a href="https://arxiv.org/abs/1806.02446">DORN: Deep Ordinal Regression Network for Monocular Depth Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/dorn.html">Notes</a>] <kbd>CVPR 2018</kbd> [monodepth, supervised]</li>
  <li><a href="https://arxiv.org/abs/1710.03958">D&amp;T: Detect to Track and Track to Detect</a> [<a href="/Learning-Deep-Learning/paper_notes/detect_track.html">Notes</a>] <kbd>ICCV 2017</kbd> (from Feichtenhofer)</li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8916629/">CRF-Net: A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/crf_net.html">Notes</a>] <kbd>SDF 2019</kbd> (radar detection)</li>
  <li><a href="https://www.researchgate.net/profile/Vijay_John3/publication/335833918_RVNet_Deep_Sensor_Fusion_of_Monocular_Camera_and_Radar_for_Image-based_Obstacle_Detection_in_Challenging_Environments/links/5d7f164e92851c87c38b09f1/RVNet-Deep-Sensor-Fusion-of-Monocular-Camera-and-Radar-for-Image-based-Obstacle-Detection-in-Challenging-Environments.pdf">RVNet: Deep Sensor Fusion of Monocular Camera and Radar for Image-based Obstacle Detection in Challenging Environments</a> [<a href="/Learning-Deep-Learning/paper_notes/rvnet.html">Notes</a>] <kbd>PSIVT 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1905.00526">RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles</a> [<a href="/Learning-Deep-Learning/paper_notes/rrpn_radar.html">Notes</a>] <kbd>ICIP 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1607.05781">ROLO: Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking</a> [<a href="/Learning-Deep-Learning/paper_notes/rolo.html">Notes</a>] <kbd>ISCAS 2016</kbd></li>
  <li><a href="https://www.merl.com/publications/docs/TR2018-137.pdf">Recurrent SSD: Recurrent Multi-frame Single Shot Detector for Video Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/recurrent_ssd.html">Notes</a>] <kbd>BMVC 2018</kbd> (Mitsubishi)</li>
  <li><a href="https://doi.org/10.1007/978-3-030-04212-7_44">Recurrent RetinaNet: A Video Object Detection Model Based on Focal Loss</a> [<a href="/Learning-Deep-Learning/paper_notes/recurrent_retinanet.html">Notes</a>] <kbd>ICONIP 2018</kbd> (single stage, recurrent)</li>
  <li><a href="https://arxiv.org/abs/2001.04608">Actions as Moving Points</a> [<a href="/Learning-Deep-Learning/paper_notes/moc.html">Notes</a>] [not suitable for online]</li>
  <li><a href="10.1109/ITSC.2019.8917433">The PREVENTION dataset: a novel benchmark for PREdiction of VEhicles iNTentIONs</a> [<a href="/Learning-Deep-Learning/paper_notes/prevention_dataset.html">Notes</a>] <kbd>ITSC 2019</kbd> [dataset, cut-in]</li>
  <li><a href="https://sci-hub.tw/10.1109/IVS.2018.8500672">Semi-Automatic High-Accuracy Labelling Tool for Multi-Modal Long-Range Sensor Dataset</a> [<a href="paper_notes/prevention_annotation.md">Notes</a>] <kbd>IV 2018</kbd></li>
  <li><a href="https://www.astyx.com/fileadmin/redakteur/dokumente/Automotive_Radar_Dataset_for_Deep_learning_Based_3D_Object_Detection.PDF">Astyx dataset: Automotive Radar Dataset for Deep Learning Based 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/astyx_dataset.html">Notes</a>] <kbd>EuRAD 2019</kbd> (Astyx)</li>
  <li><a href="https://www.astyx.net/fileadmin/redakteur/dokumente/Deep_Learning_Based_3D_Object_Detection_for_Automotive_Radar_and_Camera.PDF">Astyx camera radar: Deep Learning Based 3D Object Detection for Automotive Radar and Camera</a> [<a href="/Learning-Deep-Learning/paper_notes/astyx_radar_camera_fusion.html">Notes</a>] <kbd>EuRAD 2019</kbd> (Astyx)</li>
</ul>

<h2 id="2019-12-12">2019-12 (12)</h2>
<ul>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf">How Do Neural Networks See Depth in Single Images?</a> [<a href="/Learning-Deep-Learning/paper_notes/what_monodepth_see.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.00275">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a> <kbd>ICRA 2019</kbd>  (depth completion)</li>
  <li><a href="https://arxiv.org/abs/1903.05421">DC: Depth Coefficients for Depth Completion</a> [<a href="/Learning-Deep-Learning/paper_notes/depth_coeff.html">Notes</a>] <kbd>CVPR 2019</kbd> [Xiaoming Liu, Multimodal]</li>
  <li><a href="https://arxiv.org/pdf/1611.02174.pdf">Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation</a> [<a href="/Learning-Deep-Learning/paper_notes/depth_from_one_line.html">Notes</a>] <kbd>ICRA 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1908.03127">VO-Monodepth: Enhancing self-supervised monocular depth estimation with traditional visual odometry</a> [<a href="/Learning-Deep-Learning/paper_notes/vo_monodepth.html">Notes</a>] <kbd>3DV 2019</kbd> (sparse to dense)</li>
  <li><a href="https://arxiv.org/abs/1811.10800">Probabilistic Object Detection: Definition and Evaluation</a> [<a href="/Learning-Deep-Learning/paper_notes/pdq.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1904.03215">The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/fishyscape.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/calib_modern_nn.html">Notes</a>] <kbd>ICML 2017</kbd> (Weinberger)</li>
  <li><a href="https://arxiv.org/abs/1708.02750">Extreme clicking for efficient object annotation</a> [<a href="/Learning-Deep-Learning/paper_notes/extreme_clicking.html">Notes</a>] <kbd>ICCV 2017</kbd></li>
  <li><a href="https://ml4ad.github.io/files/papers/Radar%20and%20Camera%20Early%20Fusion%20for%20Vehicle%20Detection%20in%20Advanced%20Driver%20Assistance%20Systems.pdf">Radar and Camera Early Fusion for Vehicle Detection in Advanced Driver Assistance Systems</a> [<a href="/Learning-Deep-Learning/paper_notes/radar_camera_qcom.html">Notes</a>] <kbd>NeurIPS 2019</kbd> (radar)</li>
  <li><a href="https://arxiv.org/abs/1901.10609">Deep Active Learning for Efficient Training of a LiDAR 3D Object Detector</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_active_learning_lidar.html">Notes</a>] <kbd>IV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1909.02533">C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion</a> [<a href="/Learning-Deep-Learning/paper_notes/c3dpo.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.02689">YOLACT: Real-time Instance Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/yolact.html">Notes</a>] <kbd>ICCV 2019</kbd> [single-stage instance seg]</li>
  <li><a href="https://arxiv.org/abs/1912.06218">YOLACT++: Better Real-time Instance Segmentation</a> [single-stage instance seg]</li>
</ul>

<h2 id="2019-11-20">2019-11 (20)</h2>
<ul>
  <li><a href="/Learning-Deep-Learning/paper_notes/review_descriptors.html">Review of Image and Feature Descriptors</a></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVRSUAD/Major_Vehicle_Detection_With_Automotive_Radar_Using_Deep_Learning_on_Range-Azimuth-Doppler_ICCVW_2019_paper.pdf">Vehicle Detection With Automotive Radar Using Deep Learning on Range-Azimuth-Doppler Tensors</a> [<a href="/Learning-Deep-Learning/paper_notes/radar_fft_qcom.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.06666">GPP: Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road</a> [<a href="/Learning-Deep-Learning/paper_notes/gpp.html">Notes</a>] <kbd>IV 2020</kbd> [UCSD, Trevidi, mono 3DOD]</li>
  <li><a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Choi_Multi-View_Reprojection_Architecture_for_Orientation_Estimation_ICCVW_2019_paper.pdf">MVRA: Multi-View Reprojection Architecture for Orientation Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/mvra.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLOv3: An Incremental Improvement</a></li>
  <li><a href="https://arxiv.org/abs/1904.04620">Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/gaussian_yolov3.html">Notes</a>] <kbd>ICCV 2019</kbd> (Detection with Uncertainty)</li>
  <li><a href="https://arxiv.org/abs/1905.10296">Bayesian YOLOv3: Uncertainty Estimation in One-Stage Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/bayesian_yolov3.html">Notes</a>] [DriveU]</li>
  <li><a href="https://arxiv.org/abs/1804.05132">Towards Safe Autonomous Driving: Capture Uncertainty in the Deep Neural Network For Lidar 3D Vehicle Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/towards_safe_ad.html">Notes</a>] <kbd>ITSC 2018</kbd> (DriveU)</li>
  <li><a href="https://arxiv.org/abs/1809.05590">Leveraging Heteroscedastic Aleatoric Uncertainties for Robust Real-Time LiDAR 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/towards_safe_ad2.html">Notes</a>] <kbd>IV 2019</kbd> (DriveU)</li>
  <li><a href="https://arxiv.org/abs/1909.12358">Can We Trust You? On Calibration of a Probabilistic Object Detector for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/towards_safe_ad_calib.html">Notes</a>] <kbd>IROS 2019</kbd> (DriveU)</li>
  <li><a href="https://arxiv.org/abs/1903.08701">LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/lasernet.html">Notes</a>] <kbd>CVPR 2019</kbd> (uncertainty)</li>
  <li><a href="https://arxiv.org/abs/1910.11375">LaserNet KL: Learning an Uncertainty-Aware Object Detector for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/lasernet_kl.html">Notes</a>] [LaserNet with KL divergence]</li>
  <li><a href="https://arxiv.org/abs/1807.11590">IoUNet: Acquisition of 	Localization Confidence for Accurate Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/iou_net.html">Notes</a>] <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1902.09630">gIoU: Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a> [<a href="/Learning-Deep-Learning/paper_notes/giou.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1705.08790">The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</a> <kbd>CVPR 2018</kbd> [IoU as loss]</li>
  <li><a href="https://arxiv.org/abs/1809.08545">KL Loss: Bounding Box Regression with Uncertainty for Accurate Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/kl_loss.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.02028">CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth</a> [<a href="/Learning-Deep-Learning/paper_notes/cam_conv.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.03838">BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors</a> [<a href="/Learning-Deep-Learning/paper_notes/bayes_od.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1906.04463">TW-SMNet: Deep Multitask Learning of Tele-Wide Stereo Matching</a> [<a href="/Learning-Deep-Learning/paper_notes/twsm_net.html">Notes</a>] <kbd>ICIP 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.00263">Accurate Uncertainties for Deep Learning Using Calibrated Regression</a> [<a href="/Learning-Deep-Learning/paper_notes/dl_regression_calib.html">Notes</a>] <kbd>ICML 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.11210">Calibrating Uncertainties in Object Localization Task</a> [<a href="/Learning-Deep-Learning/paper_notes/2dod_calib.html">Notes</a>] <kbd>NIPS 2018</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_On_the_Over-Smoothing_Problem_of_CNN_Based_Disparity_Estimation_ICCV_2019_paper.pdf">SMWA: On the Over-Smoothing Problem of CNN Based Disparity Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/smwa.html">Notes</a>] <kbd>ICCV 2019</kbd> [Multimodal, depth estimation]</li>
  <li><a href="https://arxiv.org/abs/1709.07492">Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image</a> [<a href="/Learning-Deep-Learning/paper_notes/sparse_to_dense.html">Notes</a>] <kbd>ICRA 2018</kbd> (depth completion)</li>
</ul>

<h2 id="2019-10-18">2019-10 (18)</h2>
<ul>
  <li><a href="/Learning-Deep-Learning/paper_notes/review_mono_3dod.html">Review of monocular object detection</a></li>
  <li><a href="/Learning-Deep-Learning/paper_notes/mono_3dod_2d3d_constraints.html">Review of 2D 3D contraints in Mono 3DOD</a></li>
  <li><a href="https://arxiv.org/abs/1905.05618">MonoGRNet 2: Monocular 3D Object Detection via Geometric Reasoning on Keypoints</a> [<a href="/Learning-Deep-Learning/paper_notes/monogrnet_russian.html">Notes</a>] [estimates depth from keypoints]</li>
  <li><a href="https://arxiv.org/abs/1703.07570">Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_manta.html">Notes</a>] <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1906.08070">SS3D: Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss</a> [<a href="/Learning-Deep-Learning/paper_notes/ss3d.html">Notes</a>] [rergess distance from images, centernet like]</li>
  <li><a href="https://arxiv.org/abs/1903.10955">GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/gs3d.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1907.06038">M3D-RPN: Monocular 3D Region Proposal Network for Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/m3d_rpn.html">Notes</a>] <kbd>ICCV 2019 oral</kbd> [3D anchors, cyclists, Xiaoming Liu]</li>
  <li><a href="https://arxiv.org/abs/1906.01193">TLNet: Triangulation Learning Network: from Monocular to Stereo 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/tlnet.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="http://wrap.warwick.ac.uk/114314/1/WRAP-survey-3D-object-detection-methods-autonomous-driving-applications-Arnold-2019.pdf">A Survey on 3D Object Detection Methods for Autonomous Driving Applications</a> [<a href="/Learning-Deep-Learning/paper_notes/3dod_review.html">Notes</a>] <kbd>TITS 2019</kbd> [Review]</li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8814050">BEV-IPM: Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image</a> [<a href="/Learning-Deep-Learning/paper_notes/bev_od_ipm.html">Notes</a>] <kbd>IV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1909.07701">ForeSeE: Task-Aware Monocular Depth Estimation for 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/foresee_mono3dod.html">Notes</a>] <kbd>AAAI 2020 oral</kbd> [successor to pseudo-lidar, mono 3DOD SOTA]</li>
  <li><a href="https://arxiv.org/abs/1909.04182">Obj-dist: Learning Object-specific Distance from a Monocular Image</a> [<a href="/Learning-Deep-Learning/paper_notes/obj_dist_iccv2019.html">Notes</a>] <kbd>ICCV 2019</kbd> (xmotors.ai + NYU) [monocular distance]</li>
  <li><a href="https://project.inria.fr/ppniv18/files/2018/10/paper22.pdf">DisNet: A novel method for distance estimation from monocular camera</a> [<a href="/Learning-Deep-Learning/paper_notes/disnet.html">Notes</a>] <kbd>IROS 2018</kbd> [monocular distance]</li>
  <li><a href="https://arxiv.org/abs/1904.08494">BirdGAN: Learning 2D to 3D Lifting for Object Detection in 3D for Autonomous Vehicles</a> [<a href="/Learning-Deep-Learning/paper_notes/birdgan.html">Notes</a>] <kbd>IROS 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1905.09970">Shift R-CNN: Deep Monocular 3D Object Detection with Closed-Form Geometric Constraints</a> [<a href="/Learning-Deep-Learning/paper_notes/shift_rcnn.html">Notes</a>] <kbd>ICIP 2019</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.pdf">3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</a> [<a href="/Learning-Deep-Learning/paper_notes/3d_rcnn.html">Notes</a>] <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.08601">Deep Optics for Monocular Depth Estimation and 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_optics.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1906.06059">MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/monoloco.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.10742">Joint Monocular 3D Vehicle Detection and Tracking</a> [<a href="/Learning-Deep-Learning/paper_notes/mono_3d_tracking.html">Notes</a>] <kbd>ICCV 2019</kbd> (Berkeley DeepDrive)</li>
  <li><a href="https://arxiv.org/abs/1909.01867">CasGeo: 3D Bounding Box Estimation for Autonomous Vehicles by Cascaded Geometric Constraints and Depurated 2D Detections Using 3D Results</a> [<a href="/Learning-Deep-Learning/paper_notes/casgeo.html">Notes</a>]</li>
</ul>

<h2 id="2019-09-17">2019-09 (17)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1812.08928">Slimmable Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/slimmable_networks.html">Notes</a>] <kbd>ICLR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.05134">Universally Slimmable Networks and Improved Training Techniques</a> [<a href="/Learning-Deep-Learning/paper_notes/universal_slimmable.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.11728">AutoSlim: Towards One-Shot Architecture Search for Channel Numbers</a></li>
  <li><a href="https://arxiv.org/pdf/1908.09791.pdf">Once for All: Train One Network and Specialize it for Efficient Deployment</a></li>
  <li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2018/03/2666.pdf">DOTA: A Large-scale Dataset for Object Detection in Aerial Images</a> [<a href="/Learning-Deep-Learning/paper_notes/dota.html">Notes</a>] <kbd>CVPR 2018</kbd> (rotated bbox)</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Ding_Learning_RoI_Transformer_for_Oriented_Object_Detection_in_Aerial_Images_CVPR_2019_paper.pdf">RoiTransformer: Learning RoI Transformer for Oriented Object Detection in Aerial Images</a> [<a href="/Learning-Deep-Learning/paper_notes/roi_transformer.html">Notes</a>] <kbd>CVPR 2019</kbd> (rotated bbox)</li>
  <li><a href="https://arxiv.org/abs/1703.01086">RRPN: Arbitrary-Oriented Scene Text Detection via Rotation Proposals</a> <kbd>TMM 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1706.09579">R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection</a> (rotated bbox)</li>
  <li><a href="https://training.ti.com/epd-pro-rap-mmwaveradar-adh-tr-webinar-eu">TI white paper: Webinar: mmWave Radar for Automotive and Industrial applications
</a> [<a href="/Learning-Deep-Learning/paper_notes/ti_mmwave_radar_webinar.html">Notes</a>] [TI, radar]</li>
  <li><a href="https://arxiv.org/abs/1610.05492">Federated Learning: Strategies for Improving Communication Efficiency</a> [<a href="/Learning-Deep-Learning/paper_notes/federated_learning_comm.html">Notes</a>] <kbd>NIPS 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1602.00763">sort: Simple Online and Realtime Tracking</a> [<a href="/Learning-Deep-Learning/paper_notes/sort.html">Notes</a>] <kbd>ICIP 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1703.07402">deep-sort: Simple Online and Realtime Tracking with a Deep Association Metric</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_sort.html">Notes</a>]</li>
  <li><a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/">MT-CNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/mtcnn.html">Notes</a>] <kbd>SPL 2016</kbd> (real time, facial landmark)</li>
  <li><a href="https://arxiv.org/abs/1905.00641">RetinaFace: Single-stage Dense Face Localisation in the Wild</a> [<a href="/Learning-Deep-Learning/paper_notes/retina_face.html">Notes</a>] <kbd>CVPR 2020</kbd> [joint object and landmark detection]</li>
  <li><a href="https://arxiv.org/abs/1908.10553">SC-SfM-Learner: Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video</a> [<a href="/Learning-Deep-Learning/paper_notes/sc_sfm_learner.html">Notes</a>] <kbd>NIPS 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1812.05050">SiamMask: Fast Online Object Tracking and Segmentation: A Unifying Approach</a> <kbd>CVPR 2019</kbd> (tracking, segmentation, label propagation)</li>
  <li><a href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/">Review of Kálmán Filter</a> (from Tim Babb, Pixar Animation) [<a href="/Learning-Deep-Learning/paper_notes/kalman_filter.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1605.06409">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/rfcn.html">Notes</a>] <kbd>NIPS 2016</kbd></li>
  <li><a href="https://arxiv.org/pdf/1412.6806.pdf">Guided backprop: Striving for Simplicity: The All Convolutional Net</a> [<a href="/Learning-Deep-Learning/paper_notes/guided_backprop.html">Notes</a>] <kbd>ICLR 2015</kbd></li>
  <li><a href="http://www.cs.cmu.edu/~mvo/index_files/Papers/ONet_19.pdf">Occlusion-Net: 2D/3D Occluded Keypoint Localization Using Graph Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/occlusion_net.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://boxy-dataset.com/boxy/index">Boxy Vehicle Detection in Large Images</a> [<a href="/Learning-Deep-Learning/paper_notes/boxy.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.12681">FQNet: Deep Fitting Degree Scoring Network for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/fqnet.html">Notes</a>] <kbd>CVPR 2019</kbd> [Mono 3DOD, Jiwen Lu]</li>
</ul>

<h2 id="2019-08-18">2019-08 (18)</h2>
<ul>
  <li><a href="https://www.cs.toronto.edu/~urtasun/publications/chen_etal_cvpr16.pdf">Mono3D: Monocular 3D Object Detection for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/mono3d.html">Notes</a>] <kbd>CVPR2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1905.12365">MonoDIS: Disentangling Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/monodis.html">Notes</a>] <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.09847">Pseudo lidar-e2e: Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud</a> [<a href="/Learning-Deep-Learning/paper_notes/pseudo_lidar_e2e.html">Notes</a>] <kbd>ICCV 2019</kbd> (pseudo-lidar with 2d and 3d consistency loss, better than PL and worse than PL++, SOTA for pure mono3D)</li>
  <li><a href="https://arxiv.org/abs/1811.10247">MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization</a> [<a href="/Learning-Deep-Learning/paper_notes/monogrnet.html">Notes</a>] <kbd>AAAI 2019</kbd> (SOTA of Mono3DOD, MLF &lt; MonoGRNet &lt; Pseudo-lidar)</li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Multi-Level_Fusion_Based_CVPR_2018_paper.pdf">MLF: Multi-Level Fusion based 3D Object Detection from Monocular Images</a> [<a href="/Learning-Deep-Learning/paper_notes/mlf.html">Notes</a>] <kbd>CVPR 2018</kbd> (precursor to pseudo-lidar)</li>
  <li><a href="https://arxiv.org/abs/1812.02781">ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape</a> [<a href="/Learning-Deep-Learning/paper_notes/roi10d.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.11444">AM3D: Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/am3d.html">Notes</a>] <kbd>ICCV 2019</kbd> [similar to pseudo-lidar, color-enhanced]</li>
  <li><a href="https://arxiv.org/abs/1901.03446">Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors</a> [<a href="/Learning-Deep-Learning/paper_notes/mono3d++.html">Notes</a>] (from Stefano Soatto) <kbd>AAAI 2019</kbd></li>
  <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613841">Deep Metadata Fusion for Traffic Light to Lane Assignment</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_lane_association.html">Notes</a>] <kbd>IEEE RA-L 2019</kbd> (traffic lights association)</li>
  <li><a href="https://ieeexplore.ieee.org/document/8569421">Automatic Traffic Light to Ego Vehicle Lane Association at Complex Intersections</a> <kbd>ITSC 2019</kbd> (traffic lights association)</li>
  <li><a href="https://arxiv.org/abs/1901.10951">Distant Vehicle Detection Using Radar and Vision</a>[<a href="/Learning-Deep-Learning/paper_notes/distant_object_radar.html">Notes</a>] <kbd>ICRA 2019</kbd> [radar, vision, radar tracklets fusion]</li>
  <li><a href="https://iopscience.iop.org/article/10.1088/1742-6596/1168/3/032040/pdf">Distance Estimation of Monocular Based on Vehicle Pose Information</a> [<a href="/Learning-Deep-Learning/paper_notes/distance_estimation_pose_radar.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1705.07115">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</a> [<a href="/Learning-Deep-Learning/paper_notes/uncertainty_multitask.html">Notes</a>] <kbd>CVPR 2018</kbd> (Alex Kendall)</li>
  <li><a href="https://arxiv.org/abs/1711.02257">GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/gradnorm.html">Notes</a>] <kbd>ICML 2018</kbd> (multitask)</li>
  <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Michelle_Guo_Focus_on_the_ECCV_2018_paper.pdf">DTP: Dynamic Task Prioritization for Multitask Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/dtp.html">Notes</a>] <kbd>ECCV 2018</kbd> [multitask, Stanford]</li>
  <li><a href="https://ieeexplore.ieee.org/document/6856477/">Will this car change the lane? - Turn signal recognition in the frequency domain</a> [<a href="/Learning-Deep-Learning/paper_notes/tsl_frequency.html">Notes</a>] <kbd>IV 2014</kbd></li>
  <li><a href="https://arxiv.org/abs/1803.06199">Complex-YOLO: Real-time 3D Object Detection on Point Clouds</a> [<a href="/Learning-Deep-Learning/paper_notes/complex_yolo.html">Notes</a>] (BEV detection only)</li>
  <li><a href="https://arxiv.org/abs/1904.07537">Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds</a> <kbd>CVPR 2019</kbd> (sensor fusion and tracking)</li>
  <li><a href="https://arxiv.org/abs/1807.03247">An intriguing failing of convolutional neural networks and the CoordConv solution</a> [<a href="/Learning-Deep-Learning/paper_notes/coord_conv.html">Notes</a>] <kbd>NIPS 2018</kbd></li>
</ul>

<h2 id="2019-07-19">2019-07 (19)</h2>
<ul>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf">Deep Parametric Continuous Convolutional Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/parametric_cont_conv.html">Notes</a>] <kbd>CVPR 2018</kbd> (@Uber, sensor fusion)</li>
  <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf">ContFuse: Deep Continuous Fusion for Multi-Sensor 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/contfuse.html">Notes</a>] <kbd>ECCV 2018</kbd> [Uber ATG, sensor fusion, BEV]</li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Fast_and_Furious_CVPR_2018_paper.pdf">Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net</a> [<a href="/Learning-Deep-Learning/paper_notes/faf.html">Notes</a>] <kbd>CVPR 2018 oral</kbd> [lidar only, perception and prediction]</li>
  <li><a href="https://arxiv.org/pdf/1904.04998.pdf">LearnK: Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras</a> [<a href="/Learning-Deep-Learning/paper_notes/learnk.html">Notes</a>] <kbd>ICCV 2019</kbd> [monocular depth estimation, intrinsic estimation, SOTA]</li>
  <li><a href="https://arxiv.org/abs/1609.03677">monodepth: Unsupervised Monocular Depth Estimation with Left-Right Consistency</a> [<a href="/Learning-Deep-Learning/paper_notes/monodepth.html">Notes</a>] <kbd>CVPR 2017 oral</kbd> (monocular depth estimation, stereo for training)</li>
  <li><a href="https://arxiv.org/abs/1811.06152">Struct2depth: Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos</a> [<a href="/Learning-Deep-Learning/paper_notes/struct2depth.html">Notes</a>] <kbd>AAAI 2019</kbd> [monocular depth estimation, estimating movement of dynamic object, infinite depth problem, online finetune]</li>
  <li><a href="https://arxiv.org/pdf/1711.03665.pdf">Unsupervised Learning of Geometry with Edge-aware Depth-Normal Consistency</a> [<a href="/Learning-Deep-Learning/paper_notes/edge_aware_depth_normal.html">Notes</a>] <kbd>AAAI 2018</kbd> (monocular depth estimation, static assumption, surface normal)</li>
  <li><a href="https://arxiv.org/pdf/1803.05648.pdf">LEGO Learning Edge with Geometry all at Once by Watching Videos</a> [<a href="/Learning-Deep-Learning/paper_notes/lego.html">Notes</a>] <kbd>CVPR 2018 spotlight</kbd> (monocular depth estimation, static assumption, surface normal)</li>
  <li><a href="https://arxiv.org/abs/1902.05394">Object Detection and 3D Estimation via an FMCW Radar Using a Fully Convolutional Network</a> [<a href="/Learning-Deep-Learning/paper_notes/radar_3d_od_fcn.html">Notes</a>] (radar, RD map, OD, Arxiv 201902)</li>
  <li><a href="https://www.researchgate.net/publication/330748053_A_Study_on_Radar_Target_Detection_Based_on_Deep_Neural_Networks">A study on Radar Target Detection Based on Deep Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/radar_target_detection_tsinghua.html">Notes</a>] (radar, RD map, OD)</li>
  <li><a href="https://arxiv.org/abs/1904.08414">2D Car Detection in Radar Data with PointNets</a> [<a href="/Learning-Deep-Learning/paper_notes/radar_detection_pointnet.html">Notes</a>] (from Ulm Univ, radar, point cloud, OD, Arxiv 201904)</li>
  <li><a href="https://arxiv.org/abs/1802.04865">Learning Confidence for Out-of-Distribution Detection in Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/learning_ood_conf.html">Notes</a>] (budget to cheat)</li>
  <li><a href="/Learning-Deep-Learning/assets/papers/bosch_traffic_lights.pdf">A Deep Learning Approach to Traffic Lights: Detection, Tracking, and Classification</a> [<a href="/Learning-Deep-Learning/paper_notes/bosch_traffic_lights.html">Notes</a>] <kbd>ICRA 2017</kbd> (Bosch, traffic lights)</li>
  <li><a href="https://arxiv.org/abs/1705.08280">How hard can it be? Estimating the difficulty of visual search in an image</a> [<a href="/Learning-Deep-Learning/paper_notes/how_hard_can_it_be.html">Notes</a>] <kbd>CVPR 2016</kbd></li>
  <li><a href="https://arxiv.org/pdf/1902.07830.pdf">Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_fusion_review.html">Notes</a>] (review from Bosch)</li>
  <li><a href="https://zhuanlan.zhihu.com/p/57029694">Review of monocular 3d object detection</a> (blog from 知乎)</li>
  <li><a href="https://arxiv.org/abs/1612.00496">Deep3dBox: 3D Bounding Box Estimation Using Deep Learning and Geometry</a> [<a href="/Learning-Deep-Learning/paper_notes/deep3dbox.html">Notes</a>] <kbd>CVPR 2017</kbd> [Zoox]</li>
  <li><a href="https://arxiv.org/abs/1904.01690">MonoPSR: Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction</a> [<a href="/Learning-Deep-Learning/paper_notes/monopsr.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.08188">OFT: Orthographic Feature Transform for Monocular 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/oft.html">Notes</a>] <kbd>BMVC 2019</kbd> [Convert camera to BEV, Alex Kendall]</li>
</ul>

<h2 id="2019-06-12">2019-06 (12)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1905.02249">MixMatch: A Holistic Approach to Semi-Supervised Learning</a> [<a href="/Learning-Deep-Learning/paper_notes/MixMatch.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/efficientnet.html">Notes</a>] <kbd>ICML 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1703.04977">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</a> [<a href="/Learning-Deep-Learning/paper_notes/uncertainty_bdl.html">Notes</a>] <kbd>NIPS 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1511.02680.pdf">Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding</a> [<a href="/Learning-Deep-Learning/paper_notes/bayesian_segnet.html">Notes</a>]<kbd>BMVC 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1811.02146.pdf">TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents</a> [<a href="/Learning-Deep-Learning/paper_notes/trafficpredict.html">Notes</a>] <kbd>AAAI 2019 oral</kbd></li>
  <li><a href="https://arxiv.org/pdf/1803.09326.pdf">Deep Depth Completion of a Single RGB-D Image</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_depth_completion_rgbd.html">Notes</a>] <kbd>CVPR 2018</kbd> (indoor)</li>
  <li><a href="https://arxiv.org/pdf/1812.00488v2.pdf">DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image</a> [<a href="/Learning-Deep-Learning/paper_notes/deeplidar.html">Notes</a>] <kbd>CVPR 2019</kbd> (outdoor)</li>
  <li><a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf">SfMLearner: Unsupervised Learning of Depth and Ego-Motion from Video</a> [<a href="/Learning-Deep-Learning/paper_notes/sfm_learner.html">Notes</a>] <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1806.01260">Monodepth2: Digging Into Self-Supervised Monocular Depth Estimation</a> [<a href="/Learning-Deep-Learning/paper_notes/monodepth2.html">Notes</a>] <kbd>ICCV 2019</kbd> [Niantic]</li>
  <li><a href="https://arxiv.org/pdf/1905.01333.pdf">DeepSignals: Predicting Intent of Drivers Through Visual Signals</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_signals.html">Notes</a>] <kbd>ICRA 2019</kbd> (@Uber, turn signal detection)</li>
  <li><a href="https://arxiv.org/abs/1904.01355">FCOS: Fully Convolutional One-Stage Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/fcos.html">Notes</a>] <kbd>ICCV 2019</kbd> [Chunhua Shen]</li>
  <li><a href="https://arxiv.org/abs/1906.06310">Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/pseudo_lidar++.html">Notes</a>] <kbd>ICLR 2020</kbd></li>
  <li><a href="http://www.cs.toronto.edu/~byang/papers/mmf.pdf">MMF: Multi-Task Multi-Sensor Fusion for 3D Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/mmf.html">Notes</a>] <kbd>CVPR 2019</kbd> (@Uber, sensor fusion)</li>
</ul>

<h2 id="2019-05-18">2019-05 (18)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1904.07850">CenterNet: Objects as points</a> (from ExtremeNet authors) [<a href="/Learning-Deep-Learning/paper_notes/centernet.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1904.08189">CenterNet: Object Detection with Keypoint Triplets</a> [<a href="paper_notes/centernet_cas.md">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1901.08225">Object Detection based on Region Decomposition and Assembly</a> [<a href="/Learning-Deep-Learning/paper_notes/object_detection_region_decomposition.html">Notes</a>] <kbd>AAAI 2019 </kbd></li>
  <li><a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/lottery_ticket_hypothesis.html">Notes</a>] <kbd>ICLR 2019 </kbd></li>
  <li><a href="https://arxiv.org/abs/1811.04533">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a> [<a href="paper_notes/m2det.md">Notes</a>] <kbd>AAAI 2019 </kbd></li>
  <li><a href="https://arxiv.org/abs/1906.12187">Deep Radar Detector</a> [<a href="/Learning-Deep-Learning/paper_notes/deep_radar_detector.html">Notes</a>] <kbd>RadarCon 2019</kbd></li>
  <li><a href="https://ieeexplore.ieee.org/document/8455344">Semantic Segmentation on Radar Point Clouds</a> [[<a href="/Learning-Deep-Learning/paper_notes/radar_point_semantic_seg.html">Notes</a>]] (from Daimler AG) <kbd>FUSION 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1608.08710.pdf">Pruning Filters for Efficient ConvNets</a> [<a href="/Learning-Deep-Learning/paper_notes/pruning_filters.html">Notes</a>] <kbd>ICLR 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1810.00518.pdf">Layer-compensated Pruning for Resource-constrained Convolutional Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/layer_compensated_pruning.html">Notes</a>] <kbd>NIPS 2018 talk</kbd></li>
  <li><a href="https://arxiv.org/pdf/1904.12368.pdf">LeGR: Filter Pruning via Learned Global Ranking</a> [<a href="/Learning-Deep-Learning/paper_notes/legr.html">Notes</a>] <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/pdf/1904.07392.pdf">NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/nas_fpn.html">Notes</a>] <kbd>CVPR 2019 </kbd></li>
  <li><a href="https://arxiv.org/abs/1805.09501">AutoAugment: Learning Augmentation Policies from Data</a> [<a href="/Learning-Deep-Learning/paper_notes/autoaugment.html">Notes</a>] <kbd>CVPR 2019 </kbd></li>
  <li><a href="https://arxiv.org/pdf/1803.01534.pdf">Path Aggregation Network for Instance Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/panet.html">Notes</a>] <kbd>CVPR 2018 </kbd></li>
  <li><a href="https://arxiv.org/pdf/1707.06168.pdf">Channel Pruning for Accelerating Very Deep Neural Networks</a> <kbd>ICCV 2017</kbd> (Face++, Yihui He) [<a href="/Learning-Deep-Learning/paper_notes/channel_pruning_megvii.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1802.03494.pdf">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</a> <kbd>ECCV 2018</kbd> (Song Han, Yihui He)</li>
  <li><a href="https://arxiv.org/pdf/1905.02244.pdf">MobileNetV3: Searching for MobileNetV3</a> [<a href="/Learning-Deep-Learning/paper_notes/mobilenets_v3.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1807.11626.pdf">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a> [<a href="/Learning-Deep-Learning/paper_notes/mnasnet.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1810.05270.pdf">Rethinking the Value of Network Pruning</a> <kbd>ICLR 2019</kbd></li>
</ul>

<h2 id="2019-04-12">2019-04 (12)</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/1801.04381.pdf">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> (MobileNets v2) [<a href="/Learning-Deep-Learning/paper_notes/mobilenets_v2.html">Notes</a>] <kbd>CVPR 2018</kbd></li>
  <li><a href="http://www.cvlibs.net/publications/Fritsch2013ITSC.pdf">A New Performance Measure and Evaluation Benchmark
for Road Detection Algorithms</a> [<a href="/Learning-Deep-Learning/paper_notes/kitti_lane.html">Notes</a>] <kbd>ITSC 2013</kbd></li>
  <li><a href="https://arxiv.org/pdf/1612.07695.pdf">MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/multinet_raquel.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1803.08707.pdf">Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction</a> (Very nice illustration of 1 and 2 stage object detection)</li>
  <li><a href="https://arxiv.org/pdf/1711.07264.pdf">Light-Head R-CNN: In Defense of Two-Stage Object Detector</a> [<a href="/Learning-Deep-Learning/paper_notes/lighthead_rcnn.html">Notes</a>] (from Megvii)</li>
  <li><a href="https://arxiv.org/abs/1904.02948">CSP: High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/csp_pedestrian.html">Notes</a>] <kbd>CVPR 2019</kbd> [center and scale prediction, anchor-free, near SOTA pedestrian]</li>
  <li><a href="https://zhuanlan.zhihu.com/p/62103812">Review of Anchor-free methods (知乎Blog) 目标检测：Anchor-Free时代</a> <a href="https://zhuanlan.zhihu.com/p/64563186">Anchor free深度学习的目标检测方法</a> <a href="https://docs.google.com/presentation/d/1_dUfxv63108bZXUnVYPIOAdEIkRZw5BR9-rOp-Ni0X0/">My Slides on CSP</a></li>
  <li><a href="https://arxiv.org/pdf/1509.04874.pdf">DenseBox: Unifying Landmark Localization with End to End Object Detection</a></li>
  <li><a href="https://arxiv.org/pdf/1808.01244.pdf">CornerNet: Detecting Objects as Paired Keypoints</a> [<a href="/Learning-Deep-Learning/paper_notes/cornernet.html">Notes</a>] <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1901.08043.pdf">ExtremeNet: Bottom-up Object Detection by Grouping Extreme and Center Points</a> [<a href="/Learning-Deep-Learning/paper_notes/extremenet.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1903.00621.pdf">FSAF: Feature Selective Anchor-Free Module for Single-Shot Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/fsaf_detection.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1904.03797v1.pdf">FoveaBox: Beyond Anchor-based Object Detector</a> (anchor-free) [<a href="/Learning-Deep-Learning/paper_notes/foveabox.html">Notes</a>]</li>
</ul>

<h2 id="2019-03-19">2019-03 (19)</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/1902.04103.pdf">Bag of Freebies for Training Object Detection Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/bag_of_freebies_object_detection.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1710.09412.pdf">mixup: Beyond Empirical Risk Minimization</a> [<a href="/Learning-Deep-Learning/paper_notes/mixup.html">Notes</a>] <kbd>ICLR 2018</kbd></li>
  <li><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf">Multi-view Convolutional Neural Networks for 3D Shape Recognition</a> (MVCNN) [<a href="/Learning-Deep-Learning/paper_notes/mvcnn.html">Notes</a>] <kbd>ICCV 2015</kbd></li>
  <li><a href="http://3dshapenets.cs.princeton.edu/paper.pdf">3D ShapeNets: A Deep Representation for Volumetric Shapes</a> [<a href="/Learning-Deep-Learning/paper_notes/3d_shapenets.html">Notes</a>] <kbd>CVPR 2015</kbd></li>
  <li><a href="https://arxiv.org/pdf/1604.03265.pdf">Volumetric and Multi-View CNNs for Object Classification on 3D Data</a> [<a href="/Learning-Deep-Learning/paper_notes/vol_vs_mvcnn.html">Notes</a>] <kbd>CVPR 2016</kbd></li>
  <li><a href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a> [<a href="/Learning-Deep-Learning/paper_notes/groupnorm.html">Notes</a>] <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1506.02025.pdf">Spatial Transformer Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/stn.html">Notes</a>] <kbd>NIPS 2015</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.08488.pdf">Frustum PointNets for 3D Object Detection from RGB-D Data</a> (F-PointNet) [<a href="/Learning-Deep-Learning/paper_notes/frustum_pointnet.html">Notes</a>] <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1801.07829.pdf">Dynamic Graph CNN for Learning on Point Clouds</a> [<a href="/Learning-Deep-Learning/paper_notes/edgeconv.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1812.04244">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a> (SOTA for 3D object detection) [<a href="/Learning-Deep-Learning/paper_notes/point_rcnn.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1611.07759">MV3D: Multi-View 3D Object Detection Network for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/mv3d.html">Notes</a>] <kbd>CVPR 2017</kbd> (Baidu, sensor fusion, BV proposal)</li>
  <li><a href="https://arxiv.org/abs/1712.02294">AVOD: Joint 3D Proposal Generation and Object Detection from View Aggregation</a> [<a href="/Learning-Deep-Learning/paper_notes/avod.html">Notes</a>] <kbd>IROS 2018</kbd> (sensor fusion, multiview proposal)</li>
  <li><a href="https://arxiv.org/pdf/1704.04861.pdf">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> [<a href="/Learning-Deep-Learning/paper_notes/mobilenets.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/1812.07179">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gafp in 3D Object Detection for Autonomous Driving</a> [<a href="/Learning-Deep-Learning/paper_notes/pseudo_lidar.html">Notes</a>] <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.06396.pdf">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a> <kbd>CVPR 2018</kbd> (Apple, first end-to-end point cloud encoding to grid)</li>
  <li><a href="https://www.mdpi.com/1424-8220/18/10/3337/pdf">SECOND: Sparsely Embedded Convolutional Detection</a> <kbd>Sensors 2018</kbd> (builds on VoxelNet)</li>
  <li><a href="https://arxiv.org/abs/1812.05784">PointPillars: Fast Encoders for Object Detection from Point Clouds</a> [<a href="/Learning-Deep-Learning/paper_notes/point_pillars.html">Notes</a>] <kbd>CVPR 2019</kbd> (builds on SECOND)</li>
  <li><a href="http://www.cvlibs.net/publications/Geiger2012CVPR.pdf">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</a> [<a href="/Learning-Deep-Learning/paper_notes/kitti.html">Notes</a>] <kbd>CVPR 2012</kbd></li>
  <li><a href="http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf">Vision meets Robotics: The KITTI Dataset</a> [<a href="/Learning-Deep-Learning/paper_notes/kitti.html">Notes</a>] <kbd>IJRR 2013</kbd></li>
</ul>

<h2 id="2019-02-9">2019-02 (9)</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/1705.07750.pdf">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a> (I3D) [<a href="/Learning-Deep-Learning/paper_notes/quo_vadis_i3d.html">Notes</a>]<kbd>Video</kbd> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1503.07274.pdf">Initialization Strategies of Spatio-Temporal Convolutional Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/quo_vadis_i3d.html">Notes</a>] <kbd>Video</kbd></li>
  <li><a href="https://arxiv.org/pdf/1712.09184.pdf">Detect-and-Track: Efficient Pose Estimation in Videos</a> [<a href="/Learning-Deep-Learning/paper_notes/quo_vadis_i3d.html">Notes</a>] <kbd>ICCV 2017</kbd> <kbd>Video</kbd></li>
  <li><a href="https://arxiv.org/pdf/1809.07082">Deep Learning Based Rib Centerline Extraction and Labeling</a> [<a href="/Learning-Deep-Learning/paper_notes/rib_centerline_philips.html">Notes</a>] <kbd>MI</kbd> <kbd>MICCAI 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1812.03982.pdf">SlowFast Networks for Video Recognition</a> [<a href="/Learning-Deep-Learning/paper_notes/slowfast.html">Notes</a>] <kbd>ICCV 2019 Oral</kbd></li>
  <li><a href="https://arxiv.org/pdf/1611.05431.pdf">Aggregated Residual Transformations for Deep Neural Networks</a> (ResNeXt) [<a href="/Learning-Deep-Learning/paper_notes/resnext.html">Notes</a>] <kbd>CVPR 2017</kbd></li>
  <li><a href="https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/">Beyond the pixel plane: sensing and learning in 3D</a> (blog, <a href="https://zhuanlan.zhihu.com/p/44386618">中文版本</a>)</li>
  <li><a href="https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</a> (VoxNet) [<a href="/Learning-Deep-Learning/paper_notes/voxnet.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1612.00593.pdf">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a> <kbd>CVPR 2017</kbd> [<a href="/Learning-Deep-Learning/paper_notes/pointnet.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1706.02413.pdf">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a> <kbd>NIPS 2017</kbd> [<a href="/Learning-Deep-Learning/paper_notes/pointnet++.html">Notes</a>]</li>
  <li><a href="https://zhuanlan.zhihu.com/p/36888114">Review of Geometric deep learning 几何深度学习前沿 (from 知乎)</a> (Up to CVPR 2018)</li>
</ul>

<h2 id="2019-01-10">2019-01 (10)</h2>
<ul>
  <li><a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">DQN: Human-level control through deep reinforcement learning (Nature DQN paper)</a> [<a href="/Learning-Deep-Learning/paper_notes/nature_dqn_paper.html">Notes</a>] <kbd>DRL</kbd></li>
  <li><a href="https://arxiv.org/pdf/1811.08661.pdf">Retina U-Net: Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection</a> [<a href="/Learning-Deep-Learning/paper_notes/retina_unet.html">Notes</a>] <kbd>MI</kbd></li>
  <li><a href="https://arxiv.org/pdf/1801.00868.pdf">Panoptic Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/panoptic_segmentation.html">Notes</a>] <kbd>PanSeg</kbd></li>
  <li><a href="https://arxiv.org/pdf/1901.02446.pdf">Panoptic Feature Pyramid Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/panoptic_fpn.html">Notes</a>] <kbd>PanSeg</kbd></li>
  <li><a href="https://arxiv.org/pdf/1812.03904.pdf">Attention-guided Unified Network for Panoptic Segmentation</a> [<a href="/Learning-Deep-Learning/paper_notes/AUNet_panoptic.html">Notes</a>] <kbd>PanSeg</kbd></li>
  <li><a href="https://arxiv.org/pdf/1812.01187.pdf">Bag of Tricks for Image Classification with Convolutional Neural Networks</a> [<a href="paper_notes/bag_of_tricks_cnn.md">Notes</a>] <kbd>CLS</kbd></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-00937-3_86">Deep Reinforcement Learning for Vessel Centerline Tracing in Multi-modality 3D Volumes</a> [<a href="/Learning-Deep-Learning/paper_notes/drl_vessel_centerline.html">Notes</a>] <kbd>DRL</kbd> <kbd>MI</kbd></li>
  <li><a href="http://cs229.stanford.edu/proj2015/362_report.pdf">Deep Reinforcement Learning for Flappy Bird</a> [<a href="/Learning-Deep-Learning/paper_notes/drl_flappy.html">Notes</a>] <kbd>DRL</kbd></li>
  <li><a href="https://arxiv.org/pdf/1812.05038.pdf">Long-Term Feature Banks for Detailed Video Understanding</a> [<a href="/Learning-Deep-Learning/paper_notes/long_term_feat_bank.html">Notes</a>] <kbd>Video</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.07971.pdf">Non-local Neural Networks</a> [<a href="/Learning-Deep-Learning/paper_notes/non_local_net.html">Notes</a>] <kbd>Video</kbd> <kbd>CVPR 2018</kbd></li>
</ul>

<h2 id="2018">2018</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a></li>
  <li><a href="https://arxiv.org/pdf/1712.00726.pdf">Cascade R-CNN: Delving into High Quality Object Detection</a></li>
  <li><a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a> (RetinaNet) [<a href="/Learning-Deep-Learning/paper_notes/focal_loss.html">Notes</a>]</li>
  <li><a href="https://arxiv.org/pdf/1709.01507">Squeeze-and-Excitation Networks</a> (SENet)</li>
  <li><a href="https://arxiv.org/pdf/1710.10196.pdf">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a></li>
  <li><a href="https://arxiv.org/abs/1703.06211">Deformable Convolutional Networks</a> <kbd>ICCV 2017</kbd> [build on R-FCN]</li>
  <li><a href="https://arxiv.org/pdf/1803.07066.pdf">Learning Region Features for Object Detection</a></li>
</ul>

<h2 id="2017-and-before">2017 and before</h2>
<ul>
  <li><a href="Learning_notes.md">Learning notes on Deep Learning</a></li>
  <li><a href="List_of_Machine_Learning_Papers.md">List of Papers on Machine Learning</a></li>
  <li><a href="paper_notes/cnn_papers.md">Notes of Literature Review on CNN in CV</a> This is the notes for all the papers in the recommended list <a href="papers_and_books_to_start.md">here</a></li>
  <li><a href="misc.md">Notes of Literature Review (Others)</a></li>
  <li><a href="ML_DL_environment_Setup.md">Notes on how to set up DL/ML environment</a></li>
  <li><a href="installation_log.md">Useful setup notes</a></li>
</ul>

<h2 id="papers-to-read">Papers to Read</h2>
<p>Here is the list of papers waiting to be read.</p>
<h3 id="deep-learning-in-general">Deep Learning in general</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1612.01051.pdf">SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/pdf/1706.02677.pdf">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></li>
  <li><a href="https://openreview.net/forum?id=Bygh9j09KX">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</a> <kbd>ICML 2019</kbd></li>
  <li><a href="https://openreview.net/forum?id=SkfMWhAqYQ">Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet</a> (BagNet) <a href="https://blog.evjang.com/2019/02/bagnet.html">blog</a> <kbd>ICML 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1803.09820v2.pdf">A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay</a></li>
  <li><a href="https://arxiv.org/pdf/1611.03530.pdf">Understanding deep learning requires rethinking generalization</a></li>
  <li><a href="https://arxiv.org/abs/1409.7495">Gradient Reversal: Unsupervised Domain Adaptation by Backpropagation</a> <kbd>ICML 2015</kbd></li>
</ul>

<h3 id="self-training">Self-training</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2006.06882">Rethinking Pre-training and Self-training</a> <kbd>NeurIPS 2020</kbd> [Quoc Le]</li>
</ul>

<h3 id="2d-object-detection-and-segmentation">2D Object Detection and Segmentation</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1903.00241.pdf">Mask Scoring R-CNN</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1604.03540.pdf">Training Region-based Object Detectors with Online Hard Example Mining</a></li>
  <li><a href="https://arxiv.org/abs/1911.09358">Gliding vertex on the horizontal bounding box for multi-oriented object detection</a></li>
  <li><a href="https://arxiv.org/abs/2003.04668">ONCE: Incremental Few-Shot Object Detection</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1803.03243">Domain Adaptive Faster R-CNN for Object Detection in the Wild</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1708.07819">Foggy Cityscapes: Semantic Foggy Scene Understanding with Synthetic Data</a> <kbd>IJCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1808.01265">Foggy Cityscapes ECCV: Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy Scene Understanding</a> <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1710.06677">Dropout Sampling for Robust Object Detection in Open-Set Conditions</a> <kbd>ICRA 2018</kbd> (Niko Sünderhauf)</li>
  <li><a href="https://arxiv.org/abs/1901.07518">Hybrid Task Cascade for Instance Segmentation</a> <kbd>CVPR 2019</kbd> (cascaded mask RCNN)</li>
  <li><a href="https://arxiv.org/abs/1809.06006">Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection</a> <kbd>ICRA 2019</kbd> (Niko Sünderhauf)</li>
  <li><a href="https://arxiv.org/pdf/1901.03784.pdf">A Unified Panoptic Segmentation Network</a> <kbd>CVPR 2019</kbd> <kbd>PanSeg</kbd></li>
  <li><a href="https://arxiv.org/pdf/1903.11900.pdf">Model Vulnerability to Distributional Shifts over Image Transformation Sets</a> (CVPR workshop) <a href="https://www.reddit.com/r/MachineLearning/comments/b81uwq/r_model_vulnerability_to_distributional_shifts/">tl:dr</a></li>
  <li><a href="https://arxiv.org/pdf/1904.07305.pdf">Automatic adaptation of object detectors to new domains using self-training</a> <kbd>CVPR 2019</kbd> (find corner case and boost)</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Weakly%20Supervised%20Learning%20for%20Real-World%20Computer%20Vision%20Applications/Xu_Missing_Labels_in_Object_Detection_CVPRW_2019_paper.pdf">Missing Labels in Object Detection</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1509.04874.pdf">DenseBox: Unifying Landmark Localization with End to End Object Detection</a></li>
  <li><a href="https://www.researchgate.net/publication/309365539_Circular_Object_Detection_in_Polar_Coordinates_for_2D_LIDAR_DataCCPR2016">Circular Object Detection in Polar Coordinates for 2D LIDAR Data</a> <kbd>CCPR 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.10633">LFFD: A Light and Fast Face Detector for Edge Devices</a> [Lightweight, face detection, car detection]</li>
  <li><a href="https://arxiv.org/abs/1608.01471">UnitBox: An Advanced Object Detection Network</a> <kbd>ACM MM 2016</kbd> [Ln IoU loss, Thomas Huang]</li>
</ul>

<h3 id="fisheye">Fisheye</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2012.02124">Generalized Object Detection on Fisheye Cameras for Autonomous Driving: Dataset, Representations and Baseline</a> <kbd>WACV 2021</kbd></li>
</ul>

<h3 id="video-understanding">Video Understanding</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1412.0767.pdf">Learning Spatiotemporal Features with 3D Convolutional Networks</a> (C3D)  <kbd>Video </kbd><kbd>ICCV 2015 </kbd></li>
  <li><a href="https://arxiv.org/pdf/1705.08421.pdf">AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions</a></li>
  <li><a href="https://arxiv.org/pdf/1611.02155.pdf">Spatiotemporal Residual Networks for Video Action Recognition</a> (decouple spatiotemporal) <kbd>NIPS 2016</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.10305.pdf">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</a> (P3D, decouple spatiotemporal) <kbd>ICCV 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.11248.pdf">A Closer Look at Spatiotemporal Convolutions for Action Recognition</a> (decouple spatiotemporal) <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1712.04851.pdf">Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</a> (decouple spatiotemporal) <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.09577.pdf">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_AGSS-VOS_Attention_Guided_Single-Shot_Video_Object_Segmentation_ICCV_2019_paper.pdf">AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1611.05198">One-Shot Video Object Segmentation</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.10172">Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Towards_High_Performance_CVPR_2018_paper.pdf">Towards High Performance Video Object Detection</a> [<a href="paper_notes/high_performance_video_od.md">Notes</a>] <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1804.05830">Towards High Performance Video Object Detection for Mobiles</a> [<a href="paper_notes/high_performance_video_od_mobile.md">Notes</a>]</li>
  <li><a href="https://arxiv.org/abs/2004.01800">Temporally Distributed Networks for Fast Video Semantic Segmentation</a> <kbd>CVPR 2020</kbd> [efficient video segmentation]</li>
  <li><a href="https://arxiv.org/abs/2003.12063">Memory Enhanced Global-Local Aggregation for Video Object Detection</a> <kbd>CVPR 2020</kbd> [efficient video object detection]</li>
  <li><a href="https://arxiv.org/abs/1804.06055">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and
Detection with Hierarchical Aggregation</a> <kbd>IJCAI 2018 oral</kbd> [video skeleton]</li>
  <li><a href="https://arxiv.org/abs/1912.00438">RST-MODNet: Real-time Spatio-temporal Moving Object Detection for Autonomous Driving</a> <kbd>NeurIPS 2019 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/1411.4389">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</a> <kbd>CVPR 2015 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1608.00859">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</a> <kbd>ECCV 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1711.08496">TRN: Temporal Relational Reasoning in Videos</a> <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.04730">X3D: Expanding Architectures for Efficient Video Recognition</a> <kbd>CVPR 2020 oral</kbd> [FAIR]</li>
  <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Temporal-Context_Enhanced_Detection_of_Heavily_Occluded_Pedestrians_CVPR_2020_paper.html">Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians</a> <kbd>CVPR 2020 oral</kbd> [pedestrian, video]</li>
  <li><a href="https://arxiv.org/abs/1703.10025">Flow-guided feature aggregation for video object detection</a> <kbd>ICCV 2017</kbd> [video, object detection]</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pavllo_3D_Human_Pose_Estimation_in_Video_With_Temporal_Convolutions_and_CVPR_2019_paper.pdf">3D human pose estimation in video with temporal convolutions and
semi-supervised training</a> <kbd>CVPR 2019</kbd> [mono3D pose estimation from video]</li>
  <li><a href="https://arxiv.org/abs/2003.14030">OmegaNet: Distilled Semantics for Comprehensive Scene Understanding from Videos</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1702.06355">Object Detection in Videos with Tubelet Proposal Networks</a> <kbd>CVPR 2017</kbd> [video object detection]</li>
  <li><a href="https://arxiv.org/abs/1604.02532">T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos</a> [video object detection]</li>
  <li><a href="https://arxiv.org/abs/1703.10025">Flow-Guided Feature Aggregation for Video Object Detection</a> <kbd>ICCV 2017</kbd> [Jifeng Dai]</li>
</ul>

<h3 id="pruning-and-compression">Pruning and Compression</h3>
<ul>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w33/Zhang_Efficient_Deep_Learning_CVPR_2018_paper.pdf">Efficient Deep Learning Inference based on Model Compression</a> (Model Compression)</li>
  <li><a href="https://intellabs.github.io/distiller/algo_pruning.html">Neural Network Distiller</a> [Intel]</li>
</ul>

<h3 id="architecture-improvements">Architecture Improvements</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1803.02579.pdf">Concurrent Spatial and Channel Squeeze &amp; Excitation in Fully Convolutional Networks</a></li>
  <li><a href="https://arxiv.org/pdf/1807.06521.pdf">CBAM: Convolutional Block Attention Module</a></li>
</ul>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<ul>
  <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a> <kbd>NIPS 2013 </kbd></li>
  <li><a href="http://comaniciu.net/Papers/MultiscaleDeepReinforcementLearning_PAMI18.pdf">Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scan</a></li>
  <li><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14751/14296">An Artificial Agent for Robust Image Registration</a></li>
</ul>

<h3 id="3d-perception">3D Perception</h3>
<ul>
  <li><a href="https://www.ri.cmu.edu/pub_files/2015/3/maturana-root.pdf">3D-CNN：3D Convolutional Neural Networks for Landing Zone Detection from LiDAR</a></li>
  <li><a href="https://arxiv.org/pdf/1608.04236.pdf">Generative and Discriminative Voxel Modeling with Convolutional Neural Networks</a></li>
  <li><a href="https://arxiv.org/pdf/1604.03351.pdf">Orientation-boosted Voxel Nets for 3D Object Recognition</a> (ORION) &lt;BMVC 2017&gt;</li>
  <li><a href="https://arxiv.org/pdf/1604.01879.pdf">GIFT: A Real-time and Scalable 3D Shape Search Engine</a> <kbd>CVPR 2016</kbd></li>
  <li><a href="https://people.cs.umass.edu/~kalo/papers/shapepfcn/">3D Shape Segmentation with Projective Convolutional Networks</a> (ShapePFCN)<kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1706.04496.pdf">Learning Local Shape Descriptors from Part Correspondences With Multi-view Convolutional Networks</a></li>
  <li><a href="http://www.open3d.org/wordpress/wp-content/paper.pdf">Open3D: A Modern Library for 3D Data Processing</a></li>
  <li><a href="https://arxiv.org/pdf/1507.06821.pdf">Multimodal Deep Learning for Robust RGB-D Object Recognition</a> <kbd>IROS 2015</kbd></li>
  <li><a href="https://arxiv.org/pdf/1806.01411.pdf">FlowNet3D: Learning Scene Flow in 3D Point Clouds</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/pdf/1712.06760.pdf">Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling</a> <kbd>CVPR 2018</kbd> (Neighbors Do Help: Deeply Exploiting Local Structures of Point Clouds)</li>
  <li><a href="https://arxiv.org/pdf/1801.06761.pdf">PU-Net: Point Cloud Upsampling Network</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1802.04402.pdf">Recurrent Slice Networks for 3D Segmentation of Point Clouds</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1802.08275.pdf">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a> <kbd>NIPS 2016</kbd></li>
  <li><a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-Supervised Classification with Graph Convolutional Networks</a> <kbd>ICLR 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1704.06803.pdf">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</a> <kbd>NIPS 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1710.10903.pdf">Graph Attention Networks</a> <kbd>ICLR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1711.00238.pdf">3D-SSD: Learning Hierarchical Features from RGB-D Images for Amodal 3D Object Detection</a> (3D SSD)</li>
  <li><a href="https://arxiv.org/pdf/1704.01222.pdf">Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models</a> <kbd>ICCV 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1612.00101.pdf">Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/pdf/1812.05276.pdf">IPOD: Intensive Point-based Object Detector for Point Cloud</a></li>
  <li><a href="https://cis.temple.edu/~latecki/Papers/DengCVPR2017.pdf">Amodal Detection of 3D Objects: Inferring 3D Bounding Boxes from 2D Ones in RGB-Depth Images</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lahoud_2D-Driven_3D_Object_ICCV_2017_paper.pdf">2D-Driven 3D Object Detection in RGB-D Images</a></li>
  <li><a href="https://arxiv.org/pdf/1711.00238.pdf">3D-SSD: Learning Hierarchical Features from RGB-D Images for Amodal 3D Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2006.04356">Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection</a> [classify occluded object]</li>
</ul>

<h3 id="stereo-and-flow">Stereo and Flow</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1803.08669.pdf">PSMNet: Pyramid Stereo Matching Network</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1902.09738.pdf">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://people.csail.mit.edu/weichium/papers/cvpr19-dsisf/paper.pdf">Deep Rigid Instance Scene Flow</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Upgrading_Optical_Flow_to_3D_Scene_Flow_Through_Optical_Expansion_CVPR_2020_paper.pdf">Upgrading Optical Flow to 3D Scene Flow through Optical Expansion</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.02096">Learning Multi-Object Tracking and Segmentation from Automatic Annotations</a> <kbd>CVPR 2020</kbd> [automatic MOTS annotation]</li>
</ul>

<h3 id="traffic-light-and-traffic-sign">Traffic light and traffic sign</h3>
<ul>
  <li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf">Traffic-Sign Detection and Classification in the Wild</a> <kbd>CVPR 2016</kbd> [Tsinghua, Tencent, traffic signs]</li>
  <li><a href="https://arxiv.org/abs/1806.07987">A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint
Traffic Sign and Light Detection</a> <kbd>IEEE CRV 2018</kbd> [U torronto]</li>
  <li><a href="https://arxiv.org/abs/1805.02523">Detecting Traffic Lights by Single Shot Detection</a> <kbd>ITSC 2018</kbd></li>
  <li><a href="https://sci-hub.st/10.1109/IVS.2016.7535408">DeepTLR: A single Deep Convolutional Network for Detection and Classification of Traffic Lights</a> <kbd>IV 2016</kbd></li>
  <li><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w9/papers/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.pdf">Evaluating State-of-the-art Object Detector on Challenging Traffic Light Data</a> <kbd>CVPR 2017 workshop</kbd></li>
  <li><a href="https://www.researchgate.net/profile/Vijay_John3/publication/265014373_Traffic_Light_Recognition_in_Varying_Illumination_using_Deep_Learning_and_Saliency_Map/links/56aac00408ae8f3865666102.pdf">Traffic light recognition in varying illumination using deep learning and saliency map</a> <kbd>ITSC 2014</kbd> [traffic light]</li>
  <li><a href="https://sci-hub.st/https://www.sciencedirect.com/science/article/abs/pii/S0921889018301234">Traffic light recognition using high-definition map features</a> <kbd>RAS 2019</kbd></li>
  <li><a href="http://cvrr.ucsd.edu/publications/2016/trafficSignalsITSTrans2016.pdf">Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives</a> <kbd>TITS 2015</kbd></li>
</ul>

<h3 id="datasets-and-surveys">Datasets and Surveys</h3>
<ul>
  <li><a href="https://ieeexplore.ieee.org/document/8460737">The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets</a> <kbd>ICRA 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1909.01300">The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset</a></li>
  <li><a href="http://cvrr.ucsd.edu/publications/2016/trafficSignalsITSTrans2016.pdf">Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives</a> (traffic light survey, UCSD LISA)</li>
  <li><a href="/Learning-Deep-Learning/paper_notes/graph_spectrum.html">Review of Graph Spectrum Theory</a> (WIP)</li>
  <li><a href="https://www.youtube.com/watch?v=8CenT_4HWyY">3D Deep Learning Tutorial at CVPR 2017</a> [<a href="/Learning-Deep-Learning/paper_notes/3ddl_cvpr2017.html">Notes</a>] - (WIP)</li>
  <li><a href="https://arxiv.org/pdf/1905.01392.pdf">A Survey on Neural Architecture Search</a></li>
  <li><a href="https://jacobgil.github.io/deeplearning/pruning-deep-learning">Network pruning tutorial</a> (blog)</li>
  <li><a href="https://xiaolonw.github.io/graphnn/">GNN tutorial at CVPR 2019</a></li>
  <li><a href="https://arxiv.org/abs/2104.10133">Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset</a> [Waymo, prediction dataset]</li>
  <li><a href="https://arxiv.org/abs/2003.04852">PANDA: A Gigapixel-level Human-centric Video Dataset</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yogamani_WoodScape_A_Multi-Task_Multi-Camera_Fisheye_Dataset_for_Autonomous_Driving_ICCV_2019_paper.pdf">WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving</a> <kbd>ICCV 2019</kbd> [Valeo]</li>
</ul>

<h3 id="unsupervised-depth-estimation">Unsupervised depth estimation</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1808.00769.pdf">Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation</a> <kbd>3DV 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1406.2283.pdf">Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</a> <kbd>NIPS 2014</kbd> (Eigen et al)</li>
  <li><a href="https://arxiv.org/abs/1712.00175">Learning Depth from Monocular Videos using Direct Methods</a> <kbd>CVPR 2018</kbd> (monocular depth estimation)</li>
  <li><a href="https://arxiv.org/abs/1907.12209">Virtual-Normal: Enforcing geometric constraints of virtual normal for depth prediction</a> [<a href="/Learning-Deep-Learning/paper_notes/virtual_normal.html">Notes</a>] <kbd>ICCV 2019</kbd> (better generation of PL)</li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Spatial_Correspondence_With_Generative_Adversarial_Network_Learning_Depth_From_Monocular_ICCV_2019_paper.pdf">Spatial Correspondence with Generative Adversarial Network: Learning Depth from Monocular Videos</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.pdf">Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.03380">Visualization of Convolutional Neural Networks for Monocular Depth Estimation</a> <kbd>ICCV 2019</kbd></li>
</ul>

<h3 id="indoor-depth">Indoor Depth</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1905.08598">Fast and Accurate Recovery of Occluding Contours in Monocular Depth Estimation</a> <kbd>ICCV 2019 workshop</kbd> [indoor]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2890_ECCV_2020_paper.php">Multi-Loss Rebalancing Algorithm for Monocular Depth Estimation</a> <kbd>ECCV 2020</kbd> [indoor depth]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3668_ECCV_2020_paper.php">Disambiguating Monocular Depth Estimation with a Single Transient</a> <kbd>ECCV 2020</kbd> [additional laser sensor, indoor depth]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5491_ECCV_2020_paper.php">Guiding Monocular Depth Estimation Using Depth-Attention Volume</a> <kbd>ECCV 2020</kbd> [indoor depth]</li>
  <li><a href="https://arxiv.org/abs/2007.11256">Improving Monocular Depth Estimation by Leveraging Structural Awareness and Complementary Datasets</a> <kbd>ECCV 2020</kbd> [indoor depth]</li>
  <li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3365_ECCV_2020_paper.php">CLIFFNet for Monocular Depth Estimation with Hierarchical Embedding Loss</a> <kbd>ECCV 2020</kbd> [indoor depth]</li>
</ul>

<h3 id="lidar">lidar</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1807.00652.pdf">PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation</a> (pointnet alternative, backbone)</li>
  <li><a href="https://arxiv.org/pdf/1608.07916.pdf">Vehicle Detection from 3D Lidar Using Fully Convolutional Network</a> (VeloFCN) <kbd>RSS 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.08889">KPConv: Flexible and Deformable Convolution for Point Clouds</a> (from the authors of PointNet)</li>
  <li><a href="https://arxiv.org/pdf/1801.07791.pdf">PointCNN: Convolution On X-Transformed Points</a> <kbd>NIPS 2018</kbd></li>
  <li><a href="https://songshiyu01.github.io/pdf/L3Net_W.Lu_Y.Zhou_S.Song_CVPR2019.pdf">L3-Net: Towards Learning based LiDAR Localization for Autonomous Driving</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1811.03818">RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement</a> (sensor fusion, 3D mono proposal, refined in point cloud)</li>
  <li><a href="https://arxiv.org/pdf/1805.04949.pdf">DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1903.01864">Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection</a> <kbd>IROS 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1910.08287">PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing</a></li>
  <li><a href="https://arxiv.org/abs/1902.04997">Gated2Depth: Real-time Dense Lidar from Gated Images</a> <kbd>ICCV 2019 oral</kbd></li>
  <li><a href="http://www.cs.cmu.edu/~youngwoo/doc/icra-14-sensor-fusion.pdf">A Multi-Sensor Fusion System for Moving Object Detection and Tracking in Urban Driving Environments</a> <kbd>ICRA 2014</kbd></li>
  <li><a href="https://arxiv.org/abs/1711.10871">PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation</a> <kbd>CVPR 2018</kbd> [sensor fusion, Zoox]</li>
  <li><a href="https://arxiv.org/abs/1904.09664">Deep Hough Voting for 3D Object Detection in Point Clouds</a> <kbd>ICCV 2019</kbd> [Charles Qi]</li>
  <li><a href="http://www.bmva.org/bmvc/2015/papers/paper109/paper109.pdf">StixelNet: A Deep Convolutional Network for Obstacle Detection and Road Segmentation</a></li>
  <li><a href="https://arxiv.org/abs/2003.14032">PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.03048">Depth Sensing Beyond LiDAR Range</a> <kbd>CVPR 2020</kbd> [wide baseline stereo with trifocal]</li>
  <li><a href="https://arxiv.org/abs/2006.04894">Probabilistic Semantic Mapping for Urban Autonomous Driving Applications</a> <kbd>IROS 2020</kbd> [lidar mapping]</li>
  <li><a href="https://arxiv.org/abs/1911.11236">RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</a> <kbd>CVPR 2020 oral</kbd> [lidar segmentation]</li>
  <li><a href="https://arxiv.org/abs/2003.14032">PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation</a> <kbd>CVPR 2020</kbd> [lidar segmentation]</li>
  <li><a href="https://arxiv.org/abs/2005.07178">OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression</a> <kbd>CVPR 2020 oral</kbd> [lidar compression]</li>
  <li><a href="https://arxiv.org/abs/2011.07590">MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models</a> <kbd>NeurIPS 2020 oral</kbd> [lidar compression]</li>
</ul>

<h3 id="egocentric-bbox-prediction">Egocentric bbox prediction</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1711.09026">Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty</a> <kbd>CVPR 2018</kbd> [on-board bbox prediction]</li>
  <li><a href="https://arxiv.org/abs/1903.00618">Unsupervised Traffic Accident Detection in First-Person Videos</a> <kbd>IROS 2019</kbd> (Honda)</li>
  <li><a href="https://arxiv.org/abs/1909.08150">NEMO: Future Object Localization Using Noisy Ego Priors</a> (Honda)</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.pdf">Robust Aleatoric Modeling for Future Vehicle Localization</a> (perspective)</li>
  <li><a href="https://arxiv.org/abs/1909.11944">Multiple Object Forecasting: Predicting Future Object Locations in Diverse
Environments</a> <kbd>WACV 2020</kbd> (perspective bbox, pedestrian)</li>
  <li><a href="https://arxiv.org/abs/1911.10535">Using panoramic videos for multi-person localization and tracking in a 3D panoramic coordinate</a></li>
</ul>

<h3 id="lane-detection">Lane Detection</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1902.00293">End-to-end Lane Detection through Differentiable Least-Squares Fitting</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://doi.org/10.1109/TITS.2019.2890870">Line-CNN: End-to-End Traffic Line Detection With Line Proposal Unit</a> <kbd>TITS 2019</kbd> [object-like proposals]</li>
  <li><a href="https://arxiv.org/abs/2003.08550">Detecting Lane and Road Markings at A Distance with Perspective Transformer Layers</a> [3D LLD]</li>
  <li><a href="https://arxiv.org/abs/2004.11757">Ultra Fast Structure-aware Deep Lane Detection</a> <kbd>ECCV 2020</kbd> [lane detection]</li>
  <li><a href="https://ieeexplore.ieee.org/document/8500551/">A Novel Approach for Detecting Road Based on Two-Stream Fusion Fully Convolutional Network</a> (convert camera to BEV)</li>
  <li><a href="https://arxiv.org/abs/1905.04354">FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network</a></li>
</ul>

<h3 id="tracking">Tracking</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2003.13870">RetinaTrack: Online Single Stage Joint Detection and Tracking</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1704.05519v2">Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art</a> (latest update in Dec 2019)</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/BMTT/Henschel_Simultaneous_Identification_and_Tracking_of_Multiple_People_Using_Video_and_CVPRW_2019_paper.pdf">Simultaneous Identification and Tracking of Multiple People Using Video and IMUs</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.pdf">Detect-and-Track: Efficient Pose Estimation in Videos</a></li>
  <li><a href="https://arxiv.org/abs/1902.01466">TrackNet: Simultaneous Object Detection and Tracking and Its Application in Traffic Video Analysis</a></li>
  <li><a href="https://arxiv.org/abs/1812.02707">Video Action Transformer Network</a> <kbd>CVPR 2019 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1611.08563">Online Real-time Multiple Spatiotemporal Action Localisation and Prediction</a> <kbd>ICCV 2017</kbd></li>
  <li><a href="https://zhuanlan.zhihu.com/p/65177442">多目标跟踪 近年论文及开源代码汇总</a></li>
  <li><a href="https://arxiv.org/abs/2006.07327">GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning</a> <kbd>CVPR 2020 oral</kbd> [3DMOT, CMU, Kris Kitani]</li>
  <li><a href="https://arxiv.org/abs/2007.14557">Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking</a> <kbd>ECCV 2020 spotlight</kbd> [MOT, Tencent]</li>
  <li><a href="https://arxiv.org/abs/1909.12605">Towards Real-Time Multi-Object Tracking</a> <kbd>ECCV 2020</kbd> [MOT]</li>
  <li><a href="https://arxiv.org/abs/2001.05673">Probabilistic 3D Multi-Object Tracking for Autonomous Driving</a> [TRI]</li>
</ul>

<h3 id="keypoints-pose-and-face">keypoints: pose and face</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1904.09658">Probabilistic Face Embeddings</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.11339">Data Uncertainty Learning in Face Recognition</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.html">Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos</a> <kbd>CVPR 2020 oral</kbd> [VGG, self-supervised, interpretable, discriminator]</li>
</ul>

<h3 id="general-dl">General DL</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1804.07612">Revisiting Small Batch Training for Deep Neural Networks</a></li>
  <li><a href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3504">ICML2019 workshop: Adaptive and Multitask Learning: Algorithms &amp; Systems</a> <kbd>ICML 2019</kbd></li>
  <li><a href="https://marcpickett.com/cl2018/CL-2018_paper_82.pdf">Adaptive Scheduling for Multi-Task Learning</a> <kbd>NIPS 2018</kbd> (NMT)</li>
  <li><a href="https://arxiv.org/abs/1709.01889">Polar Transformer Networks</a> <kbd>ICLR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.01685">Measuring Calibration in Deep Learning</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1908.00598">Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation</a> <kbd>ICCV 2019</kbd> (epistemic uncertainty)</li>
  <li><a href="https://arxiv.org/abs/1904.11486">Making Convolutional Networks Shift-Invariant Again</a> <kbd>ICML</kbd></li>
  <li><a href="https://arxiv.org/abs/1906.12340">Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty</a> <kbd>NeurIPS 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a> <kbd>ICLR 2017</kbd> [ICLR best paper]</li>
  <li><a href="https://arxiv.org/abs/1610.02136">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</a> <kbd>ICLR 2017</kbd> (NLL score as anomaly score)</li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination</a> <kbd>CVPR 2018 spotlight</kbd> (Stella Yu)</li>
  <li><a href="https://arxiv.org/abs/1707.04926">Theoretical insights into the optimization landscape of over-parameterized shallow neural networks</a> <kbd>TIP 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1712.06559">The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning</a> <kbd>ICML 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.13678">Designing Network Design Spaces</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.04297">Moco2: Improved Baselines with Momentum Contrastive Learning</a></li>
  <li><a href="https://arxiv.org/abs/1905.11604">SGD on Neural Networks Learns Functions of Increasing Complexity</a> <kbd>NIPS 2019</kbd> (SGD learns a linear classifier first)</li>
  <li><a href="https://arxiv.org/abs/1907.13075">Pay attention to the activations: a modular attention mechanism for fine-grained image recognition</a></li>
  <li><a href="https://arxiv.org/abs/1805.03225">A Mixed Classification-Regression Framework for 3D Pose Estimation from 2D Images</a> <kbd>BMVC 2018</kbd> (multi-bin, what’s new?)</li>
  <li><a href="https://arxiv.org/abs/1712.02616">In-Place Activated BatchNorm for Memory-Optimized Training of DNNs</a> <kbd>CVPR 2018</kbd> (optimized BatchNorm + ReLU)</li>
  <li><a href="http://ecmlpkdd2017.ijs.si/papers/paperID11.pdf">FCNN: Fourier Convolutional Neural Networks</a> (FFT as CNN)</li>
  <li><a href="https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf">Visualizing the Loss Landscape of Neural Nets</a> <kbd>NIPS 2018</kbd></li>
  <li><a href="https://arxiv.org/pdf/1610.02357.pdf">Xception: Deep Learning with Depthwise Separable Convolutions</a> (Xception)</li>
  <li><a href="https://arxiv.org/abs/1705.07115">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</a> (uncertainty)</li>
  <li><a href="https://arxiv.org/abs/1812.03823">Learning to Drive from Simulation without Real World Labels</a> <kbd>ICRA 2019</kbd> (domain adaptation, sim2real)</li>
  <li><a href="https://arxiv.org/abs/1911.09737">Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks</a> <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.09739">Switchable Whitening for Deep Representation Learning</a> <kbd>ICCV 2019</kbd> [domain adaptation]</li>
  <li><a href="https://arxiv.org/abs/2006.09512">Visual Chirality</a> <kbd>CVPR 2020 oral</kbd> [best paper nominee]</li>
  <li><a href="https://arxiv.org/abs/2002.11297">Generalized ODIN: Detecting Out-of-Distribution Image Without Learning From Out-of-Distribution Data</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/1911.04252">Self-training with Noisy Student improves ImageNet classification</a> <kbd>CVPR 2020</kbd> [distillation]</li>
  <li><a href="https://arxiv.org/abs/2005.12551">Keep it Simple: Image Statistics Matching for Domain Adaptation</a> <kbd>CVPRW 2020</kbd> [Domain adaptation for 2D mod bbox]</li>
  <li><a href="https://arxiv.org/abs/2005.04551">Epipolar Transformers</a> <kbd>CVPR 2020</kbd> [Yihui He]</li>
  <li><a href="https://arxiv.org/abs/2003.03396">Scalable Uncertainty for Computer Vision With Functional Variational Inference</a> <kbd>CVPR 2020</kbd> [epistemic uncertainty with one fwd pass]</li>
</ul>

<h3 id="mono3d">Mono3D</h3>
<ul>
  <li><a href="https://papers.nips.cc/paper/5644-3d-object-proposals-for-accurate-object-class-detection">3DOP: 3D Object Proposals for Accurate Object Class Detection</a> <kbd>NIPS 2015</kbd></li>
  <li><a href="https://arxiv.org/abs/1904.10097">DirectShape: Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation</a></li>
  <li><a href="https://arxiv.org/abs/1808.06253">Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular Depth Estimation to 360° Panoramic Imagery</a> <kbd>ECCV 2018</kbd> (Monocular 3D object detection and depth estimation)</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Towards_Scene_Understanding_Unsupervised_Monocular_Depth_Estimation_With_Semantic-Aware_Representation_CVPR_2019_paper.pdf">Towards Scene Understanding: Unsupervised Monocular Depth Estimation with Semantic-aware Representation</a> <kbd>CVPR 2019</kbd> [unified conditional decoder]</li>
  <li><a href="https://arxiv.org/abs/1901.10034">DDP: Dense Depth Posterior from Single Image and Sparse Range</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1708.01566">Augmented Reality Meets Computer Vision : Efficient Data Generation for Urban Driving Scenes</a> <kbd>IJCV 2018</kbd> (data augmentation with AR, Toyota)</li>
  <li><a href="https://arxiv.org/abs/2005.07424">Exploring the Capabilities and Limits of 3D Monocular Object Detection – A Study on Simulation and Real World Data</a> <kbd>IITS</kbd></li>
  <li><a href="https://arxiv.org/abs/1411.5935">Towards Scene Understanding with Detailed 3D Object Representations</a> <kbd>IJCV 2014</kbd> (keypoint, 3D bbox annotation)</li>
  <li><a href="https://arxiv.org/abs/1611.10010">Deep Cuboid Detection: Beyond 2D Bounding Boxes</a> (Magic Leap)</li>
  <li><a href="https://arxiv.org/abs/1411.6067">Viewpoints and Keypoints</a> (Malik)</li>
  <li><a href="https://arxiv.org/abs/1503.06465">Lifting Object Detection Datasets into 3D</a> (PASCAL)</li>
  <li><a href="https://arxiv.org/abs/1503.05038">3D Object Class Detection in the Wild</a> (keypoint based)</li>
  <li><a href="https://arxiv.org/abs/1609.05590">Fast Single Shot Detection and Pose Estimation</a> <kbd>3DV 2016</kbd> (SSD + pose, Wei Liu)</li>
  <li><a href="https://arxiv.org/abs/2001.10773">Virtual KITTI 2</a></li>
  <li><a href="https://arxiv.org/abs/1612.02699">Deep Supervision with Shape Concepts for Occlusion-Aware 3D Object Parsing</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1505.05641">Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views</a> <kbd>ICCV 2015 Oral</kbd></li>
  <li><a href="https://arxiv.org/abs/1711.08848">Real-Time Seamless Single Shot 6D Object Pose Prediction</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1806.01677">Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching</a> <kbd>NIPS 2018</kbd> [disparity estimation]</li>
  <li><a href="https://arxiv.org/abs/1807.00275">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a> <kbd>ICRA 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1810.02695">Learning Depth with Convolutional Spatial Propagation Network</a> (Baidu, depth from SPN) <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.00497">Just Go with the Flow: Self-Supervised Scene Flow Estimation</a> <kbd>CVPR 2020 oral</kbd> [Scene flow, Lidar]</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf">Online Depth Learning against Forgetting in Monocular Videos</a> <kbd>CVPR 2020</kbd> [monodepth]</li>
  <li><a href="https://arxiv.org/abs/2005.06136">Self-Supervised Deep Visual Odometry with Online Adaptation</a> <kbd>CVPR 2020 oral</kbd> [DF-VO, TrianFlow, meta-learning]</li>
  <li><a href="https://arxiv.org/abs/2003.13951">Self-supervised Monocular Trained Depth Estimation using Self-attention and Discrete Disparity Volume</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf">Online Depth Learning against Forgetting in Monocular Videos</a> <kbd>CVPR 2020</kbd> [monodepth, online learning]</li>
  <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_SDC-Depth_Semantic_Divide-and-Conquer_Network_for_Monocular_Depth_Estimation_CVPR_2020_paper.pdf">SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth Estimation</a>  <kbd>CVPR 2020</kbd> [monodepth, semantic]</li>
  <li><a href="http://www.contrib.andrew.cmu.edu/~gengshay/wordpress/wp-content/uploads/2018/11/iros_monodepth_uncertainty.pdf">Inferring Distributions Over Depth from a Single Image</a> <kbd>TRO</kbd> [Depth confidence, stitching them together]</li>
  <li><a href="https://arxiv.org/abs/2004.01294">Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.00171">The Edge of Depth: Explicit Constraints between Segmentation and Depth</a> <kbd>CVPR 2020</kbd> [Xiaoming Liu, multimodal, depth bleeding]</li>
</ul>

<h3 id="radar-perception">Radar Perception</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2103.16214">MV-RSS: Multi-View Radar Semantic Segmentation</a> <kbd>ICCV 2021</kbd></li>
  <li><a href="http://sci-hub.tw/10.1109/APMC.2017.8251453">Classification of Objects in Polarimetric Radar Images Using CNNs at 77 GHz</a> (Radar, polar)</li>
  <li><a href="https://ml4ad.github.io/files/papers/CNNs%20for%20Interference%20Mitigation%20and%20Denoising%20in%20Automotive%20Radar%20Using%20Real-World%20Data.pdf">CNNs for Interference Mitigation and Denoising in Automotive Radar Using Real-World Data</a> <kbd>NeurIPS 2019</kbd> (radar)</li>
  <li><a href="https://arxiv.org/abs/1904.00415">Road Scene Understanding by Occupancy Grid Learning from Sparse Radar Clusters using Semantic Segmentation</a> <kbd>ICCV 2019</kbd> (radar)</li>
  <li><a href="https://arxiv.org/abs/2007.14366">RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects</a> <kbd>ECCV 2020</kbd> [Uber ATG]</li>
  <li><a href="https://arxiv.org/abs/2010.00058">Depth Estimation from Monocular Images and Sparse Radar Data</a> <kbd>IROS 2020</kbd> [Camera + Radar for monodepth, nuscenes]</li>
  <li><a href="https://arxiv.org/abs/2009.08428">RPR: Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles</a> <kbd>IROS 2020</kbd> [radar proposal refinement]</li>
  <li><a href="https://arxiv.org/abs/2012.12809">Warping of Radar Data into Camera Image for Cross-Modal Supervision in Automotive Applications</a></li>
</ul>

<h3 id="slam">SLAM</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1505.07427">PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</a> [<a href="/Learning-Deep-Learning/paper_notes/posenet.html">Notes</a>] <kbd>ICCV 2015</kbd></li>
  <li><a href="https://arxiv.org/abs/1509.05909">PoseNet2: Modelling Uncertainty in Deep Learning for Camera Relocalization</a> <kbd>ICRA 2016</kbd></li>
  <li><a href="https://arxiv.org/abs/1704.00390">PoseNet3: Geometric Loss Functions for Camera Pose Regression with Deep Learning</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1703.05593">EssNet: Convolutional neural network architecture for geometric matching</a> <kbd>CVPR 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/1810.10510">NC-EssNet: Neighbourhood Consensus Networks</a> <kbd>NeurIPS 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1912.00623">Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task</a> <kbd>CVPR 2020 oral</kbd> [Eric Brachmann, ngransac]</li>
  <li><a href="https://arxiv.org/pdf/1802.05522.pdf">Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</a> <kbd>CVPR 2018</kbd></li>
  <li><a href="https://siegedog.com/dynslam/">DynSLAM: Robust Dense Mapping for Large-Scale Dynamic Environments</a> [dynamic SLAM, Andreas Geiger] <kbd>ICRA 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1902.11046">GCNv2: Efficient Correspondence Prediction for Real-Time SLAM</a> <kbd>LRA 2019</kbd> [Superpoint + orb slam]</li>
  <li><a href="Real-time Scalable Dense Surfel Mapping">Real-time Scalable Dense Surfel Mapping</a> <kbd>ICRA 2019</kbd> [dense reconstruction, monodepth]</li>
  <li><a href="https://arxiv.org/abs/2002.08584">Dynamic SLAM: The Need For Speed</a></li>
  <li><a href="https://arxiv.org/abs/1902.07995">GSLAM: A General SLAM Framework and Benchmark</a> <kbd>ICCV 2019</kbd></li>
</ul>

<h3 id="radar-perception-1">Radar Perception</h3>
<ul>
  <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Scheiner_Seeing_Around_Street_Corners_Non-Line-of-Sight_Detection_and_Tracking_In-the-Wild_Using_CVPR_2020_paper.pdf">Seeing Around Street Corners: Non-Line-of-Sight Detection and Tracking In-the-Wild Using Doppler Radar</a> <kbd>CVPR 2020</kbd> [Daimler]</li>
  <li><a href="https://arxiv.org/abs/2008.13642">Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous Vehicles</a> <kbd>ICIP 2020</kbd></li>
  <li><a href="https://www.mdpi.com/1424-8220/20/4/956">Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor</a> <kbd>sensors 2020</kbd> [radar, camera, early fusion]</li>
</ul>

<h3 id="reviews-and-surveys">Reviews and Surveys</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2006.12567">A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence</a></li>
  <li><a href="https://arxiv.org/abs/2003.06620">Monocular Depth Estimation Based On Deep Learning: An Overview</a></li>
</ul>

<h3 id="beyond-perception-in-autonomous-driving">Beyond Perception in Autonomous Driving</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1906.11129">Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining</a> <kbd>CVPR 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/1805.11730">Learn to Combine Modalities in Multimodal Deep Learning</a> (sensor fusion, general DL)</li>
  <li><a href="https://arxiv.org/abs/1906.09788">Safe Trajectory Generation For Complex Urban Environments Using Spatio-temporal Semantic Corridor</a> <kbd>LRA 2019</kbd> [Motion planning]</li>
  <li><a href="https://arxiv.org/abs/1804.09364">DAgger: Driving Policy Transfer via Modularity and Abstraction</a> <kbd>CoRL 2018</kbd> [DAgger, Immitation Learning]</li>
  <li><a href="https://arxiv.org/abs/2003.02746">Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching</a> <kbd>ICRA 2020</kbd> [Motion planning]</li>
  <li><a href="https://arxiv.org/abs/1807.08048">Baidu Apollo EM Motion Planner</a></li>
  <li><a href="https://arxiv.org/pdf/1812.11445.pdf">Calibration of Heterogeneous Sensor Systems</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/40967227">Intro：Sensor Fusion for Adas 无人驾驶中的数据融合 (from 知乎)</a> (Up to CVPR 2018)</li>
  <li><a href="https://arxiv.org/pdf/1904.05673.pdf">YUVMultiNet: Real-time YUV multi-task CNN for autonomous driving</a> <kbd>CVPR 2019</kbd> (Real Time, Low Power)</li>
  <li><a href="http://sci-hub.tw/10.1109/VLSI-DAT.2018.8373245">Deep Fusion of Heterogeneous Sensor Modalities for the Advancements of ADAS to Autonomous Vehicles</a></li>
  <li><a href="https://arxiv.org/abs/1908.11757">Temporal Coherence for Active Learning in Videos</a> <kbd>ICCVW 2019</kbd> [active learning, temporal coherence]</li>
  <li><a href="https://arxiv.org/abs/2011.06372">R-TOD: Real-Time Object Detector with Minimized End-to-End Delay for Autonomous Driving</a> <kbd>RTSS 2020</kbd> [perception system design]</li>
</ul>

<h3 id="prediction-and-planning">Prediction and Planning</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2007.13732">Learning Lane Graph Representations for Motion Forecasting</a> <kbd>ECCV 2020</kbd> [Uber ATG]</li>
  <li><a href="https://arxiv.org/abs/2008.06041">DSDNet: Deep Structured self-Driving Network</a> <kbd>ECCV 2020</kbd> [Uber ATG]</li>
</ul>

<h3 id="annotation-and-tooling">Annotation and Tooling</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1908.11757">Temporal Coherence for Active Learning in Videos</a> <kbd>ICCV 2019 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/1807.06072">Leveraging Pre-Trained 3D Object Detection Models For Fast Ground Truth Generation</a> <kbd>ITSC 2018</kbd> [UToronto, autolabeling]</li>
  <li><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Porzi_Learning_Multi-Object_Tracking_and_Segmentation_From_Automatic_Annotations_CVPR_2020_paper.html">Learning Multi-Object Tracking and Segmentation From Automatic Annotations</a> <kbd>CVPR 2020</kbd> [Autolabeling]</li>
  <li><a href="https://arxiv.org/abs/1907.10043">Canonical Surface Mapping via Geometric Cycle Consistency</a> <kbd>ICCV 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2008.08115">TIDE: A General Toolbox for Identifying Object Detection Errors</a> <kbd>ECCV 2018</kbd> [tools]</li>
</ul>

<h3 id="low-level-dl">Low level DL</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2112.03325">Self-Supervised Camera Self-Calibration from Video</a> [TRI, intrinsic calibration, fisheye/pinhole]</li>
</ul>

<h2 id="non-dl">Non-DL</h2>
<ul>
  <li><a href="https://github.com/wzhe06/Ad-papers">Ad推荐系统方向文章汇总</a></li>
  <li><a href="https://arxiv.org/abs/1802.03426">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a> [<a href="/Learning-Deep-Learning/paper_notes/umap.html">Notes</a>] (dimension reduction, better than t-SNE)</li>
</ul>

<h2 id="technical-debt">Technical Debt</h2>
<ul>
  <li><a href="/Learning-Deep-Learning/paper_notes/classical_keypoints.html">Review Notes of Classical Key Points and Descriptors</a></li>
  <li>CRF</li>
  <li><a href="https://link.springer.com/content/pdf/10.1007%2Fs40903-015-0032-7.pdf">Visual SLAM and Visual Odometry</a></li>
  <li>ORB SLAM</li>
  <li>Bundle Adjustment</li>
  <li>3D vision</li>
  <li><a href="https://zhuanlan.zhihu.com/p/34995102">SLAM/VIO学习总结</a></li>
  <li><a href="https://refactoring.guru/design-patterns/python">Design Patterns</a></li>
</ul>

<h2 id="cvpr-2021-and-iccv-2021-the-pile-to-be-read">CVPR 2021 and ICCV 2021 (the pile to be read)</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2103.05687">Capturing Omni-Range Context for Omnidirectional Segmentation</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2011.09094">UP-DETR: Unsupervised Pre-training for Object Detection with Transformers</a> <kbd>CVPR 2021</kbd> [transformers]</li>
  <li><a href="https://arxiv.org/abs/2011.09670">DCL: Dense Label Encoding for Boundary Discontinuity Free Rotation Detection</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2102.12472">4D Panoptic LiDAR Segmentation</a> <kbd>CVPR 2021</kbd> [TUM]</li>
  <li><a href="https://arxiv.org/abs/2011.14679">CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild</a> <kbd>CVPR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.06877">Fast and Accurate Model Scaling</a> <kbd>CVPR 2021</kbd> [FAIR]</li>
  <li><a href="https://arxiv.org/abs/2011.10033">Cylinder3D: Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation</a> <kbd>CVPR 2021</kbd> [lidar semantic segmentation]</li>
  <li><a href="https://arxiv.org/abs/2103.15297">LiDAR R-CNN: An Efficient and Universal 3D Object Detector</a> <kbd>CVPR 2021</kbd> [TuSimple, Lidar]</li>
  <li><a href="https://arxiv.org/abs/2011.13005">PREDATOR: Registration of 3D Point Clouds with Low Overlap</a> <kbd>CVPR 2021 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.13425">DBB: Diverse Branch Block: Building a Convolution as an Inception-like Unit</a> <kbd>CVPR 2021</kbd> [RepVGG, ACNet, Xiaohan Ding, Megvii]</li>
  <li><a href="https://arxiv.org/abs/2103.17202">GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection</a> <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2103.16470">DDMP: Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection</a> <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2103.13164">M3DSSD: Monocular 3D Single Stage Object Detector</a> <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2103.12605">MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation</a> <kbd>CVPR 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2104.00902">HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection</a> <kbd>CVPR 2021</kbd> [Lidar]</li>
  <li><a href="https://arxiv.org/abs/2101.06594">PLUME: Efficient 3D Object Detection from Stereo Images</a> [Yan Wang, Uber ATG]</li>
  <li><a href="https://arxiv.org/abs/2104.03106">V2F-Net: Explicit Decomposition of Occluded Pedestrian Detection</a> [crowded, pedestrian, megvii]</li>
  <li><a href="https://arxiv.org/abs/1802.00036">IP-basic: In Defense of Classical Image Processing: Fast Depth Completion on the CPU</a> <kbd>CRV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1908.01570">Revisiting Feature Alignment for One-stage Object Detection</a> [cls+reg]</li>
  <li><a href="https://arxiv.org/abs/2009.08650">Per-frame mAP Prediction for Continuous Performance Monitoring of Object Detection During Deployment</a> <kbd>WACV 2021</kbd> [SafetyNet]</li>
  <li><a href="https://arxiv.org/abs/2003.07540">TSD: Revisiting the Sibling Head in Object Detector</a> <kbd>CVPR 2020</kbd> [sensetime, cls+reg]</li>
  <li><a href="https://arxiv.org/abs/2003.07557">1st Place Solutions for OpenImage2019 – Object Detection and Instance Segmentation</a> [sensetime, cls+reg, 1st place OpenImage2019]</li>
  <li><a href="https://cvssp.org/Personal/OscarMendez/papers/pdf/SahaICRA2021.pdf">Enabling spatio-temporal aggregation in Birds-Eye-View Vehicle Estimation</a> <kbd>ICRA 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/1902.00293">End-to-end Lane Detection through Differentiable Least-Squares Fitting</a> <kbd>ICCV workshop 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2103.07579">Revisiting ResNets: Improved Training and Scaling Strategies</a></li>
  <li><a href="https://arxiv.org/abs/2012.12741">Multi-Modality Cut and Paste for 3D Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2102.12252">LD: Localization Distillation for Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/1912.02801">PolyTransform: Deep Polygon Transformer for Instance Segmentation</a> <kbd>CVPR 2020</kbd> [single stage instance segmentation]</li>
  <li><a href="https://arxiv.org/abs/2102.11585">ROAD: The ROad event Awareness Dataset for Autonomous Driving</a></li>
  <li><a href="https://arxiv.org/abs/2103.04056">LidarMTL: A Simple and Efficient Multi-task Network for 3D Object Detection and Road Understanding</a> [lidar MTL]</li>
  <li><a href="https://arxiv.org/abs/2102.06171">High-Performance Large-Scale Image Recognition Without Normalization</a> <kbd>ICLR 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2102.00690">Ground-aware Monocular 3D Object Detection for Autonomous Driving</a> <kbd>RA-L</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2012.05796">Demystifying Pseudo-LiDAR for Monocular 3D Object Detection</a> [mono3d]</li>
  <li><a href="https://arxiv.org/abs/2103.02093">Pseudo-labeling for Scalable 3D Object Detection</a> [Waymo]</li>
  <li><a href="https://arxiv.org/abs/2101.04307">LLA: Loss-aware Label Assignment for Dense Pedestrian Detection</a> [Megvii]</li>
  <li><a href="https://arxiv.org/abs/2005.04259">VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation</a> <kbd>CVPR 2020</kbd> [Waymo]</li>
  <li><a href="https://arxiv.org/abs/1911.10298">CoverNet: Multimodal Behavior Prediction using Trajectory Sets</a> <kbd>CVPR 2020</kbd> [prediction, nuScenes]</li>
  <li><a href="https://arxiv.org/abs/2011.14660">SplitNet: Divide and Co-training</a></li>
  <li><a href="https://arxiv.org/abs/1904.09730">VoVNet: An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</a> <kbd>CVPR 2019 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/1909.03205">Isometric Neural Networks: Non-discriminative data or weak model? On the relative importance of data and model resolution</a> <kbd>ICCV 2019 workshop</kbd> [spatial2channel]</li>
  <li><a href="https://arxiv.org/abs/2003.13630">TResNet</a> <kbd>WACV 2021</kbd> [spatial2channel]</li>
  <li><a href="https://arxiv.org/abs/1911.08287">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a> <kbd>AAAI 2020</kbd> [DIOU, NMS]</li>
  <li><a href="https://arxiv.org/abs/2003.13678">RegNet: Designing Network Design Spaces</a> <kbd>CVPR 2020</kbd> [FAIR]</li>
  <li><a href="https://arxiv.org/abs/1905.13214">On Network Design Spaces for Visual Recognition</a> [FAIR]</li>
  <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6308985/">Lane Endpoint Detection and Position Accuracy Evaluation for Sensor Fusion-Based Vehicle Localization on Highways</a> <kbd>Sensors 2018</kbd> [lane endpoints]</li>
  <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826538">Map-Matching-Based Cascade Landmark Detection and Vehicle Localization</a> <kbd>IEEE Access 2019</kbd> [lane endpoints]</li>
  <li><a href="https://arxiv.org/abs/1703.04309">GCNet: End-to-End Learning of Geometry and Context for Deep Stereo Regression</a> <kbd>ICCV 2017</kbd> [disparity estimation, Alex Kendall, cost volume]</li>
  <li><a href="https://arxiv.org/abs/2007.16072">Traffic Control Gesture Recognition for Autonomous Vehicles</a> <kbd>IROS 2020</kbd> [Daimler]</li>
  <li><a href="https://arxiv.org/abs/2007.15649">Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.15107">OrcVIO: Object residual constrained Visual-Inertial Odometry</a> [dynamic SLAM, very mathematical]</li>
  <li><a href="https://arxiv.org/abs/2007.08556">InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2003.03026">DA4AD: End-to-End Deep Attention-based Visual Localization for Autonomous Driving</a> <kbd>ECCV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2008.08311">Towards Lightweight Lane Detection by Optimizing Spatial Embedding</a> <kbd>ECCV 2020 workshop</kbd> [LLD]</li>
  <li><a href="https://arxiv.org/abs/2009.11859">Multi-Frame to Single-Frame: Knowledge Distillation for 3D Object Detection</a> <kbd>ECCV 2020 workshop</kbd> [lidar]</li>
  <li><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yi_Li_DeepIM_Deep_Iterative_ECCV_2018_paper.pdf">DeepIM: Deep iterative matching for 6d pose estimation</a> <kbd>ECCV 2018</kbd> [pose estimation]</li>
  <li><a href="https://arxiv.org/abs/2003.09763">Monocular Depth Prediction through Continuous 3D Loss</a> <kbd>IROS 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.13379">Multi-Task Learning for Dense Prediction Tasks: A Survey</a> [MTL, Luc Van Gool]</li>
  <li><a href="https://arxiv.org/abs/2001.02223">Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems</a> <kbd>ITSC 2020 oral</kbd> [MTL]</li>
  <li><a href="https://arxiv.org/abs/1902.03589">NeurAll: Towards a Unified Model for Visual Perception in Automated Driving</a> <kbd>ITSC 2019 oral</kbd> [MTL]</li>
  <li><a href="https://papers.nips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf">Deep Evidential Regression</a> <kbd>NeurIPS 2020</kbd> [one-pass aleatoric/epistemic uncertainty]</li>
  <li><a href="http://www.cs.toronto.edu/~yaojian/freeSpace.pdf">Estimating Drivable Collision-Free Space from Monocular Video</a> <kbd>WACV 2015</kbd> [Drivable space]</li>
  <li><a href="https://arxiv.org/abs/1904.03380">Visualization of Convolutional Neural Networks for Monocular Depth Estimation</a> <kbd>ICCV 2019</kbd> [monodepth]</li>
  <li><a href="https://arxiv.org/abs/2006.12057">Differentiable Rendering: A Survey</a> [differentiable rendering, TRI]</li>
  <li><a href="https://arxiv.org/abs/2010.02893">SAFENet: Self-Supervised Monocular Depth Estimation with Semantic-Aware
Feature Extraction</a> [monodepth, semantics, Naver labs]</li>
  <li><a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Le_Toward_Interactive_Self-Annotation_For_Video_Object_Bounding_Box_Recurrent_Self-Learning_WACV_2020_paper.pdf">Toward Interactive Self-Annotation For Video Object Bounding Box: Recurrent Self-Learning And Hierarchical Annotation Based Framework</a> <kbd>WACV 2020</kbd></li>
  <li><a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Fang_Towards_Good_Practice_for_CNN-Based_Monocular_Depth_Estimation_WACV_2020_paper.pdf">Towards Good Practice for CNN-Based Monocular Depth Estimation</a> <kbd>WACV 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2004.02788">Self-Supervised Scene De-occlusion</a> <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2009.05505">TP-LSD: Tri-Points Based Line Segment Detector</a></li>
  <li><a href="https://arxiv.org/abs/1712.04440">Data Distillation: Towards Omni-Supervised Learning</a> <kbd>CVPR 2018</kbd> [Kaiming He, FAIR]</li>
  <li><a href="https://arxiv.org/abs/1907.01341">MiDas: Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</a> [monodepth, dynamic object, synthetic dataset]</li>
  <li><a href="https://arxiv.org/abs/2006.04371">Semantics-Driven Unsupervised Learning for Monocular Depth and Ego-Motion Estimation</a> [monodepth]</li>
  <li><a href="https://arxiv.org/abs/2008.08311">Towards Lightweight Lane Detection by Optimizing Spatial Embedding</a> <kbd>ECCV 2020 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2007.04023">Synthetic-to-Real Domain Adaptation for Lane Detection</a> [GM Israel, LLD]</li>
  <li><a href="https://arxiv.org/abs/2004.10924">PolyLaneNet: Lane Estimation via Deep Polynomial Regression</a> <kbd>ICPR 2020</kbd> [polynomial, LLD]</li>
  <li><a href="https://arxiv.org/abs/2012.01050">Learning Universal Shape Dictionary for Realtime Instance Segmentation</a></li>
  <li><a href="https://arxiv.org/abs/2011.14503">End-to-End Video Instance Segmentation with Transformers</a> [DETR, transformers]</li>
  <li><a href="https://arxiv.org/abs/1910.01279">Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks</a> <kbd>CVPR 2020 workshop</kbd></li>
  <li><a href="https://arxiv.org/abs/2011.11156">When and Why Test-Time Augmentation Works</a></li>
  <li><a href="https://arxiv.org/abs/2004.06376">Footprints and Free Space from a Single Color Image</a> <kbd>CVPR 2020 oral</kbd> [Parking use, footprint]</li>
  <li><a href="https://arxiv.org/abs/2008.04047">Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning</a> [BEV, only predict footprint]</li>
  <li><a href="https://arxiv.org/abs/1904.06493">Rethinking Classification and Localization for Object Detection</a> <kbd>CVPR 2020</kbd></li>
  <li><a href="https://arxiv.org/abs/2011.14589">Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2012.07177">Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</a></li>
  <li><a href="https://arxiv.org/abs/2012.05258">ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation</a></li>
  <li><a href="https://arxiv.org/abs/1804.02505">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a> <kbd>ECCV 2018</kbd></li>
  <li><a href="https://arxiv.org/abs/1902.10556">Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference</a> <kbd>CVPR 2019</kbd> [Deep learning + MVS, Vidar, same author MVSNet]</li>
  <li><a href="https://arxiv.org/abs/2012.08274">Artificial Dummies for Urban Dataset Augmentation</a> <kbd>AAAI 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/2012.06785">DETR for Pedestrian Detection</a> [transformer, pedestrian detection]</li>
  <li><a href="https://arxiv.org/abs/2012.12741">Multi-Modality Cut and Paste for 3D Object Detection</a> [SenseTime]</li>
  <li><a href="https://arxiv.org/abs/2012.15840">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</a> [transformer, semantic segmenatation]</li>
  <li><a href="https://arxiv.org/abs/2012.14214">TransPose: Towards Explainable Human Pose Estimation by Transformer</a> [transformer, pose estimation]</li>
  <li><a href="https://arxiv.org/abs/2008.10032">Seesaw Loss for Long-Tailed Instance Segmentation</a></li>
  <li><a href="https://arxiv.org/abs/2012.12645">SWA Object Detection</a> [Stochastic Weights Averaging (SWA)]</li>
  <li><a href="https://arxiv.org/abs/2012.11409">3D Object Detection with Pointformer</a></li>
  <li><a href="https://arxiv.org/abs/2012.09958">Toward Transformer-Based Object Detection</a> [DETR-like]</li>
  <li><a href="https://arxiv.org/abs/2012.10296">Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion</a> [dense SfM]</li>
  <li><a href="https://arxiv.org/abs/2012.12741">Multi-Modality Cut and Paste for 3D Object Detection</a></li>
  <li><a href="http://ras.papercept.net/images/temp/IROS/files/1899.pdf">Vision Global Localization with Semantic Segmentation and Interest Feature Points</a></li>
  <li><a href="https://arxiv.org/abs/2012.09838">Transformer Interpretability Beyond Attention Visualization</a> [transformers]</li>
  <li><a href="https://arxiv.org/abs/2012.07489">Scaling Semantic Segmentation Beyond 1K Classes on a Single GPU</a></li>
  <li><a href="https://arxiv.org/abs/2006.02334">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></li>
  <li><a href="https://arxiv.org/abs/1911.12451">Empirical Upper Bound in Object Detection and More</a></li>
  <li><a href="https://arxiv.org/abs/2012.02124">Generalized Object Detection on Fisheye Cameras for Autonomous Driving: Dataset, Representations and Baseline</a> [Fisheye, Senthil Yogamani]</li>
  <li><a href="https://arxiv.org/abs/2011.14589">Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation</a> [mono3D]</li>
  <li><a href="http://arxiv.org/abs/2101.07422">SOSD-Net: Joint Semantic Object Segmentation and Depth Estimation from Monocular images</a> [Jiwen Lu, monodepth]</li>
  <li><a href="https://arxiv.org/abs/2103.16690">Sparse Auxiliary Networks for Unified Monocular Depth Prediction and Completion</a> [TRI]</li>
  <li><a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear Complexity</a></li>
  <li><a href="https://arxiv.org/abs/1810.00825">Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</a> <kbd>ICML 2019</kbd></li>
  <li><a href="https://arxiv.org/abs/2012.09688">PCT: Point cloud transformer</a> <kbd>Computational Visual Media 2021</kbd></li>
  <li><a href="https://arxiv.org/abs/1707.06397">DDT: Unsupervised Object Discovery and Co-Localization by Deep Descriptor Transforming</a> <kbd>IJCAI 2017</kbd></li>
  <li><a href="https://arxiv.org/abs/2104.00084">Hierarchical Road Topology Learning for Urban Map-less Driving</a> [Mercedes]</li>
  <li><a href="https://arxiv.org/abs/2003.06409">Probabilistic Future Prediction for Video Scene Understanding</a> <kbd>ECCV 2020</kbd> [Alex Kendall]</li>
  <li><a href="https://arxiv.org/abs/2012.02647">Detecting 32 Pedestrian Attributes for Autonomous Vehicles</a> [VRU, MTL]</li>
  <li><a href="https://arxiv.org/abs/2006.07778">Cascaded deep monocular 3D human pose estimation with evolutionary training data</a> <kbd>CVPR 2020 oral</kbd></li>
  <li><a href="https://arxiv.org/abs/2107.13931">MonoGeo: Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2107.13269">Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2107.02493">Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor Distance Voting</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2105.00268">Lite-FPN for Keypoint-based Monocular 3D Object Detection</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2104.09035">Lidar Point Cloud Guided Monocular 3D Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/2103.13413">Vision Transformers for Dense Prediction</a> [Vladlen Koltun, Intel]</li>
  <li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a></li>
  <li><a href="https://arxiv.org/abs/2108.08810">Do Vision Transformers See Like Convolutional Neural Networks?</a></li>
  <li><a href="http://arxiv.org/abs/2108.05793">Progressive Coordinate Transforms for Monocular 3D Object Detection</a> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2108.11127">AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection</a> <kbd>ICCV 2021</kbd> [mono3D]</li>
  <li><a href="https://arxiv.org/abs/2006.10204">BlazePose: On-device Real-time Body Pose tracking</a></li>
</ul>


      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>
